{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Course (980)\n",
    "## Assignment Three \n",
    "\n",
    "__Assignment Goals:__\n",
    "\n",
    "- Implementing RNN based language models.\n",
    "- Implementing and applying a Recurrent Neural Network on text classification problem using TensorFlow.\n",
    "- Implementing __many to one__ and __many to many__ RNN sequence processing.\n",
    "\n",
    "In this assignment, you will implement RNN-based language models and compare extracted word representation from different models. You will also compare two different training methods for sequential data: Truncated Backpropagation Through Time __(TBTT)__ and Backpropagation Through Time __(BTT)__. \n",
    "Also, you will be asked to apply Vanilla RNN to capture word representations and solve a text classification problem. \n",
    "\n",
    "\n",
    "__DataSets__: You will use two datasets, an English Literature for language model task (part 1 to 4) and 20Newsgroups for text classification (part 5). \n",
    "\n",
    "\n",
    "1. (30 points) Implement the RNN based language model described by Mikolov et al.[1], also called __Elman network__ and train a language model on the English Literature dataset. This network contains input, hidden and output layer and is trained by standard backpropagation (TBTT with τ = 1) using the cross-entropy loss. \n",
    "   - The input represents the current word while using 1-of-N coding (thus its size is equal to the size of the vocabulary) and vector s(t − 1) that represents output values in the hidden layer from the previous time step. \n",
    "   - The hidden layer is a fully connected sigmoid layer with size 500. \n",
    "   - Softmax Output Layer to capture a valid probability distribution.\n",
    "   - The model is trained with truncated backpropagation through time (TBTT) with τ = 1: the weights of the network are updated based on the error vector computed only for the current time step.\n",
    "   \n",
    "   Download the English Literature dataset and train the language model as described, report the model cross-entropy loss on the train set. Use nltk.word_tokenize to tokenize the documents. \n",
    "For initialization, s(0) can be set to a vector of small values. Note that we are not interested in the *dynamic model* mentioned in the original paper. \n",
    "To make the implementation simpler you can use Keras to define neural net layers, including Keras.Embedding. (Keras.Embedding will create an additional mapping layer compared to the Elman architecture.) \n",
    "\n",
    "2. (20 points) TBTT has less computational cost and memory needs in comparison with *backpropagation through time algorithm (BTT)*. These benefits come at the cost of losing long term dependencies [2]. Now let's try to investigate computational costs and performance of learning our language model with BTT. For training the Elman-type RNN with BTT, one option is to perform mini-batch gradient descent with exactly one sentence per mini-batch. (The input  size will be [1, Sentence Length]). \n",
    "\n",
    "    1. Split the document into sentences (you can use nltk.tokenize.sent_tokenize).\n",
    "    2. For each sentence, perform one pass that computes the mean/sum loss for this sentence; then perform a gradient update for the whole sentence. (So the mini-batch size varies for the sentences with different lengths). You can truncate long sentences to fit the data in memory. \n",
    "    3. Report the model cross-entropy loss.\n",
    "\n",
    "3. (15 points) It does not seem that simple recurrent neural networks can capture truly exploit context information with long dependencies, because of the problem that gradients vanish and exploding. To solve this problem, gating mechanisms for recurrent neural networks were introduced. Try to learn your last model (Elman + BTT) with the SimpleRnn unit replaced with a Gated Recurrent Unit (GRU). Report the model cross-entropy loss. Compare your results in terms of cross-entropy loss with two other approach(part 1 and 2). Use each model to generate 10 synthetic sentences of 15 words each. Discuss the quality of the sentences generated - do they look like proper English? Do they match the training set?\n",
    "    Text generation from a given language model can be done using the following iterative process:\n",
    "   1. Set sequence = \\[first_word\\], chosen randomly.\n",
    "   2. Select a new word based on the sequence so far, add this word to the sequence, and repeat. At each iteration, select the word with maximum probability given the sequence so far. The trained language model outputs this probability. \n",
    "\n",
    "4. (15 points) The text describes how to extract a word representation from a trained RNN (Chapter 4). How we can evaluate the extracted word representation for your trained RNN? Compare the words representation extracted from each of the approaches using one of the existing methods.\n",
    "\n",
    "5. (20 points) We are aiming to learn an RNN model that predicts document categories given its content (text classification). For this task, we will use the 20Newsgroupst dataset. The 20Newsgroupst contains messages from twenty newsgroups.  We selected four major categories (comp, politics, rec, and religion) comprising around 13k documents altogether. Your model should learn word representations to support the classification task. For solving this problem modify the __Elman network__ architecture such that the last layer is a softmax layer with just 4 output neurons (one for each category). \n",
    "\n",
    "    1. Download the 20Newsgroups dataset, and use the implemented code from the notebook to read in the dataset.\n",
    "    2. Split the data into a training set (90 percent) and validation set (10 percent). Train the model on  20Newsgroups.\n",
    "    3. Report your accuracy results on the validation set.\n",
    "\n",
    "__NOTE__: Please use Jupyter Notebook. The notebook should include the final code, results and your answers. You should submit your Notebook in (.pdf or .html) and .ipynb format. (penalty 10 points) \n",
    "\n",
    "To reduce the parameters, you can merge all words that occur less often than a threshold into a special rare token (\\__unk__).\n",
    "\n",
    "__Instructions__:\n",
    "\n",
    "The university policy on academic dishonesty and plagiarism (cheating) will be taken very seriously in this course. Everything submitted should be your own writing or coding. You must not let other students copy your work. Spelling and grammar count.\n",
    "\n",
    "Your assignments will be marked based on correctness, originality (the implementations and ideas are from yourself), clarification and test performance.\n",
    "\n",
    "\n",
    "[1] Tom´ as Mikolov, Martin Kara ˇ fiat, Luk´ ´ as Burget, Jan ˇ Cernock´ ˇ y,Sanjeev Khudanpur: Recurrent neural network based language model, In: Proc. INTERSPEECH 2010\n",
    "\n",
    "[2] Tallec, Corentin, and Yann Ollivier. \"Unbiasing truncated backpropagation through time.\" arXiv preprint arXiv:1705.08209 (2017).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'datasets/20news_subsampled'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d0436a4d4b75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"datasets/20news_subsampled\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mnews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-d0436a4d4b75>\u001b[0m in \u001b[0;36mdata_loader\u001b[1;34m(images_dir)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mcategories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mnews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# news content\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# category which it belong to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'datasets/20news_subsampled'"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"This code is used to read all news and their labels\"\"\"\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def to_categories(name, cat=[\"politics\",\"rec\",\"comp\",\"religion\"]):\n",
    "    for i in range(len(cat)):\n",
    "        if str.find(name,cat[i])>-1:\n",
    "            return(i)\n",
    "    print(\"Unexpected folder: \" + name) # print the folder name which does not include expected categories\n",
    "    return(\"wth\")\n",
    "\n",
    "def data_loader(images_dir):\n",
    "    categories = os.listdir(data_path)\n",
    "    news = [] # news content\n",
    "    groups = [] # category which it belong to\n",
    "    \n",
    "    for cat in categories:\n",
    "        print(\"Category:\"+cat)\n",
    "        for the_new_path in glob.glob(data_path + '/' + cat + '/*'):\n",
    "            news.append(open(the_new_path,encoding = \"ISO-8859-1\", mode ='r').read())\n",
    "            groups.append(cat)\n",
    "\n",
    "    return news, list(map(to_categories, groups))\n",
    "\n",
    "\n",
    "\n",
    "data_path = \"datasets/20news_subsampled\"\n",
    "news, groups = data_loader(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\songyih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254533\n"
     ]
    }
   ],
   "source": [
    "'''Implementing RNN based language models.\n",
    "'''\n",
    "from nltk import word_tokenize, download\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# load the English Literature dataset\n",
    "english_literature_path = './datasets/English Literature.txt'\n",
    "with open(english_literature_path) as f:\n",
    "    english_literature_text = f.read()\n",
    "print(len(english_literature_text))\n",
    "\n",
    "# tokenize the English Literature dataset\n",
    "download('punkt')\n",
    "english_literature_tokens = word_tokenize(english_literature_text)\n",
    "print(len(english_literature_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14309\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary\n",
    "from collections import Counter\n",
    "\n",
    "word2index = {}\n",
    "index2word = []\n",
    "english_literature_counter = Counter(english_literature_tokens)\n",
    "for word, count in english_literature_counter.items():\n",
    "    index2word.append(word)\n",
    "    word2index[word] = len(word2index)\n",
    "\n",
    "vocabulary_size = len(word2index)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0%\r",
      "Progress: 0.04%\r",
      "Progress: 0.08%\r",
      "Progress: 0.12%\r",
      "Progress: 0.16%\r",
      "Progress: 0.2%\r",
      "Progress: 0.24%\r",
      "Progress: 0.28%\r",
      "Progress: 0.31%\r",
      "Progress: 0.35%\r",
      "Progress: 0.39%\r",
      "Progress: 0.43%\r",
      "Progress: 0.47%\r",
      "Progress: 0.51%\r",
      "Progress: 0.55%\r",
      "Progress: 0.59%\r",
      "Progress: 0.63%\r",
      "Progress: 0.67%\r",
      "Progress: 0.71%\r",
      "Progress: 0.75%\r",
      "Progress: 0.79%\r",
      "Progress: 0.83%\r",
      "Progress: 0.86%\r",
      "Progress: 0.9%\r",
      "Progress: 0.94%\r",
      "Progress: 0.98%\r",
      "Progress: 1.02%\r",
      "Progress: 1.06%\r",
      "Progress: 1.1%\r",
      "Progress: 1.14%\r",
      "Progress: 1.18%\r",
      "Progress: 1.22%\r",
      "Progress: 1.26%\r",
      "Progress: 1.3%\r",
      "Progress: 1.34%\r",
      "Progress: 1.38%\r",
      "Progress: 1.41%\r",
      "Progress: 1.45%\r",
      "Progress: 1.49%\r",
      "Progress: 1.53%\r",
      "Progress: 1.57%\r",
      "Progress: 1.61%\r",
      "Progress: 1.65%\r",
      "Progress: 1.69%\r",
      "Progress: 1.73%\r",
      "Progress: 1.77%\r",
      "Progress: 1.81%\r",
      "Progress: 1.85%\r",
      "Progress: 1.89%\r",
      "Progress: 1.93%\r",
      "Progress: 1.96%\r",
      "Progress: 2.0%\r",
      "Progress: 2.04%\r",
      "Progress: 2.08%\r",
      "Progress: 2.12%\r",
      "Progress: 2.16%\r",
      "Progress: 2.2%\r",
      "Progress: 2.24%\r",
      "Progress: 2.28%\r",
      "Progress: 2.32%\r",
      "Progress: 2.36%\r",
      "Progress: 2.4%\r",
      "Progress: 2.44%\r",
      "Progress: 2.48%\r",
      "Progress: 2.51%\r",
      "Progress: 2.55%\r",
      "Progress: 2.59%\r",
      "Progress: 2.63%\r",
      "Progress: 2.67%\r",
      "Progress: 2.71%\r",
      "Progress: 2.75%\r",
      "Progress: 2.79%\r",
      "Progress: 2.83%\r",
      "Progress: 2.87%\r",
      "Progress: 2.91%\r",
      "Progress: 2.95%\r",
      "Progress: 2.99%\r",
      "Progress: 3.03%\r",
      "Progress: 3.06%\r",
      "Progress: 3.1%\r",
      "Progress: 3.14%\r",
      "Progress: 3.18%\r",
      "Progress: 3.22%\r",
      "Progress: 3.26%\r",
      "Progress: 3.3%\r",
      "Progress: 3.34%\r",
      "Progress: 3.38%\r",
      "Progress: 3.42%\r",
      "Progress: 3.46%\r",
      "Progress: 3.5%\r",
      "Progress: 3.54%\r",
      "Progress: 3.58%\r",
      "Progress: 3.61%\r",
      "Progress: 3.65%\r",
      "Progress: 3.69%\r",
      "Progress: 3.73%\r",
      "Progress: 3.77%\r",
      "Progress: 3.81%\r",
      "Progress: 3.85%\r",
      "Progress: 3.89%\r",
      "Progress: 3.93%\r",
      "Progress: 3.97%\r",
      "Progress: 4.01%\r",
      "Progress: 4.05%\r",
      "Progress: 4.09%\r",
      "Progress: 4.13%\r",
      "Progress: 4.16%\r",
      "Progress: 4.2%\r",
      "Progress: 4.24%\r",
      "Progress: 4.28%\r",
      "Progress: 4.32%\r",
      "Progress: 4.36%\r",
      "Progress: 4.4%\r",
      "Progress: 4.44%\r",
      "Progress: 4.48%\r",
      "Progress: 4.52%\r",
      "Progress: 4.56%\r",
      "Progress: 4.6%\r",
      "Progress: 4.64%\r",
      "Progress: 4.68%\r",
      "Progress: 4.71%\r",
      "Progress: 4.75%\r",
      "Progress: 4.79%\r",
      "Progress: 4.83%\r",
      "Progress: 4.87%\r",
      "Progress: 4.91%\r",
      "Progress: 4.95%\r",
      "Progress: 4.99%\r",
      "Progress: 5.03%\r",
      "Progress: 5.07%\r",
      "Progress: 5.11%\r",
      "Progress: 5.15%\r",
      "Progress: 5.19%\r",
      "Progress: 5.23%\r",
      "Progress: 5.26%\r",
      "Progress: 5.3%\r",
      "Progress: 5.34%\r",
      "Progress: 5.38%\r",
      "Progress: 5.42%\r",
      "Progress: 5.46%\r",
      "Progress: 5.5%\r",
      "Progress: 5.54%\r",
      "Progress: 5.58%\r",
      "Progress: 5.62%\r",
      "Progress: 5.66%\r",
      "Progress: 5.7%\r",
      "Progress: 5.74%\r",
      "Progress: 5.78%\r",
      "Progress: 5.81%\r",
      "Progress: 5.85%\r",
      "Progress: 5.89%\r",
      "Progress: 5.93%\r",
      "Progress: 5.97%\r",
      "Progress: 6.01%\r",
      "Progress: 6.05%\r",
      "Progress: 6.09%\r",
      "Progress: 6.13%\r",
      "Progress: 6.17%\r",
      "Progress: 6.21%\r",
      "Progress: 6.25%\r",
      "Progress: 6.29%\r",
      "Progress: 6.33%\r",
      "Progress: 6.36%\r",
      "Progress: 6.4%\r",
      "Progress: 6.44%\r",
      "Progress: 6.48%\r",
      "Progress: 6.52%\r",
      "Progress: 6.56%\r",
      "Progress: 6.6%\r",
      "Progress: 6.64%\r",
      "Progress: 6.68%\r",
      "Progress: 6.72%\r",
      "Progress: 6.76%\r",
      "Progress: 6.8%\r",
      "Progress: 6.84%\r",
      "Progress: 6.88%\r",
      "Progress: 6.91%\r",
      "Progress: 6.95%\r",
      "Progress: 6.99%\r",
      "Progress: 7.03%\r",
      "Progress: 7.07%\r",
      "Progress: 7.11%\r",
      "Progress: 7.15%\r",
      "Progress: 7.19%\r",
      "Progress: 7.23%\r",
      "Progress: 7.27%\r",
      "Progress: 7.31%\r",
      "Progress: 7.35%\r",
      "Progress: 7.39%\r",
      "Progress: 7.43%\r",
      "Progress: 7.46%\r",
      "Progress: 7.5%\r",
      "Progress: 7.54%\r",
      "Progress: 7.58%\r",
      "Progress: 7.62%\r",
      "Progress: 7.66%\r",
      "Progress: 7.7%\r",
      "Progress: 7.74%\r",
      "Progress: 7.78%\r",
      "Progress: 7.82%\r",
      "Progress: 7.86%\r",
      "Progress: 7.9%\r",
      "Progress: 7.94%\r",
      "Progress: 7.98%\r",
      "Progress: 8.01%\r",
      "Progress: 8.05%\r",
      "Progress: 8.09%\r",
      "Progress: 8.13%\r",
      "Progress: 8.17%\r",
      "Progress: 8.21%\r",
      "Progress: 8.25%\r",
      "Progress: 8.29%\r",
      "Progress: 8.33%\r",
      "Progress: 8.37%\r",
      "Progress: 8.41%\r",
      "Progress: 8.45%\r",
      "Progress: 8.49%\r",
      "Progress: 8.53%\r",
      "Progress: 8.56%\r",
      "Progress: 8.6%\r",
      "Progress: 8.64%\r",
      "Progress: 8.68%\r",
      "Progress: 8.72%\r",
      "Progress: 8.76%\r",
      "Progress: 8.8%\r",
      "Progress: 8.84%\r",
      "Progress: 8.88%\r",
      "Progress: 8.92%\r",
      "Progress: 8.96%\r",
      "Progress: 9.0%\r",
      "Progress: 9.04%\r",
      "Progress: 9.08%\r",
      "Progress: 9.11%\r",
      "Progress: 9.15%\r",
      "Progress: 9.19%\r",
      "Progress: 9.23%\r",
      "Progress: 9.27%\r",
      "Progress: 9.31%\r",
      "Progress: 9.35%\r",
      "Progress: 9.39%\r",
      "Progress: 9.43%\r",
      "Progress: 9.47%\r",
      "Progress: 9.51%\r",
      "Progress: 9.55%\r",
      "Progress: 9.59%\r",
      "Progress: 9.63%\r",
      "Progress: 9.66%\r",
      "Progress: 9.7%\r",
      "Progress: 9.74%\r",
      "Progress: 9.78%\r",
      "Progress: 9.82%\r",
      "Progress: 9.86%\r",
      "Progress: 9.9%\r",
      "Progress: 9.94%\r",
      "Progress: 9.98%\r",
      "Progress: 10.02%\r",
      "Progress: 10.06%\r",
      "Progress: 10.1%\r",
      "Progress: 10.14%\r",
      "Progress: 10.18%\r",
      "Progress: 10.21%\r",
      "Progress: 10.25%\r",
      "Progress: 10.29%\r",
      "Progress: 10.33%\r",
      "Progress: 10.37%\r",
      "Progress: 10.41%\r",
      "Progress: 10.45%\r",
      "Progress: 10.49%\r",
      "Progress: 10.53%\r",
      "Progress: 10.57%\r",
      "Progress: 10.61%\r",
      "Progress: 10.65%\r",
      "Progress: 10.69%\r",
      "Progress: 10.73%\r",
      "Progress: 10.76%\r",
      "Progress: 10.8%\r",
      "Progress: 10.84%\r",
      "Progress: 10.88%\r",
      "Progress: 10.92%\r",
      "Progress: 10.96%\r",
      "Progress: 11.0%\r",
      "Progress: 11.04%\r",
      "Progress: 11.08%\r",
      "Progress: 11.12%\r",
      "Progress: 11.16%\r",
      "Progress: 11.2%\r",
      "Progress: 11.24%\r",
      "Progress: 11.28%\r",
      "Progress: 11.31%\r",
      "Progress: 11.35%\r",
      "Progress: 11.39%\r",
      "Progress: 11.43%\r",
      "Progress: 11.47%\r",
      "Progress: 11.51%\r",
      "Progress: 11.55%\r",
      "Progress: 11.59%\r",
      "Progress: 11.63%\r",
      "Progress: 11.67%\r",
      "Progress: 11.71%\r",
      "Progress: 11.75%\r",
      "Progress: 11.79%\r",
      "Progress: 11.83%\r",
      "Progress: 11.86%\r",
      "Progress: 11.9%\r",
      "Progress: 11.94%\r",
      "Progress: 11.98%\r",
      "Progress: 12.02%\r",
      "Progress: 12.06%\r",
      "Progress: 12.1%\r",
      "Progress: 12.14%\r",
      "Progress: 12.18%\r",
      "Progress: 12.22%\r",
      "Progress: 12.26%\r",
      "Progress: 12.3%\r",
      "Progress: 12.34%\r",
      "Progress: 12.38%\r",
      "Progress: 12.41%\r",
      "Progress: 12.45%\r",
      "Progress: 12.49%\r",
      "Progress: 12.53%\r",
      "Progress: 12.57%\r",
      "Progress: 12.61%\r",
      "Progress: 12.65%\r",
      "Progress: 12.69%\r",
      "Progress: 12.73%\r",
      "Progress: 12.77%\r",
      "Progress: 12.81%\r",
      "Progress: 12.85%\r",
      "Progress: 12.89%\r",
      "Progress: 12.93%\r",
      "Progress: 12.96%\r",
      "Progress: 13.0%\r",
      "Progress: 13.04%\r",
      "Progress: 13.08%\r",
      "Progress: 13.12%\r",
      "Progress: 13.16%\r",
      "Progress: 13.2%\r",
      "Progress: 13.24%\r",
      "Progress: 13.28%\r",
      "Progress: 13.32%\r",
      "Progress: 13.36%\r",
      "Progress: 13.4%\r",
      "Progress: 13.44%\r",
      "Progress: 13.48%\r",
      "Progress: 13.51%\r",
      "Progress: 13.55%\r",
      "Progress: 13.59%\r",
      "Progress: 13.63%\r",
      "Progress: 13.67%\r",
      "Progress: 13.71%\r",
      "Progress: 13.75%\r",
      "Progress: 13.79%\r",
      "Progress: 13.83%\r",
      "Progress: 13.87%\r",
      "Progress: 13.91%\r",
      "Progress: 13.95%\r",
      "Progress: 13.99%\r",
      "Progress: 14.03%\r",
      "Progress: 14.06%\r",
      "Progress: 14.1%\r",
      "Progress: 14.14%\r",
      "Progress: 14.18%\r",
      "Progress: 14.22%\r",
      "Progress: 14.26%\r",
      "Progress: 14.3%\r",
      "Progress: 14.34%\r",
      "Progress: 14.38%\r",
      "Progress: 14.42%\r",
      "Progress: 14.46%\r",
      "Progress: 14.5%\r",
      "Progress: 14.54%\r",
      "Progress: 14.58%\r",
      "Progress: 14.62%\r",
      "Progress: 14.65%\r",
      "Progress: 14.69%\r",
      "Progress: 14.73%\r",
      "Progress: 14.77%\r",
      "Progress: 14.81%\r",
      "Progress: 14.85%\r",
      "Progress: 14.89%\r",
      "Progress: 14.93%\r",
      "Progress: 14.97%\r",
      "Progress: 15.01%\r",
      "Progress: 15.05%\r",
      "Progress: 15.09%\r",
      "Progress: 15.13%\r",
      "Progress: 15.17%\r",
      "Progress: 15.2%\r",
      "Progress: 15.24%\r",
      "Progress: 15.28%\r",
      "Progress: 15.32%\r",
      "Progress: 15.36%\r",
      "Progress: 15.4%\r",
      "Progress: 15.44%\r",
      "Progress: 15.48%\r",
      "Progress: 15.52%\r",
      "Progress: 15.56%\r",
      "Progress: 15.6%\r",
      "Progress: 15.64%\r",
      "Progress: 15.68%\r",
      "Progress: 15.72%\r",
      "Progress: 15.75%\r",
      "Progress: 15.79%\r",
      "Progress: 15.83%\r",
      "Progress: 15.87%\r",
      "Progress: 15.91%\r",
      "Progress: 15.95%\r",
      "Progress: 15.99%\r",
      "Progress: 16.03%\r",
      "Progress: 16.07%\r",
      "Progress: 16.11%\r",
      "Progress: 16.15%\r",
      "Progress: 16.19%\r",
      "Progress: 16.23%\r",
      "Progress: 16.27%\r",
      "Progress: 16.3%\r",
      "Progress: 16.34%\r",
      "Progress: 16.38%\r",
      "Progress: 16.42%\r",
      "Progress: 16.46%\r",
      "Progress: 16.5%\r",
      "Progress: 16.54%\r",
      "Progress: 16.58%\r",
      "Progress: 16.62%\r",
      "Progress: 16.66%\r",
      "Progress: 16.7%\r",
      "Progress: 16.74%\r",
      "Progress: 16.78%\r",
      "Progress: 16.82%\r",
      "Progress: 16.85%\r",
      "Progress: 16.89%\r",
      "Progress: 16.93%\r",
      "Progress: 16.97%\r",
      "Progress: 17.01%\r",
      "Progress: 17.05%\r",
      "Progress: 17.09%\r",
      "Progress: 17.13%\r",
      "Progress: 17.17%\r",
      "Progress: 17.21%\r",
      "Progress: 17.25%\r",
      "Progress: 17.29%\r",
      "Progress: 17.33%\r",
      "Progress: 17.37%\r",
      "Progress: 17.4%\r",
      "Progress: 17.44%\r",
      "Progress: 17.48%\r",
      "Progress: 17.52%\r",
      "Progress: 17.56%\r",
      "Progress: 17.6%\r",
      "Progress: 17.64%\r",
      "Progress: 17.68%\r",
      "Progress: 17.72%\r",
      "Progress: 17.76%\r",
      "Progress: 17.8%\r",
      "Progress: 17.84%\r",
      "Progress: 17.88%\r",
      "Progress: 17.92%\r",
      "Progress: 17.95%\r",
      "Progress: 17.99%\r",
      "Progress: 18.03%\r",
      "Progress: 18.07%\r",
      "Progress: 18.11%\r",
      "Progress: 18.15%\r",
      "Progress: 18.19%\r",
      "Progress: 18.23%\r",
      "Progress: 18.27%\r",
      "Progress: 18.31%\r",
      "Progress: 18.35%\r",
      "Progress: 18.39%\r",
      "Progress: 18.43%\r",
      "Progress: 18.47%\r",
      "Progress: 18.5%\r",
      "Progress: 18.54%\r",
      "Progress: 18.58%\r",
      "Progress: 18.62%\r",
      "Progress: 18.66%\r",
      "Progress: 18.7%\r",
      "Progress: 18.74%\r",
      "Progress: 18.78%\r",
      "Progress: 18.82%\r",
      "Progress: 18.86%\r",
      "Progress: 18.9%\r",
      "Progress: 18.94%\r",
      "Progress: 18.98%\r",
      "Progress: 19.02%\r",
      "Progress: 19.05%\r",
      "Progress: 19.09%\r",
      "Progress: 19.13%\r",
      "Progress: 19.17%\r",
      "Progress: 19.21%\r",
      "Progress: 19.25%\r",
      "Progress: 19.29%\r",
      "Progress: 19.33%\r",
      "Progress: 19.37%\r",
      "Progress: 19.41%\r",
      "Progress: 19.45%\r",
      "Progress: 19.49%\r",
      "Progress: 19.53%\r",
      "Progress: 19.57%\r",
      "Progress: 19.6%\r",
      "Progress: 19.64%\r",
      "Progress: 19.68%\r",
      "Progress: 19.72%\r",
      "Progress: 19.76%\r",
      "Progress: 19.8%\r",
      "Progress: 19.84%\r",
      "Progress: 19.88%\r",
      "Progress: 19.92%\r",
      "Progress: 19.96%\r",
      "Progress: 20.0%\r",
      "Progress: 20.04%\r",
      "Progress: 20.08%\r",
      "Progress: 20.12%\r",
      "Progress: 20.15%\r",
      "Progress: 20.19%\r",
      "Progress: 20.23%\r",
      "Progress: 20.27%\r",
      "Progress: 20.31%\r",
      "Progress: 20.35%\r",
      "Progress: 20.39%\r",
      "Progress: 20.43%\r",
      "Progress: 20.47%\r",
      "Progress: 20.51%\r",
      "Progress: 20.55%\r",
      "Progress: 20.59%\r",
      "Progress: 20.63%\r",
      "Progress: 20.67%\r",
      "Progress: 20.7%\r",
      "Progress: 20.74%\r",
      "Progress: 20.78%\r",
      "Progress: 20.82%\r",
      "Progress: 20.86%\r",
      "Progress: 20.9%\r",
      "Progress: 20.94%\r",
      "Progress: 20.98%\r",
      "Progress: 21.02%\r",
      "Progress: 21.06%\r",
      "Progress: 21.1%\r",
      "Progress: 21.14%\r",
      "Progress: 21.18%\r",
      "Progress: 21.22%\r",
      "Progress: 21.25%\r",
      "Progress: 21.29%\r",
      "Progress: 21.33%\r",
      "Progress: 21.37%\r",
      "Progress: 21.41%\r",
      "Progress: 21.45%\r",
      "Progress: 21.49%\r",
      "Progress: 21.53%\r",
      "Progress: 21.57%\r",
      "Progress: 21.61%\r",
      "Progress: 21.65%\r",
      "Progress: 21.69%\r",
      "Progress: 21.73%\r",
      "Progress: 21.77%\r",
      "Progress: 21.8%\r",
      "Progress: 21.84%\r",
      "Progress: 21.88%\r",
      "Progress: 21.92%\r",
      "Progress: 21.96%\r",
      "Progress: 22.0%\r",
      "Progress: 22.04%\r",
      "Progress: 22.08%\r",
      "Progress: 22.12%\r",
      "Progress: 22.16%\r",
      "Progress: 22.2%\r",
      "Progress: 22.24%\r",
      "Progress: 22.28%\r",
      "Progress: 22.32%\r",
      "Progress: 22.35%\r",
      "Progress: 22.39%\r",
      "Progress: 22.43%\r",
      "Progress: 22.47%\r",
      "Progress: 22.51%\r",
      "Progress: 22.55%\r",
      "Progress: 22.59%\r",
      "Progress: 22.63%\r",
      "Progress: 22.67%\r",
      "Progress: 22.71%\r",
      "Progress: 22.75%\r",
      "Progress: 22.79%\r",
      "Progress: 22.83%\r",
      "Progress: 22.87%\r",
      "Progress: 22.9%\r",
      "Progress: 22.94%\r",
      "Progress: 22.98%\r",
      "Progress: 23.02%\r",
      "Progress: 23.06%\r",
      "Progress: 23.1%\r",
      "Progress: 23.14%\r",
      "Progress: 23.18%\r",
      "Progress: 23.22%\r",
      "Progress: 23.26%\r",
      "Progress: 23.3%\r",
      "Progress: 23.34%\r",
      "Progress: 23.38%\r",
      "Progress: 23.42%\r",
      "Progress: 23.45%\r",
      "Progress: 23.49%\r",
      "Progress: 23.53%\r",
      "Progress: 23.57%\r",
      "Progress: 23.61%\r",
      "Progress: 23.65%\r",
      "Progress: 23.69%\r",
      "Progress: 23.73%\r",
      "Progress: 23.77%\r",
      "Progress: 23.81%\r",
      "Progress: 23.85%\r",
      "Progress: 23.89%\r",
      "Progress: 23.93%\r",
      "Progress: 23.97%\r",
      "Progress: 24.0%\r",
      "Progress: 24.04%\r",
      "Progress: 24.08%\r",
      "Progress: 24.12%\r",
      "Progress: 24.16%\r",
      "Progress: 24.2%\r",
      "Progress: 24.24%\r",
      "Progress: 24.28%\r",
      "Progress: 24.32%\r",
      "Progress: 24.36%\r",
      "Progress: 24.4%\r",
      "Progress: 24.44%\r",
      "Progress: 24.48%\r",
      "Progress: 24.52%\r",
      "Progress: 24.55%\r",
      "Progress: 24.59%\r",
      "Progress: 24.63%\r",
      "Progress: 24.67%\r",
      "Progress: 24.71%\r",
      "Progress: 24.75%\r",
      "Progress: 24.79%\r",
      "Progress: 24.83%\r",
      "Progress: 24.87%\r",
      "Progress: 24.91%\r",
      "Progress: 24.95%\r",
      "Progress: 24.99%\r",
      "Progress: 25.03%\r",
      "Progress: 25.07%\r",
      "Progress: 25.1%\r",
      "Progress: 25.14%\r",
      "Progress: 25.18%\r",
      "Progress: 25.22%\r",
      "Progress: 25.26%\r",
      "Progress: 25.3%\r",
      "Progress: 25.34%\r",
      "Progress: 25.38%\r",
      "Progress: 25.42%\r",
      "Progress: 25.46%\r",
      "Progress: 25.5%\r",
      "Progress: 25.54%\r",
      "Progress: 25.58%\r",
      "Progress: 25.62%\r",
      "Progress: 25.65%\r",
      "Progress: 25.69%\r",
      "Progress: 25.73%\r",
      "Progress: 25.77%\r",
      "Progress: 25.81%\r",
      "Progress: 25.85%\r",
      "Progress: 25.89%\r",
      "Progress: 25.93%\r",
      "Progress: 25.97%\r",
      "Progress: 26.01%\r",
      "Progress: 26.05%\r",
      "Progress: 26.09%\r",
      "Progress: 26.13%\r",
      "Progress: 26.17%\r",
      "Progress: 26.2%\r",
      "Progress: 26.24%\r",
      "Progress: 26.28%\r",
      "Progress: 26.32%\r",
      "Progress: 26.36%\r",
      "Progress: 26.4%\r",
      "Progress: 26.44%\r",
      "Progress: 26.48%\r",
      "Progress: 26.52%\r",
      "Progress: 26.56%\r",
      "Progress: 26.6%\r",
      "Progress: 26.64%\r",
      "Progress: 26.68%\r",
      "Progress: 26.72%\r",
      "Progress: 26.75%\r",
      "Progress: 26.79%\r",
      "Progress: 26.83%\r",
      "Progress: 26.87%\r",
      "Progress: 26.91%\r",
      "Progress: 26.95%\r",
      "Progress: 26.99%\r",
      "Progress: 27.03%\r",
      "Progress: 27.07%\r",
      "Progress: 27.11%\r",
      "Progress: 27.15%\r",
      "Progress: 27.19%\r",
      "Progress: 27.23%\r",
      "Progress: 27.27%\r",
      "Progress: 27.3%\r",
      "Progress: 27.34%\r",
      "Progress: 27.38%\r",
      "Progress: 27.42%\r",
      "Progress: 27.46%\r",
      "Progress: 27.5%\r",
      "Progress: 27.54%\r",
      "Progress: 27.58%\r",
      "Progress: 27.62%\r",
      "Progress: 27.66%\r",
      "Progress: 27.7%\r",
      "Progress: 27.74%\r",
      "Progress: 27.78%\r",
      "Progress: 27.82%\r",
      "Progress: 27.85%\r",
      "Progress: 27.89%\r",
      "Progress: 27.93%\r",
      "Progress: 27.97%\r",
      "Progress: 28.01%\r",
      "Progress: 28.05%\r",
      "Progress: 28.09%\r",
      "Progress: 28.13%\r",
      "Progress: 28.17%\r",
      "Progress: 28.21%\r",
      "Progress: 28.25%\r",
      "Progress: 28.29%\r",
      "Progress: 28.33%\r",
      "Progress: 28.37%\r",
      "Progress: 28.4%\r",
      "Progress: 28.44%\r",
      "Progress: 28.48%\r",
      "Progress: 28.52%\r",
      "Progress: 28.56%\r",
      "Progress: 28.6%\r",
      "Progress: 28.64%\r",
      "Progress: 28.68%\r",
      "Progress: 28.72%\r",
      "Progress: 28.76%\r",
      "Progress: 28.8%\r",
      "Progress: 28.84%\r",
      "Progress: 28.88%\r",
      "Progress: 28.92%\r",
      "Progress: 28.95%\r",
      "Progress: 28.99%\r",
      "Progress: 29.03%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress: 29.07%\r",
      "Progress: 29.11%\r",
      "Progress: 29.15%\r",
      "Progress: 29.19%\r",
      "Progress: 29.23%\r",
      "Progress: 29.27%\r",
      "Progress: 29.31%\r",
      "Progress: 29.35%\r",
      "Progress: 29.39%\r",
      "Progress: 29.43%\r",
      "Progress: 29.47%\r",
      "Progress: 29.51%\r",
      "Progress: 29.54%\r",
      "Progress: 29.58%\r",
      "Progress: 29.62%\r",
      "Progress: 29.66%\r",
      "Progress: 29.7%\r",
      "Progress: 29.74%\r",
      "Progress: 29.78%\r",
      "Progress: 29.82%\r",
      "Progress: 29.86%\r",
      "Progress: 29.9%\r",
      "Progress: 29.94%\r",
      "Progress: 29.98%\r",
      "Progress: 30.02%\r",
      "Progress: 30.06%\r",
      "Progress: 30.09%\r",
      "Progress: 30.13%\r",
      "Progress: 30.17%\r",
      "Progress: 30.21%\r",
      "Progress: 30.25%\r",
      "Progress: 30.29%\r",
      "Progress: 30.33%\r",
      "Progress: 30.37%\r",
      "Progress: 30.41%\r",
      "Progress: 30.45%\r",
      "Progress: 30.49%\r",
      "Progress: 30.53%\r",
      "Progress: 30.57%\r",
      "Progress: 30.61%\r",
      "Progress: 30.64%\r",
      "Progress: 30.68%\r",
      "Progress: 30.72%\r",
      "Progress: 30.76%\r",
      "Progress: 30.8%\r",
      "Progress: 30.84%\r",
      "Progress: 30.88%\r",
      "Progress: 30.92%\r",
      "Progress: 30.96%\r",
      "Progress: 31.0%\r",
      "Progress: 31.04%\r",
      "Progress: 31.08%\r",
      "Progress: 31.12%\r",
      "Progress: 31.16%\r",
      "Progress: 31.19%\r",
      "Progress: 31.23%\r",
      "Progress: 31.27%\r",
      "Progress: 31.31%\r",
      "Progress: 31.35%\r",
      "Progress: 31.39%\r",
      "Progress: 31.43%\r",
      "Progress: 31.47%\r",
      "Progress: 31.51%\r",
      "Progress: 31.55%\r",
      "Progress: 31.59%\r",
      "Progress: 31.63%\r",
      "Progress: 31.67%\r",
      "Progress: 31.71%\r",
      "Progress: 31.74%\r",
      "Progress: 31.78%\r",
      "Progress: 31.82%\r",
      "Progress: 31.86%\r",
      "Progress: 31.9%\r",
      "Progress: 31.94%\r",
      "Progress: 31.98%\r",
      "Progress: 32.02%\r",
      "Progress: 32.06%\r",
      "Progress: 32.1%\r",
      "Progress: 32.14%\r",
      "Progress: 32.18%\r",
      "Progress: 32.22%\r",
      "Progress: 32.26%\r",
      "Progress: 32.29%\r",
      "Progress: 32.33%\r",
      "Progress: 32.37%\r",
      "Progress: 32.41%\r",
      "Progress: 32.45%\r",
      "Progress: 32.49%\r",
      "Progress: 32.53%\r",
      "Progress: 32.57%\r",
      "Progress: 32.61%\r",
      "Progress: 32.65%\r",
      "Progress: 32.69%\r",
      "Progress: 32.73%\r",
      "Progress: 32.77%\r",
      "Progress: 32.81%\r",
      "Progress: 32.84%\r",
      "Progress: 32.88%\r",
      "Progress: 32.92%\r",
      "Progress: 32.96%\r",
      "Progress: 33.0%\r",
      "Progress: 33.04%\r",
      "Progress: 33.08%\r",
      "Progress: 33.12%\r",
      "Progress: 33.16%\r",
      "Progress: 33.2%\r",
      "Progress: 33.24%\r",
      "Progress: 33.28%\r",
      "Progress: 33.32%\r",
      "Progress: 33.36%\r",
      "Progress: 33.39%\r",
      "Progress: 33.43%\r",
      "Progress: 33.47%\r",
      "Progress: 33.51%\r",
      "Progress: 33.55%\r",
      "Progress: 33.59%\r",
      "Progress: 33.63%\r",
      "Progress: 33.67%\r",
      "Progress: 33.71%\r",
      "Progress: 33.75%\r",
      "Progress: 33.79%\r",
      "Progress: 33.83%\r",
      "Progress: 33.87%\r",
      "Progress: 33.91%\r",
      "Progress: 33.94%\r",
      "Progress: 33.98%\r",
      "Progress: 34.02%\r",
      "Progress: 34.06%\r",
      "Progress: 34.1%\r",
      "Progress: 34.14%\r",
      "Progress: 34.18%\r",
      "Progress: 34.22%\r",
      "Progress: 34.26%\r",
      "Progress: 34.3%\r",
      "Progress: 34.34%\r",
      "Progress: 34.38%\r",
      "Progress: 34.42%\r",
      "Progress: 34.46%\r",
      "Progress: 34.49%\r",
      "Progress: 34.53%\r",
      "Progress: 34.57%\r",
      "Progress: 34.61%\r",
      "Progress: 34.65%\r",
      "Progress: 34.69%\r",
      "Progress: 34.73%\r",
      "Progress: 34.77%\r",
      "Progress: 34.81%\r",
      "Progress: 34.85%\r",
      "Progress: 34.89%\r",
      "Progress: 34.93%\r",
      "Progress: 34.97%\r",
      "Progress: 35.01%\r",
      "Progress: 35.04%\r",
      "Progress: 35.08%\r",
      "Progress: 35.12%\r",
      "Progress: 35.16%\r",
      "Progress: 35.2%\r",
      "Progress: 35.24%\r",
      "Progress: 35.28%\r",
      "Progress: 35.32%\r",
      "Progress: 35.36%\r",
      "Progress: 35.4%\r",
      "Progress: 35.44%\r",
      "Progress: 35.48%\r",
      "Progress: 35.52%\r",
      "Progress: 35.56%\r",
      "Progress: 35.59%\r",
      "Progress: 35.63%\r",
      "Progress: 35.67%\r",
      "Progress: 35.71%\r",
      "Progress: 35.75%\r",
      "Progress: 35.79%\r",
      "Progress: 35.83%\r",
      "Progress: 35.87%\r",
      "Progress: 35.91%\r",
      "Progress: 35.95%\r",
      "Progress: 35.99%\r",
      "Progress: 36.03%\r",
      "Progress: 36.07%\r",
      "Progress: 36.11%\r",
      "Progress: 36.14%\r",
      "Progress: 36.18%\r",
      "Progress: 36.22%\r",
      "Progress: 36.26%\r",
      "Progress: 36.3%\r",
      "Progress: 36.34%\r",
      "Progress: 36.38%\r",
      "Progress: 36.42%\r",
      "Progress: 36.46%\r",
      "Progress: 36.5%\r",
      "Progress: 36.54%\r",
      "Progress: 36.58%\r",
      "Progress: 36.62%\r",
      "Progress: 36.66%\r",
      "Progress: 36.69%\r",
      "Progress: 36.73%\r",
      "Progress: 36.77%\r",
      "Progress: 36.81%\r",
      "Progress: 36.85%\r",
      "Progress: 36.89%\r",
      "Progress: 36.93%\r",
      "Progress: 36.97%\r",
      "Progress: 37.01%\r",
      "Progress: 37.05%\r",
      "Progress: 37.09%\r",
      "Progress: 37.13%\r",
      "Progress: 37.17%\r",
      "Progress: 37.21%\r",
      "Progress: 37.24%\r",
      "Progress: 37.28%\r",
      "Progress: 37.32%\r",
      "Progress: 37.36%\r",
      "Progress: 37.4%\r",
      "Progress: 37.44%\r",
      "Progress: 37.48%\r",
      "Progress: 37.52%\r",
      "Progress: 37.56%\r",
      "Progress: 37.6%\r",
      "Progress: 37.64%\r",
      "Progress: 37.68%\r",
      "Progress: 37.72%\r",
      "Progress: 37.76%\r",
      "Progress: 37.79%\r",
      "Progress: 37.83%\r",
      "Progress: 37.87%\r",
      "Progress: 37.91%\r",
      "Progress: 37.95%\r",
      "Progress: 37.99%\r",
      "Progress: 38.03%\r",
      "Progress: 38.07%\r",
      "Progress: 38.11%\r",
      "Progress: 38.15%\r",
      "Progress: 38.19%\r",
      "Progress: 38.23%\r",
      "Progress: 38.27%\r",
      "Progress: 38.31%\r",
      "Progress: 38.34%\r",
      "Progress: 38.38%\r",
      "Progress: 38.42%\r",
      "Progress: 38.46%\r",
      "Progress: 38.5%\r",
      "Progress: 38.54%\r",
      "Progress: 38.58%\r",
      "Progress: 38.62%\r",
      "Progress: 38.66%\r",
      "Progress: 38.7%\r",
      "Progress: 38.74%\r",
      "Progress: 38.78%\r",
      "Progress: 38.82%\r",
      "Progress: 38.86%\r",
      "Progress: 38.89%\r",
      "Progress: 38.93%\r",
      "Progress: 38.97%\r",
      "Progress: 39.01%\r",
      "Progress: 39.05%\r",
      "Progress: 39.09%\r",
      "Progress: 39.13%\r",
      "Progress: 39.17%\r",
      "Progress: 39.21%\r",
      "Progress: 39.25%\r",
      "Progress: 39.29%\r",
      "Progress: 39.33%\r",
      "Progress: 39.37%\r",
      "Progress: 39.41%\r",
      "Progress: 39.44%\r",
      "Progress: 39.48%\r",
      "Progress: 39.52%\r",
      "Progress: 39.56%\r",
      "Progress: 39.6%\r",
      "Progress: 39.64%\r",
      "Progress: 39.68%\r",
      "Progress: 39.72%\r",
      "Progress: 39.76%\r",
      "Progress: 39.8%\r",
      "Progress: 39.84%\r",
      "Progress: 39.88%\r",
      "Progress: 39.92%\r",
      "Progress: 39.96%\r",
      "Progress: 39.99%\r",
      "Progress: 40.03%\r",
      "Progress: 40.07%\r",
      "Progress: 40.11%\r",
      "Progress: 40.15%\r",
      "Progress: 40.19%\r",
      "Progress: 40.23%\r",
      "Progress: 40.27%\r",
      "Progress: 40.31%\r",
      "Progress: 40.35%\r",
      "Progress: 40.39%\r",
      "Progress: 40.43%\r",
      "Progress: 40.47%\r",
      "Progress: 40.51%\r",
      "Progress: 40.54%\r",
      "Progress: 40.58%\r",
      "Progress: 40.62%\r",
      "Progress: 40.66%\r",
      "Progress: 40.7%\r",
      "Progress: 40.74%\r",
      "Progress: 40.78%\r",
      "Progress: 40.82%\r",
      "Progress: 40.86%\r",
      "Progress: 40.9%\r",
      "Progress: 40.94%\r",
      "Progress: 40.98%\r",
      "Progress: 41.02%\r",
      "Progress: 41.06%\r",
      "Progress: 41.09%\r",
      "Progress: 41.13%\r",
      "Progress: 41.17%\r",
      "Progress: 41.21%\r",
      "Progress: 41.25%\r",
      "Progress: 41.29%\r",
      "Progress: 41.33%\r",
      "Progress: 41.37%\r",
      "Progress: 41.41%\r",
      "Progress: 41.45%\r",
      "Progress: 41.49%\r",
      "Progress: 41.53%\r",
      "Progress: 41.57%\r",
      "Progress: 41.61%\r",
      "Progress: 41.64%\r",
      "Progress: 41.68%\r",
      "Progress: 41.72%\r",
      "Progress: 41.76%\r",
      "Progress: 41.8%\r",
      "Progress: 41.84%\r",
      "Progress: 41.88%\r",
      "Progress: 41.92%\r",
      "Progress: 41.96%\r",
      "Progress: 42.0%\r",
      "Progress: 42.04%\r",
      "Progress: 42.08%\r",
      "Progress: 42.12%\r",
      "Progress: 42.16%\r",
      "Progress: 42.19%\r",
      "Progress: 42.23%\r",
      "Progress: 42.27%\r",
      "Progress: 42.31%\r",
      "Progress: 42.35%\r",
      "Progress: 42.39%\r",
      "Progress: 42.43%\r",
      "Progress: 42.47%\r",
      "Progress: 42.51%\r",
      "Progress: 42.55%\r",
      "Progress: 42.59%\r",
      "Progress: 42.63%\r",
      "Progress: 42.67%\r",
      "Progress: 42.71%\r",
      "Progress: 42.74%\r",
      "Progress: 42.78%\r",
      "Progress: 42.82%\r",
      "Progress: 42.86%\r",
      "Progress: 42.9%\r",
      "Progress: 42.94%\r",
      "Progress: 42.98%\r",
      "Progress: 43.02%\r",
      "Progress: 43.06%\r",
      "Progress: 43.1%\r",
      "Progress: 43.14%\r",
      "Progress: 43.18%\r",
      "Progress: 43.22%\r",
      "Progress: 43.26%\r",
      "Progress: 43.29%\r",
      "Progress: 43.33%\r",
      "Progress: 43.37%\r",
      "Progress: 43.41%\r",
      "Progress: 43.45%\r",
      "Progress: 43.49%\r",
      "Progress: 43.53%\r",
      "Progress: 43.57%\r",
      "Progress: 43.61%\r",
      "Progress: 43.65%\r",
      "Progress: 43.69%\r",
      "Progress: 43.73%\r",
      "Progress: 43.77%\r",
      "Progress: 43.81%\r",
      "Progress: 43.85%\r",
      "Progress: 43.88%\r",
      "Progress: 43.92%\r",
      "Progress: 43.96%\r",
      "Progress: 44.0%\r",
      "Progress: 44.04%\r",
      "Progress: 44.08%\r",
      "Progress: 44.12%\r",
      "Progress: 44.16%\r",
      "Progress: 44.2%\r",
      "Progress: 44.24%\r",
      "Progress: 44.28%\r",
      "Progress: 44.32%\r",
      "Progress: 44.36%\r",
      "Progress: 44.4%\r",
      "Progress: 44.43%\r",
      "Progress: 44.47%\r",
      "Progress: 44.51%\r",
      "Progress: 44.55%\r",
      "Progress: 44.59%\r",
      "Progress: 44.63%\r",
      "Progress: 44.67%\r",
      "Progress: 44.71%\r",
      "Progress: 44.75%\r",
      "Progress: 44.79%\r",
      "Progress: 44.83%\r",
      "Progress: 44.87%\r",
      "Progress: 44.91%\r",
      "Progress: 44.95%\r",
      "Progress: 44.98%\r",
      "Progress: 45.02%\r",
      "Progress: 45.06%\r",
      "Progress: 45.1%\r",
      "Progress: 45.14%\r",
      "Progress: 45.18%\r",
      "Progress: 45.22%\r",
      "Progress: 45.26%\r",
      "Progress: 45.3%\r",
      "Progress: 45.34%\r",
      "Progress: 45.38%\r",
      "Progress: 45.42%\r",
      "Progress: 45.46%\r",
      "Progress: 45.5%\r",
      "Progress: 45.53%\r",
      "Progress: 45.57%\r",
      "Progress: 45.61%\r",
      "Progress: 45.65%\r",
      "Progress: 45.69%\r",
      "Progress: 45.73%\r",
      "Progress: 45.77%\r",
      "Progress: 45.81%\r",
      "Progress: 45.85%\r",
      "Progress: 45.89%\r",
      "Progress: 45.93%\r",
      "Progress: 45.97%\r",
      "Progress: 46.01%\r",
      "Progress: 46.05%\r",
      "Progress: 46.08%\r",
      "Progress: 46.12%\r",
      "Progress: 46.16%\r",
      "Progress: 46.2%\r",
      "Progress: 46.24%\r",
      "Progress: 46.28%\r",
      "Progress: 46.32%\r",
      "Progress: 46.36%\r",
      "Progress: 46.4%\r",
      "Progress: 46.44%\r",
      "Progress: 46.48%\r",
      "Progress: 46.52%\r",
      "Progress: 46.56%\r",
      "Progress: 46.6%\r",
      "Progress: 46.63%\r",
      "Progress: 46.67%\r",
      "Progress: 46.71%\r",
      "Progress: 46.75%\r",
      "Progress: 46.79%\r",
      "Progress: 46.83%\r",
      "Progress: 46.87%\r",
      "Progress: 46.91%\r",
      "Progress: 46.95%\r",
      "Progress: 46.99%\r",
      "Progress: 47.03%\r",
      "Progress: 47.07%\r",
      "Progress: 47.11%\r",
      "Progress: 47.15%\r",
      "Progress: 47.18%\r",
      "Progress: 47.22%\r",
      "Progress: 47.26%\r",
      "Progress: 47.3%\r",
      "Progress: 47.34%\r",
      "Progress: 47.38%\r",
      "Progress: 47.42%\r",
      "Progress: 47.46%\r",
      "Progress: 47.5%\r",
      "Progress: 47.54%\r",
      "Progress: 47.58%\r",
      "Progress: 47.62%\r",
      "Progress: 47.66%\r",
      "Progress: 47.7%\r",
      "Progress: 47.73%\r",
      "Progress: 47.77%\r",
      "Progress: 47.81%\r",
      "Progress: 47.85%\r",
      "Progress: 47.89%\r",
      "Progress: 47.93%\r",
      "Progress: 47.97%\r",
      "Progress: 48.01%\r",
      "Progress: 48.05%\r",
      "Progress: 48.09%\r",
      "Progress: 48.13%\r",
      "Progress: 48.17%\r",
      "Progress: 48.21%\r",
      "Progress: 48.25%\r",
      "Progress: 48.28%\r",
      "Progress: 48.32%\r",
      "Progress: 48.36%\r",
      "Progress: 48.4%\r",
      "Progress: 48.44%\r",
      "Progress: 48.48%\r",
      "Progress: 48.52%\r",
      "Progress: 48.56%\r",
      "Progress: 48.6%\r",
      "Progress: 48.64%\r",
      "Progress: 48.68%\r",
      "Progress: 48.72%\r",
      "Progress: 48.76%\r",
      "Progress: 48.8%\r",
      "Progress: 48.83%\r",
      "Progress: 48.87%\r",
      "Progress: 48.91%\r",
      "Progress: 48.95%\r",
      "Progress: 48.99%\r",
      "Progress: 49.03%\r",
      "Progress: 49.07%\r",
      "Progress: 49.11%\r",
      "Progress: 49.15%\r",
      "Progress: 49.19%\r",
      "Progress: 49.23%\r",
      "Progress: 49.27%\r",
      "Progress: 49.31%\r",
      "Progress: 49.35%\r",
      "Progress: 49.38%\r",
      "Progress: 49.42%\r",
      "Progress: 49.46%\r",
      "Progress: 49.5%\r",
      "Progress: 49.54%\r",
      "Progress: 49.58%\r",
      "Progress: 49.62%\r",
      "Progress: 49.66%\r",
      "Progress: 49.7%\r",
      "Progress: 49.74%\r",
      "Progress: 49.78%\r",
      "Progress: 49.82%\r",
      "Progress: 49.86%\r",
      "Progress: 49.9%\r",
      "Progress: 49.93%\r",
      "Progress: 49.97%\r",
      "Progress: 50.01%\r",
      "Progress: 50.05%\r",
      "Progress: 50.09%\r",
      "Progress: 50.13%\r",
      "Progress: 50.17%\r",
      "Progress: 50.21%\r",
      "Progress: 50.25%\r",
      "Progress: 50.29%\r",
      "Progress: 50.33%\r",
      "Progress: 50.37%\r",
      "Progress: 50.41%\r",
      "Progress: 50.45%\r",
      "Progress: 50.48%\r",
      "Progress: 50.52%\r",
      "Progress: 50.56%\r",
      "Progress: 50.6%\r",
      "Progress: 50.64%\r",
      "Progress: 50.68%\r",
      "Progress: 50.72%\r",
      "Progress: 50.76%\r",
      "Progress: 50.8%\r",
      "Progress: 50.84%\r",
      "Progress: 50.88%\r",
      "Progress: 50.92%\r",
      "Progress: 50.96%\r",
      "Progress: 51.0%\r",
      "Progress: 51.03%\r",
      "Progress: 51.07%\r",
      "Progress: 51.11%\r",
      "Progress: 51.15%\r",
      "Progress: 51.19%\r",
      "Progress: 51.23%\r",
      "Progress: 51.27%\r",
      "Progress: 51.31%\r",
      "Progress: 51.35%\r",
      "Progress: 51.39%\r",
      "Progress: 51.43%\r",
      "Progress: 51.47%\r",
      "Progress: 51.51%\r",
      "Progress: 51.55%\r",
      "Progress: 51.58%\r",
      "Progress: 51.62%\r",
      "Progress: 51.66%\r",
      "Progress: 51.7%\r",
      "Progress: 51.74%\r",
      "Progress: 51.78%\r",
      "Progress: 51.82%\r",
      "Progress: 51.86%\r",
      "Progress: 51.9%\r",
      "Progress: 51.94%\r",
      "Progress: 51.98%\r",
      "Progress: 52.02%\r",
      "Progress: 52.06%\r",
      "Progress: 52.1%\r",
      "Progress: 52.13%\r",
      "Progress: 52.17%\r",
      "Progress: 52.21%\r",
      "Progress: 52.25%\r",
      "Progress: 52.29%\r",
      "Progress: 52.33%\r",
      "Progress: 52.37%\r",
      "Progress: 52.41%\r",
      "Progress: 52.45%\r",
      "Progress: 52.49%\r",
      "Progress: 52.53%\r",
      "Progress: 52.57%\r",
      "Progress: 52.61%\r",
      "Progress: 52.65%\r",
      "Progress: 52.68%\r",
      "Progress: 52.72%\r",
      "Progress: 52.76%\r",
      "Progress: 52.8%\r",
      "Progress: 52.84%\r",
      "Progress: 52.88%\r",
      "Progress: 52.92%\r",
      "Progress: 52.96%\r",
      "Progress: 53.0%\r",
      "Progress: 53.04%\r",
      "Progress: 53.08%\r",
      "Progress: 53.12%\r",
      "Progress: 53.16%\r",
      "Progress: 53.2%\r",
      "Progress: 53.23%\r",
      "Progress: 53.27%\r",
      "Progress: 53.31%\r",
      "Progress: 53.35%\r",
      "Progress: 53.39%\r",
      "Progress: 53.43%\r",
      "Progress: 53.47%\r",
      "Progress: 53.51%\r",
      "Progress: 53.55%\r",
      "Progress: 53.59%\r",
      "Progress: 53.63%\r",
      "Progress: 53.67%\r",
      "Progress: 53.71%\r",
      "Progress: 53.75%\r",
      "Progress: 53.78%\r",
      "Progress: 53.82%\r",
      "Progress: 53.86%\r",
      "Progress: 53.9%\r",
      "Progress: 53.94%\r",
      "Progress: 53.98%\r",
      "Progress: 54.02%\r",
      "Progress: 54.06%\r",
      "Progress: 54.1%\r",
      "Progress: 54.14%\r",
      "Progress: 54.18%\r",
      "Progress: 54.22%\r",
      "Progress: 54.26%\r",
      "Progress: 54.3%\r",
      "Progress: 54.33%\r",
      "Progress: 54.37%\r",
      "Progress: 54.41%\r",
      "Progress: 54.45%\r",
      "Progress: 54.49%\r",
      "Progress: 54.53%\r",
      "Progress: 54.57%\r",
      "Progress: 54.61%\r",
      "Progress: 54.65%\r",
      "Progress: 54.69%\r",
      "Progress: 54.73%\r",
      "Progress: 54.77%\r",
      "Progress: 54.81%\r",
      "Progress: 54.85%\r",
      "Progress: 54.88%\r",
      "Progress: 54.92%\r",
      "Progress: 54.96%\r",
      "Progress: 55.0%\r",
      "Progress: 55.04%\r",
      "Progress: 55.08%\r",
      "Progress: 55.12%\r",
      "Progress: 55.16%\r",
      "Progress: 55.2%\r",
      "Progress: 55.24%\r",
      "Progress: 55.28%\r",
      "Progress: 55.32%\r",
      "Progress: 55.36%\r",
      "Progress: 55.4%\r",
      "Progress: 55.43%\r",
      "Progress: 55.47%\r",
      "Progress: 55.51%\r",
      "Progress: 55.55%\r",
      "Progress: 55.59%\r",
      "Progress: 55.63%\r",
      "Progress: 55.67%\r",
      "Progress: 55.71%\r",
      "Progress: 55.75%\r",
      "Progress: 55.79%\r",
      "Progress: 55.83%\r",
      "Progress: 55.87%\r",
      "Progress: 55.91%\r",
      "Progress: 55.95%\r",
      "Progress: 55.98%\r",
      "Progress: 56.02%\r",
      "Progress: 56.06%\r",
      "Progress: 56.1%\r",
      "Progress: 56.14%\r",
      "Progress: 56.18%\r",
      "Progress: 56.22%\r",
      "Progress: 56.26%\r",
      "Progress: 56.3%\r",
      "Progress: 56.34%\r",
      "Progress: 56.38%\r",
      "Progress: 56.42%\r",
      "Progress: 56.46%\r",
      "Progress: 56.5%\r",
      "Progress: 56.53%\r",
      "Progress: 56.57%\r",
      "Progress: 56.61%\r",
      "Progress: 56.65%\r",
      "Progress: 56.69%\r",
      "Progress: 56.73%\r",
      "Progress: 56.77%\r",
      "Progress: 56.81%\r",
      "Progress: 56.85%\r",
      "Progress: 56.89%\r",
      "Progress: 56.93%\r",
      "Progress: 56.97%\r",
      "Progress: 57.01%\r",
      "Progress: 57.05%\r",
      "Progress: 57.08%\r",
      "Progress: 57.12%\r",
      "Progress: 57.16%\r",
      "Progress: 57.2%\r",
      "Progress: 57.24%\r",
      "Progress: 57.28%\r",
      "Progress: 57.32%\r",
      "Progress: 57.36%\r",
      "Progress: 57.4%\r",
      "Progress: 57.44%\r",
      "Progress: 57.48%\r",
      "Progress: 57.52%\r",
      "Progress: 57.56%\r",
      "Progress: 57.6%\r",
      "Progress: 57.63%\r",
      "Progress: 57.67%\r",
      "Progress: 57.71%\r",
      "Progress: 57.75%\r",
      "Progress: 57.79%\r",
      "Progress: 57.83%\r",
      "Progress: 57.87%\r",
      "Progress: 57.91%\r",
      "Progress: 57.95%\r",
      "Progress: 57.99%\r",
      "Progress: 58.03%\r",
      "Progress: 58.07%\r",
      "Progress: 58.11%\r",
      "Progress: 58.15%\r",
      "Progress: 58.18%\r",
      "Progress: 58.22%\r",
      "Progress: 58.26%\r",
      "Progress: 58.3%\r",
      "Progress: 58.34%\r",
      "Progress: 58.38%\r",
      "Progress: 58.42%\r",
      "Progress: 58.46%\r",
      "Progress: 58.5%\r",
      "Progress: 58.54%\r",
      "Progress: 58.58%\r",
      "Progress: 58.62%\r",
      "Progress: 58.66%\r",
      "Progress: 58.7%\r",
      "Progress: 58.74%\r",
      "Progress: 58.77%\r",
      "Progress: 58.81%\r",
      "Progress: 58.85%\r",
      "Progress: 58.89%\r",
      "Progress: 58.93%\r",
      "Progress: 58.97%\r",
      "Progress: 59.01%\r",
      "Progress: 59.05%\r",
      "Progress: 59.09%\r",
      "Progress: 59.13%\r",
      "Progress: 59.17%\r",
      "Progress: 59.21%\r",
      "Progress: 59.25%\r",
      "Progress: 59.29%\r",
      "Progress: 59.32%\r",
      "Progress: 59.36%\r",
      "Progress: 59.4%\r",
      "Progress: 59.44%\r",
      "Progress: 59.48%\r",
      "Progress: 59.52%\r",
      "Progress: 59.56%\r",
      "Progress: 59.6%\r",
      "Progress: 59.64%\r",
      "Progress: 59.68%\r",
      "Progress: 59.72%\r",
      "Progress: 59.76%\r",
      "Progress: 59.8%\r",
      "Progress: 59.84%\r",
      "Progress: 59.87%\r",
      "Progress: 59.91%\r",
      "Progress: 59.95%\r",
      "Progress: 59.99%\r",
      "Progress: 60.03%\r",
      "Progress: 60.07%\r",
      "Progress: 60.11%\r",
      "Progress: 60.15%\r",
      "Progress: 60.19%\r",
      "Progress: 60.23%\r",
      "Progress: 60.27%\r",
      "Progress: 60.31%\r",
      "Progress: 60.35%\r",
      "Progress: 60.39%\r",
      "Progress: 60.42%\r",
      "Progress: 60.46%\r",
      "Progress: 60.5%\r",
      "Progress: 60.54%\r",
      "Progress: 60.58%\r",
      "Progress: 60.62%\r",
      "Progress: 60.66%\r",
      "Progress: 60.7%\r",
      "Progress: 60.74%\r",
      "Progress: 60.78%\r",
      "Progress: 60.82%\r",
      "Progress: 60.86%\r",
      "Progress: 60.9%\r",
      "Progress: 60.94%\r",
      "Progress: 60.97%\r",
      "Progress: 61.01%\r",
      "Progress: 61.05%\r",
      "Progress: 61.09%\r",
      "Progress: 61.13%\r",
      "Progress: 61.17%\r",
      "Progress: 61.21%\r",
      "Progress: 61.25%\r",
      "Progress: 61.29%\r",
      "Progress: 61.33%\r",
      "Progress: 61.37%\r",
      "Progress: 61.41%\r",
      "Progress: 61.45%\r",
      "Progress: 61.49%\r",
      "Progress: 61.52%\r",
      "Progress: 61.56%\r",
      "Progress: 61.6%\r",
      "Progress: 61.64%\r",
      "Progress: 61.68%\r",
      "Progress: 61.72%\r",
      "Progress: 61.76%\r",
      "Progress: 61.8%\r",
      "Progress: 61.84%\r",
      "Progress: 61.88%\r",
      "Progress: 61.92%\r",
      "Progress: 61.96%\r",
      "Progress: 62.0%\r",
      "Progress: 62.04%\r",
      "Progress: 62.07%\r",
      "Progress: 62.11%\r",
      "Progress: 62.15%\r",
      "Progress: 62.19%\r",
      "Progress: 62.23%\r",
      "Progress: 62.27%\r",
      "Progress: 62.31%\r",
      "Progress: 62.35%\r",
      "Progress: 62.39%\r",
      "Progress: 62.43%\r",
      "Progress: 62.47%\r",
      "Progress: 62.51%\r",
      "Progress: 62.55%\r",
      "Progress: 62.59%\r",
      "Progress: 62.62%\r",
      "Progress: 62.66%\r",
      "Progress: 62.7%\r",
      "Progress: 62.74%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 62.78%\r",
      "Progress: 62.82%\r",
      "Progress: 62.86%\r",
      "Progress: 62.9%\r",
      "Progress: 62.94%\r",
      "Progress: 62.98%\r",
      "Progress: 63.02%\r",
      "Progress: 63.06%\r",
      "Progress: 63.1%\r",
      "Progress: 63.14%\r",
      "Progress: 63.17%\r",
      "Progress: 63.21%\r",
      "Progress: 63.25%\r",
      "Progress: 63.29%\r",
      "Progress: 63.33%\r",
      "Progress: 63.37%\r",
      "Progress: 63.41%\r",
      "Progress: 63.45%\r",
      "Progress: 63.49%\r",
      "Progress: 63.53%\r",
      "Progress: 63.57%\r",
      "Progress: 63.61%\r",
      "Progress: 63.65%\r",
      "Progress: 63.69%\r",
      "Progress: 63.72%\r",
      "Progress: 63.76%\r",
      "Progress: 63.8%\r",
      "Progress: 63.84%\r",
      "Progress: 63.88%\r",
      "Progress: 63.92%\r",
      "Progress: 63.96%\r",
      "Progress: 64.0%\r",
      "Progress: 64.04%\r",
      "Progress: 64.08%\r",
      "Progress: 64.12%\r",
      "Progress: 64.16%\r",
      "Progress: 64.2%\r",
      "Progress: 64.24%\r",
      "Progress: 64.27%\r",
      "Progress: 64.31%\r",
      "Progress: 64.35%\r",
      "Progress: 64.39%\r",
      "Progress: 64.43%\r",
      "Progress: 64.47%\r",
      "Progress: 64.51%\r",
      "Progress: 64.55%\r",
      "Progress: 64.59%\r",
      "Progress: 64.63%\r",
      "Progress: 64.67%\r",
      "Progress: 64.71%\r",
      "Progress: 64.75%\r",
      "Progress: 64.79%\r",
      "Progress: 64.82%\r",
      "Progress: 64.86%\r",
      "Progress: 64.9%\r",
      "Progress: 64.94%\r",
      "Progress: 64.98%\r",
      "Progress: 65.02%\r",
      "Progress: 65.06%\r",
      "Progress: 65.1%\r",
      "Progress: 65.14%\r",
      "Progress: 65.18%\r",
      "Progress: 65.22%\r",
      "Progress: 65.26%\r",
      "Progress: 65.3%\r",
      "Progress: 65.34%\r",
      "Progress: 65.37%\r",
      "Progress: 65.41%\r",
      "Progress: 65.45%\r",
      "Progress: 65.49%\r",
      "Progress: 65.53%\r",
      "Progress: 65.57%\r",
      "Progress: 65.61%\r",
      "Progress: 65.65%\r",
      "Progress: 65.69%\r",
      "Progress: 65.73%\r",
      "Progress: 65.77%\r",
      "Progress: 65.81%\r",
      "Progress: 65.85%\r",
      "Progress: 65.89%\r",
      "Progress: 65.92%\r",
      "Progress: 65.96%\r",
      "Progress: 66.0%\r",
      "Progress: 66.04%\r",
      "Progress: 66.08%\r",
      "Progress: 66.12%\r",
      "Progress: 66.16%\r",
      "Progress: 66.2%\r",
      "Progress: 66.24%\r",
      "Progress: 66.28%\r",
      "Progress: 66.32%\r",
      "Progress: 66.36%\r",
      "Progress: 66.4%\r",
      "Progress: 66.44%\r",
      "Progress: 66.47%\r",
      "Progress: 66.51%\r",
      "Progress: 66.55%\r",
      "Progress: 66.59%\r",
      "Progress: 66.63%\r",
      "Progress: 66.67%\r",
      "Progress: 66.71%\r",
      "Progress: 66.75%\r",
      "Progress: 66.79%\r",
      "Progress: 66.83%\r",
      "Progress: 66.87%\r",
      "Progress: 66.91%\r",
      "Progress: 66.95%\r",
      "Progress: 66.99%\r",
      "Progress: 67.02%\r",
      "Progress: 67.06%\r",
      "Progress: 67.1%\r",
      "Progress: 67.14%\r",
      "Progress: 67.18%\r",
      "Progress: 67.22%\r",
      "Progress: 67.26%\r",
      "Progress: 67.3%\r",
      "Progress: 67.34%\r",
      "Progress: 67.38%\r",
      "Progress: 67.42%\r",
      "Progress: 67.46%\r",
      "Progress: 67.5%\r",
      "Progress: 67.54%\r",
      "Progress: 67.57%\r",
      "Progress: 67.61%\r",
      "Progress: 67.65%\r",
      "Progress: 67.69%\r",
      "Progress: 67.73%\r",
      "Progress: 67.77%\r",
      "Progress: 67.81%\r",
      "Progress: 67.85%\r",
      "Progress: 67.89%\r",
      "Progress: 67.93%\r",
      "Progress: 67.97%\r",
      "Progress: 68.01%\r",
      "Progress: 68.05%\r",
      "Progress: 68.09%\r",
      "Progress: 68.12%\r",
      "Progress: 68.16%\r",
      "Progress: 68.2%\r",
      "Progress: 68.24%\r",
      "Progress: 68.28%\r",
      "Progress: 68.32%\r",
      "Progress: 68.36%\r",
      "Progress: 68.4%\r",
      "Progress: 68.44%\r",
      "Progress: 68.48%\r",
      "Progress: 68.52%\r",
      "Progress: 68.56%\r",
      "Progress: 68.6%\r",
      "Progress: 68.64%\r",
      "Progress: 68.67%\r",
      "Progress: 68.71%\r",
      "Progress: 68.75%\r",
      "Progress: 68.79%\r",
      "Progress: 68.83%\r",
      "Progress: 68.87%\r",
      "Progress: 68.91%\r",
      "Progress: 68.95%\r",
      "Progress: 68.99%\r",
      "Progress: 69.03%\r",
      "Progress: 69.07%\r",
      "Progress: 69.11%\r",
      "Progress: 69.15%\r",
      "Progress: 69.19%\r",
      "Progress: 69.22%\r",
      "Progress: 69.26%\r",
      "Progress: 69.3%\r",
      "Progress: 69.34%\r",
      "Progress: 69.38%\r",
      "Progress: 69.42%\r",
      "Progress: 69.46%\r",
      "Progress: 69.5%\r",
      "Progress: 69.54%\r",
      "Progress: 69.58%\r",
      "Progress: 69.62%\r",
      "Progress: 69.66%\r",
      "Progress: 69.7%\r",
      "Progress: 69.74%\r",
      "Progress: 69.77%\r",
      "Progress: 69.81%\r",
      "Progress: 69.85%\r",
      "Progress: 69.89%\r",
      "Progress: 69.93%\r",
      "Progress: 69.97%\r",
      "Progress: 70.01%\r",
      "Progress: 70.05%\r",
      "Progress: 70.09%\r",
      "Progress: 70.13%\r",
      "Progress: 70.17%\r",
      "Progress: 70.21%\r",
      "Progress: 70.25%\r",
      "Progress: 70.29%\r",
      "Progress: 70.32%\r",
      "Progress: 70.36%\r",
      "Progress: 70.4%\r",
      "Progress: 70.44%\r",
      "Progress: 70.48%\r",
      "Progress: 70.52%\r",
      "Progress: 70.56%\r",
      "Progress: 70.6%\r",
      "Progress: 70.64%\r",
      "Progress: 70.68%\r",
      "Progress: 70.72%\r",
      "Progress: 70.76%\r",
      "Progress: 70.8%\r",
      "Progress: 70.84%\r",
      "Progress: 70.87%\r",
      "Progress: 70.91%\r",
      "Progress: 70.95%\r",
      "Progress: 70.99%\r",
      "Progress: 71.03%\r",
      "Progress: 71.07%\r",
      "Progress: 71.11%\r",
      "Progress: 71.15%\r",
      "Progress: 71.19%\r",
      "Progress: 71.23%\r",
      "Progress: 71.27%\r",
      "Progress: 71.31%\r",
      "Progress: 71.35%\r",
      "Progress: 71.39%\r",
      "Progress: 71.42%\r",
      "Progress: 71.46%\r",
      "Progress: 71.5%\r",
      "Progress: 71.54%\r",
      "Progress: 71.58%\r",
      "Progress: 71.62%\r",
      "Progress: 71.66%\r",
      "Progress: 71.7%\r",
      "Progress: 71.74%\r",
      "Progress: 71.78%\r",
      "Progress: 71.82%\r",
      "Progress: 71.86%\r",
      "Progress: 71.9%\r",
      "Progress: 71.94%\r",
      "Progress: 71.97%\r",
      "Progress: 72.01%\r",
      "Progress: 72.05%\r",
      "Progress: 72.09%\r",
      "Progress: 72.13%\r",
      "Progress: 72.17%\r",
      "Progress: 72.21%\r",
      "Progress: 72.25%\r",
      "Progress: 72.29%\r",
      "Progress: 72.33%\r",
      "Progress: 72.37%\r",
      "Progress: 72.41%\r",
      "Progress: 72.45%\r",
      "Progress: 72.49%\r",
      "Progress: 72.52%\r",
      "Progress: 72.56%\r",
      "Progress: 72.6%\r",
      "Progress: 72.64%\r",
      "Progress: 72.68%\r",
      "Progress: 72.72%\r",
      "Progress: 72.76%\r",
      "Progress: 72.8%\r",
      "Progress: 72.84%\r",
      "Progress: 72.88%\r",
      "Progress: 72.92%\r",
      "Progress: 72.96%\r",
      "Progress: 73.0%\r",
      "Progress: 73.04%\r",
      "Progress: 73.08%\r",
      "Progress: 73.11%\r",
      "Progress: 73.15%\r",
      "Progress: 73.19%\r",
      "Progress: 73.23%\r",
      "Progress: 73.27%\r",
      "Progress: 73.31%\r",
      "Progress: 73.35%\r",
      "Progress: 73.39%\r",
      "Progress: 73.43%\r",
      "Progress: 73.47%\r",
      "Progress: 73.51%\r",
      "Progress: 73.55%\r",
      "Progress: 73.59%\r",
      "Progress: 73.63%\r",
      "Progress: 73.66%\r",
      "Progress: 73.7%\r",
      "Progress: 73.74%\r",
      "Progress: 73.78%\r",
      "Progress: 73.82%\r",
      "Progress: 73.86%\r",
      "Progress: 73.9%\r",
      "Progress: 73.94%\r",
      "Progress: 73.98%\r",
      "Progress: 74.02%\r",
      "Progress: 74.06%\r",
      "Progress: 74.1%\r",
      "Progress: 74.14%\r",
      "Progress: 74.18%\r",
      "Progress: 74.21%\r",
      "Progress: 74.25%\r",
      "Progress: 74.29%\r",
      "Progress: 74.33%\r",
      "Progress: 74.37%\r",
      "Progress: 74.41%\r",
      "Progress: 74.45%\r",
      "Progress: 74.49%\r",
      "Progress: 74.53%\r",
      "Progress: 74.57%\r",
      "Progress: 74.61%\r",
      "Progress: 74.65%\r",
      "Progress: 74.69%\r",
      "Progress: 74.73%\r",
      "Progress: 74.76%\r",
      "Progress: 74.8%\r",
      "Progress: 74.84%\r",
      "Progress: 74.88%\r",
      "Progress: 74.92%\r",
      "Progress: 74.96%\r",
      "Progress: 75.0%\r",
      "Progress: 75.04%\r",
      "Progress: 75.08%\r",
      "Progress: 75.12%\r",
      "Progress: 75.16%\r",
      "Progress: 75.2%\r",
      "Progress: 75.24%\r",
      "Progress: 75.28%\r",
      "Progress: 75.31%\r",
      "Progress: 75.35%\r",
      "Progress: 75.39%\r",
      "Progress: 75.43%\r",
      "Progress: 75.47%\r",
      "Progress: 75.51%\r",
      "Progress: 75.55%\r",
      "Progress: 75.59%\r",
      "Progress: 75.63%\r",
      "Progress: 75.67%\r",
      "Progress: 75.71%\r",
      "Progress: 75.75%\r",
      "Progress: 75.79%\r",
      "Progress: 75.83%\r",
      "Progress: 75.86%\r",
      "Progress: 75.9%\r",
      "Progress: 75.94%\r",
      "Progress: 75.98%\r",
      "Progress: 76.02%\r",
      "Progress: 76.06%\r",
      "Progress: 76.1%\r",
      "Progress: 76.14%\r",
      "Progress: 76.18%\r",
      "Progress: 76.22%\r",
      "Progress: 76.26%\r",
      "Progress: 76.3%\r",
      "Progress: 76.34%\r",
      "Progress: 76.38%\r",
      "Progress: 76.41%\r",
      "Progress: 76.45%\r",
      "Progress: 76.49%\r",
      "Progress: 76.53%\r",
      "Progress: 76.57%\r",
      "Progress: 76.61%\r",
      "Progress: 76.65%\r",
      "Progress: 76.69%\r",
      "Progress: 76.73%\r",
      "Progress: 76.77%\r",
      "Progress: 76.81%\r",
      "Progress: 76.85%\r",
      "Progress: 76.89%\r",
      "Progress: 76.93%\r",
      "Progress: 76.96%\r",
      "Progress: 77.0%\r",
      "Progress: 77.04%\r",
      "Progress: 77.08%\r",
      "Progress: 77.12%\r",
      "Progress: 77.16%\r",
      "Progress: 77.2%\r",
      "Progress: 77.24%\r",
      "Progress: 77.28%\r",
      "Progress: 77.32%\r",
      "Progress: 77.36%\r",
      "Progress: 77.4%\r",
      "Progress: 77.44%\r",
      "Progress: 77.48%\r",
      "Progress: 77.51%\r",
      "Progress: 77.55%\r",
      "Progress: 77.59%\r",
      "Progress: 77.63%\r",
      "Progress: 77.67%\r",
      "Progress: 77.71%\r",
      "Progress: 77.75%\r",
      "Progress: 77.79%\r",
      "Progress: 77.83%\r",
      "Progress: 77.87%\r",
      "Progress: 77.91%\r",
      "Progress: 77.95%\r",
      "Progress: 77.99%\r",
      "Progress: 78.03%\r",
      "Progress: 78.06%\r",
      "Progress: 78.1%\r",
      "Progress: 78.14%\r",
      "Progress: 78.18%\r",
      "Progress: 78.22%\r",
      "Progress: 78.26%\r",
      "Progress: 78.3%\r",
      "Progress: 78.34%\r",
      "Progress: 78.38%\r",
      "Progress: 78.42%\r",
      "Progress: 78.46%\r",
      "Progress: 78.5%\r",
      "Progress: 78.54%\r",
      "Progress: 78.58%\r",
      "Progress: 78.61%\r",
      "Progress: 78.65%\r",
      "Progress: 78.69%\r",
      "Progress: 78.73%\r",
      "Progress: 78.77%\r",
      "Progress: 78.81%\r",
      "Progress: 78.85%\r",
      "Progress: 78.89%\r",
      "Progress: 78.93%\r",
      "Progress: 78.97%\r",
      "Progress: 79.01%\r",
      "Progress: 79.05%\r",
      "Progress: 79.09%\r",
      "Progress: 79.13%\r",
      "Progress: 79.16%\r",
      "Progress: 79.2%\r",
      "Progress: 79.24%\r",
      "Progress: 79.28%\r",
      "Progress: 79.32%\r",
      "Progress: 79.36%\r",
      "Progress: 79.4%\r",
      "Progress: 79.44%\r",
      "Progress: 79.48%\r",
      "Progress: 79.52%\r",
      "Progress: 79.56%\r",
      "Progress: 79.6%\r",
      "Progress: 79.64%\r",
      "Progress: 79.68%\r",
      "Progress: 79.71%\r",
      "Progress: 79.75%\r",
      "Progress: 79.79%\r",
      "Progress: 79.83%\r",
      "Progress: 79.87%\r",
      "Progress: 79.91%\r",
      "Progress: 79.95%\r",
      "Progress: 79.99%\r",
      "Progress: 80.03%\r",
      "Progress: 80.07%\r",
      "Progress: 80.11%\r",
      "Progress: 80.15%\r",
      "Progress: 80.19%\r",
      "Progress: 80.23%\r",
      "Progress: 80.26%\r",
      "Progress: 80.3%\r",
      "Progress: 80.34%\r",
      "Progress: 80.38%\r",
      "Progress: 80.42%\r",
      "Progress: 80.46%\r",
      "Progress: 80.5%\r",
      "Progress: 80.54%\r",
      "Progress: 80.58%\r",
      "Progress: 80.62%\r",
      "Progress: 80.66%\r",
      "Progress: 80.7%\r",
      "Progress: 80.74%\r",
      "Progress: 80.78%\r",
      "Progress: 80.81%\r",
      "Progress: 80.85%\r",
      "Progress: 80.89%\r",
      "Progress: 80.93%\r",
      "Progress: 80.97%\r",
      "Progress: 81.01%\r",
      "Progress: 81.05%\r",
      "Progress: 81.09%\r",
      "Progress: 81.13%\r",
      "Progress: 81.17%\r",
      "Progress: 81.21%\r",
      "Progress: 81.25%\r",
      "Progress: 81.29%\r",
      "Progress: 81.33%\r",
      "Progress: 81.36%\r",
      "Progress: 81.4%\r",
      "Progress: 81.44%\r",
      "Progress: 81.48%\r",
      "Progress: 81.52%\r",
      "Progress: 81.56%\r",
      "Progress: 81.6%\r",
      "Progress: 81.64%\r",
      "Progress: 81.68%\r",
      "Progress: 81.72%\r",
      "Progress: 81.76%\r",
      "Progress: 81.8%\r",
      "Progress: 81.84%\r",
      "Progress: 81.88%\r",
      "Progress: 81.91%\r",
      "Progress: 81.95%\r",
      "Progress: 81.99%\r",
      "Progress: 82.03%\r",
      "Progress: 82.07%\r",
      "Progress: 82.11%\r",
      "Progress: 82.15%\r",
      "Progress: 82.19%\r",
      "Progress: 82.23%\r",
      "Progress: 82.27%\r",
      "Progress: 82.31%\r",
      "Progress: 82.35%\r",
      "Progress: 82.39%\r",
      "Progress: 82.43%\r",
      "Progress: 82.46%\r",
      "Progress: 82.5%\r",
      "Progress: 82.54%\r",
      "Progress: 82.58%\r",
      "Progress: 82.62%\r",
      "Progress: 82.66%\r",
      "Progress: 82.7%\r",
      "Progress: 82.74%\r",
      "Progress: 82.78%\r",
      "Progress: 82.82%\r",
      "Progress: 82.86%\r",
      "Progress: 82.9%\r",
      "Progress: 82.94%\r",
      "Progress: 82.98%\r",
      "Progress: 83.01%\r",
      "Progress: 83.05%\r",
      "Progress: 83.09%\r",
      "Progress: 83.13%\r",
      "Progress: 83.17%\r",
      "Progress: 83.21%\r",
      "Progress: 83.25%\r",
      "Progress: 83.29%\r",
      "Progress: 83.33%\r",
      "Progress: 83.37%\r",
      "Progress: 83.41%\r",
      "Progress: 83.45%\r",
      "Progress: 83.49%\r",
      "Progress: 83.53%\r",
      "Progress: 83.56%\r",
      "Progress: 83.6%\r",
      "Progress: 83.64%\r",
      "Progress: 83.68%\r",
      "Progress: 83.72%\r",
      "Progress: 83.76%\r",
      "Progress: 83.8%\r",
      "Progress: 83.84%\r",
      "Progress: 83.88%\r",
      "Progress: 83.92%\r",
      "Progress: 83.96%\r",
      "Progress: 84.0%\r",
      "Progress: 84.04%\r",
      "Progress: 84.08%\r",
      "Progress: 84.11%\r",
      "Progress: 84.15%\r",
      "Progress: 84.19%\r",
      "Progress: 84.23%\r",
      "Progress: 84.27%\r",
      "Progress: 84.31%\r",
      "Progress: 84.35%\r",
      "Progress: 84.39%\r",
      "Progress: 84.43%\r",
      "Progress: 84.47%\r",
      "Progress: 84.51%\r",
      "Progress: 84.55%\r",
      "Progress: 84.59%\r",
      "Progress: 84.63%\r",
      "Progress: 84.66%\r",
      "Progress: 84.7%\r",
      "Progress: 84.74%\r",
      "Progress: 84.78%\r",
      "Progress: 84.82%\r",
      "Progress: 84.86%\r",
      "Progress: 84.9%\r",
      "Progress: 84.94%\r",
      "Progress: 84.98%\r",
      "Progress: 85.02%\r",
      "Progress: 85.06%\r",
      "Progress: 85.1%\r",
      "Progress: 85.14%\r",
      "Progress: 85.18%\r",
      "Progress: 85.21%\r",
      "Progress: 85.25%\r",
      "Progress: 85.29%\r",
      "Progress: 85.33%\r",
      "Progress: 85.37%\r",
      "Progress: 85.41%\r",
      "Progress: 85.45%\r",
      "Progress: 85.49%\r",
      "Progress: 85.53%\r",
      "Progress: 85.57%\r",
      "Progress: 85.61%\r",
      "Progress: 85.65%\r",
      "Progress: 85.69%\r",
      "Progress: 85.73%\r",
      "Progress: 85.76%\r",
      "Progress: 85.8%\r",
      "Progress: 85.84%\r",
      "Progress: 85.88%\r",
      "Progress: 85.92%\r",
      "Progress: 85.96%\r",
      "Progress: 86.0%\r",
      "Progress: 86.04%\r",
      "Progress: 86.08%\r",
      "Progress: 86.12%\r",
      "Progress: 86.16%\r",
      "Progress: 86.2%\r",
      "Progress: 86.24%\r",
      "Progress: 86.28%\r",
      "Progress: 86.31%\r",
      "Progress: 86.35%\r",
      "Progress: 86.39%\r",
      "Progress: 86.43%\r",
      "Progress: 86.47%\r",
      "Progress: 86.51%\r",
      "Progress: 86.55%\r",
      "Progress: 86.59%\r",
      "Progress: 86.63%\r",
      "Progress: 86.67%\r",
      "Progress: 86.71%\r",
      "Progress: 86.75%\r",
      "Progress: 86.79%\r",
      "Progress: 86.83%\r",
      "Progress: 86.86%\r",
      "Progress: 86.9%\r",
      "Progress: 86.94%\r",
      "Progress: 86.98%\r",
      "Progress: 87.02%\r",
      "Progress: 87.06%\r",
      "Progress: 87.1%\r",
      "Progress: 87.14%\r",
      "Progress: 87.18%\r",
      "Progress: 87.22%\r",
      "Progress: 87.26%\r",
      "Progress: 87.3%\r",
      "Progress: 87.34%\r",
      "Progress: 87.38%\r",
      "Progress: 87.41%\r",
      "Progress: 87.45%\r",
      "Progress: 87.49%\r",
      "Progress: 87.53%\r",
      "Progress: 87.57%\r",
      "Progress: 87.61%\r",
      "Progress: 87.65%\r",
      "Progress: 87.69%\r",
      "Progress: 87.73%\r",
      "Progress: 87.77%\r",
      "Progress: 87.81%\r",
      "Progress: 87.85%\r",
      "Progress: 87.89%\r",
      "Progress: 87.93%\r",
      "Progress: 87.97%\r",
      "Progress: 88.0%\r",
      "Progress: 88.04%\r",
      "Progress: 88.08%\r",
      "Progress: 88.12%\r",
      "Progress: 88.16%\r",
      "Progress: 88.2%\r",
      "Progress: 88.24%\r",
      "Progress: 88.28%\r",
      "Progress: 88.32%\r",
      "Progress: 88.36%\r",
      "Progress: 88.4%\r",
      "Progress: 88.44%\r",
      "Progress: 88.48%\r",
      "Progress: 88.52%\r",
      "Progress: 88.55%\r",
      "Progress: 88.59%\r",
      "Progress: 88.63%\r",
      "Progress: 88.67%\r",
      "Progress: 88.71%\r",
      "Progress: 88.75%\r",
      "Progress: 88.79%\r",
      "Progress: 88.83%\r",
      "Progress: 88.87%\r",
      "Progress: 88.91%\r",
      "Progress: 88.95%\r",
      "Progress: 88.99%\r",
      "Progress: 89.03%\r",
      "Progress: 89.07%\r",
      "Progress: 89.1%\r",
      "Progress: 89.14%\r",
      "Progress: 89.18%\r",
      "Progress: 89.22%\r",
      "Progress: 89.26%\r",
      "Progress: 89.3%\r",
      "Progress: 89.34%\r",
      "Progress: 89.38%\r",
      "Progress: 89.42%\r",
      "Progress: 89.46%\r",
      "Progress: 89.5%\r",
      "Progress: 89.54%\r",
      "Progress: 89.58%\r",
      "Progress: 89.62%\r",
      "Progress: 89.65%\r",
      "Progress: 89.69%\r",
      "Progress: 89.73%\r",
      "Progress: 89.77%\r",
      "Progress: 89.81%\r",
      "Progress: 89.85%\r",
      "Progress: 89.89%\r",
      "Progress: 89.93%\r",
      "Progress: 89.97%\r",
      "Progress: 90.01%\r",
      "Progress: 90.05%\r",
      "Progress: 90.09%\r",
      "Progress: 90.13%\r",
      "Progress: 90.17%\r",
      "Progress: 90.2%\r",
      "Progress: 90.24%\r",
      "Progress: 90.28%\r",
      "Progress: 90.32%\r",
      "Progress: 90.36%\r",
      "Progress: 90.4%\r",
      "Progress: 90.44%\r",
      "Progress: 90.48%\r",
      "Progress: 90.52%\r",
      "Progress: 90.56%\r",
      "Progress: 90.6%\r",
      "Progress: 90.64%\r",
      "Progress: 90.68%\r",
      "Progress: 90.72%\r",
      "Progress: 90.75%\r",
      "Progress: 90.79%\r",
      "Progress: 90.83%\r",
      "Progress: 90.87%\r",
      "Progress: 90.91%\r",
      "Progress: 90.95%\r",
      "Progress: 90.99%\r",
      "Progress: 91.03%\r",
      "Progress: 91.07%\r",
      "Progress: 91.11%\r",
      "Progress: 91.15%\r",
      "Progress: 91.19%\r",
      "Progress: 91.23%\r",
      "Progress: 91.27%\r",
      "Progress: 91.3%\r",
      "Progress: 91.34%\r",
      "Progress: 91.38%\r",
      "Progress: 91.42%\r",
      "Progress: 91.46%\r",
      "Progress: 91.5%\r",
      "Progress: 91.54%\r",
      "Progress: 91.58%\r",
      "Progress: 91.62%\r",
      "Progress: 91.66%\r",
      "Progress: 91.7%\r",
      "Progress: 91.74%\r",
      "Progress: 91.78%\r",
      "Progress: 91.82%\r",
      "Progress: 91.85%\r",
      "Progress: 91.89%\r",
      "Progress: 91.93%\r",
      "Progress: 91.97%\r",
      "Progress: 92.01%\r",
      "Progress: 92.05%\r",
      "Progress: 92.09%\r",
      "Progress: 92.13%\r",
      "Progress: 92.17%\r",
      "Progress: 92.21%\r",
      "Progress: 92.25%\r",
      "Progress: 92.29%\r",
      "Progress: 92.33%\r",
      "Progress: 92.37%\r",
      "Progress: 92.4%\r",
      "Progress: 92.44%\r",
      "Progress: 92.48%\r",
      "Progress: 92.52%\r",
      "Progress: 92.56%\r",
      "Progress: 92.6%\r",
      "Progress: 92.64%\r",
      "Progress: 92.68%\r",
      "Progress: 92.72%\r",
      "Progress: 92.76%\r",
      "Progress: 92.8%\r",
      "Progress: 92.84%\r",
      "Progress: 92.88%\r",
      "Progress: 92.92%\r",
      "Progress: 92.95%\r",
      "Progress: 92.99%\r",
      "Progress: 93.03%\r",
      "Progress: 93.07%\r",
      "Progress: 93.11%\r",
      "Progress: 93.15%\r",
      "Progress: 93.19%\r",
      "Progress: 93.23%\r",
      "Progress: 93.27%\r",
      "Progress: 93.31%\r",
      "Progress: 93.35%\r",
      "Progress: 93.39%\r",
      "Progress: 93.43%\r",
      "Progress: 93.47%\r",
      "Progress: 93.5%\r",
      "Progress: 93.54%\r",
      "Progress: 93.58%\r",
      "Progress: 93.62%\r",
      "Progress: 93.66%\r",
      "Progress: 93.7%\r",
      "Progress: 93.74%\r",
      "Progress: 93.78%\r",
      "Progress: 93.82%\r",
      "Progress: 93.86%\r",
      "Progress: 93.9%\r",
      "Progress: 93.94%\r",
      "Progress: 93.98%\r",
      "Progress: 94.02%\r",
      "Progress: 94.05%\r",
      "Progress: 94.09%\r",
      "Progress: 94.13%\r",
      "Progress: 94.17%\r",
      "Progress: 94.21%\r",
      "Progress: 94.25%\r",
      "Progress: 94.29%\r",
      "Progress: 94.33%\r",
      "Progress: 94.37%\r",
      "Progress: 94.41%\r",
      "Progress: 94.45%\r",
      "Progress: 94.49%\r",
      "Progress: 94.53%\r",
      "Progress: 94.57%\r",
      "Progress: 94.6%\r",
      "Progress: 94.64%\r",
      "Progress: 94.68%\r",
      "Progress: 94.72%\r",
      "Progress: 94.76%\r",
      "Progress: 94.8%\r",
      "Progress: 94.84%\r",
      "Progress: 94.88%\r",
      "Progress: 94.92%\r",
      "Progress: 94.96%\r",
      "Progress: 95.0%\r",
      "Progress: 95.04%\r",
      "Progress: 95.08%\r",
      "Progress: 95.12%\r",
      "Progress: 95.15%\r",
      "Progress: 95.19%\r",
      "Progress: 95.23%\r",
      "Progress: 95.27%\r",
      "Progress: 95.31%\r",
      "Progress: 95.35%\r",
      "Progress: 95.39%\r",
      "Progress: 95.43%\r",
      "Progress: 95.47%\r",
      "Progress: 95.51%\r",
      "Progress: 95.55%\r",
      "Progress: 95.59%\r",
      "Progress: 95.63%\r",
      "Progress: 95.67%\r",
      "Progress: 95.7%\r",
      "Progress: 95.74%\r",
      "Progress: 95.78%\r",
      "Progress: 95.82%\r",
      "Progress: 95.86%\r",
      "Progress: 95.9%\r",
      "Progress: 95.94%\r",
      "Progress: 95.98%\r",
      "Progress: 96.02%\r",
      "Progress: 96.06%\r",
      "Progress: 96.1%\r",
      "Progress: 96.14%\r",
      "Progress: 96.18%\r",
      "Progress: 96.22%\r",
      "Progress: 96.25%\r",
      "Progress: 96.29%\r",
      "Progress: 96.33%\r",
      "Progress: 96.37%\r",
      "Progress: 96.41%\r",
      "Progress: 96.45%\r",
      "Progress: 96.49%\r",
      "Progress: 96.53%\r",
      "Progress: 96.57%\r",
      "Progress: 96.61%\r",
      "Progress: 96.65%\r",
      "Progress: 96.69%\r",
      "Progress: 96.73%\r",
      "Progress: 96.77%\r",
      "Progress: 96.8%\r",
      "Progress: 96.84%\r",
      "Progress: 96.88%\r",
      "Progress: 96.92%\r",
      "Progress: 96.96%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 99.99%\n",
      "(254532, 1) (254532, 14309)\n"
     ]
    }
   ],
   "source": [
    "# preprocess the dataset to get training data\n",
    "\n",
    "max_input_len = 1\n",
    "step = 1\n",
    "x = []\n",
    "y = []\n",
    "for i in range(0, len(english_literature_tokens) - max_input_len, step):\n",
    "    if i % 100 == 0:\n",
    "        print(\"Progress: {0}%\".format(round(i / len(english_literature_tokens) * 100, 2)), end=\"\\r\")\n",
    "    curr_words = english_literature_tokens[i:i + max_input_len]\n",
    "    x.append([word2index.get(curr_word, 0) for curr_word in curr_words])\n",
    "    next_word = english_literature_tokens[i + max_input_len]\n",
    "    y.append(word2index.get(next_word, 0))\n",
    "X = np.array(x)\n",
    "Y = to_categorical(y, vocabulary_size)\n",
    "print(\"\")\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1, 40)             572360    \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 500)               270500    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 14309)             7168809   \n",
      "=================================================================\n",
      "Total params: 8,011,669\n",
      "Trainable params: 8,011,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_elman = Sequential()\n",
    "model_elman.add(Embedding(vocabulary_size, 40, input_length=max_input_len))\n",
    "model_elman.add(SimpleRNN(units=500, activation='sigmoid'))\n",
    "model_elman.add(Dense(vocabulary_size, activation='softmax'))\n",
    "\n",
    "model_elman.summary()\n",
    "model_elman.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7479196750477196282\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6700198133\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15660811588422128530\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Epoch 1/20\n",
      "254532/254532 [==============================] - 42s 164us/sample - loss: 6.4883 - acc: 0.0984\n",
      "Epoch 2/20\n",
      "254532/254532 [==============================] - 41s 163us/sample - loss: 5.9161 - acc: 0.1298\n",
      "Epoch 3/20\n",
      "254532/254532 [==============================] - 41s 162us/sample - loss: 5.6417 - acc: 0.1428\n",
      "Epoch 4/20\n",
      "254532/254532 [==============================] - 41s 162us/sample - loss: 5.4709 - acc: 0.1528\n",
      "Epoch 5/20\n",
      "254532/254532 [==============================] - 41s 162us/sample - loss: 5.3443 - acc: 0.1605\n",
      "Epoch 6/20\n",
      "254532/254532 [==============================] - 41s 162us/sample - loss: 5.2401 - acc: 0.1658\n",
      "Epoch 7/20\n",
      "254532/254532 [==============================] - 41s 163us/sample - loss: 5.1517 - acc: 0.1699\n",
      "Epoch 8/20\n",
      "254532/254532 [==============================] - 41s 162us/sample - loss: 5.0740 - acc: 0.1732A: 1s - loss: 5.0\n",
      "Epoch 9/20\n",
      "254532/254532 [==============================] - 41s 162us/sample - loss: 5.0008 - acc: 0.1758\n",
      "Epoch 10/20\n",
      "254532/254532 [==============================] - 42s 163us/sample - loss: 4.9328 - acc: 0.1784\n",
      "Epoch 11/20\n",
      "254532/254532 [==============================] - 42s 163us/sample - loss: 4.8707 - acc: 0.1806\n",
      "Epoch 12/20\n",
      "254532/254532 [==============================] - 41s 163us/sample - loss: 4.8129 - acc: 0.1822\n",
      "Epoch 13/20\n",
      "254532/254532 [==============================] - 41s 163us/sample - loss: 4.7597 - acc: 0.1832\n",
      "Epoch 14/20\n",
      "254532/254532 [==============================] - 41s 163us/sample - loss: 4.7094 - acc: 0.1844 - loss: 4.7125 - acc: 0.1 - ETA: \n",
      "Epoch 15/20\n",
      "254532/254532 [==============================] - 41s 163us/sample - loss: 4.6614 - acc: 0.1859\n",
      "Epoch 16/20\n",
      "254532/254532 [==============================] - 41s 163us/sample - loss: 4.6119 - acc: 0.1871 - loss: 4.6 - ETA: 1s - \n",
      "Epoch 17/20\n",
      "254532/254532 [==============================] - 42s 163us/sample - loss: 4.5663 - acc: 0.1883\n",
      "Epoch 18/20\n",
      "254532/254532 [==============================] - 41s 163us/sample - loss: 4.5296 - acc: 0.1888\n",
      "Epoch 19/20\n",
      "254532/254532 [==============================] - 42s 164us/sample - loss: 4.4926 - acc: 0.1895\n",
      "Epoch 20/20\n",
      "254532/254532 [==============================] - 42s 163us/sample - loss: 4.4613 - acc: 0.1900 - loss:\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())\n",
    "filepath = \"./model_elman.pth\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, save_weights_only=False)\n",
    "elman_training_history = model_elman.fit(\n",
    "    X,\n",
    "    Y,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=[checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' backpropagation through time algorithm\n",
    "'''\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# prepare sentence sequences of the dataset\n",
    "english_literature_sentences = sent_tokenize(english_literature_text)\n",
    "english_literature_sentences_seq = []\n",
    "english_literature_sentences_length = []\n",
    "max_length = 40\n",
    "for sentence in english_literature_sentences:\n",
    "    tmp_tokens = word_tokenize(sentence)\n",
    "    if len(tmp_tokens) > max_length:\n",
    "        tmp_tokens = tmp_tokens[:max_length]\n",
    "    for i in range(1, len(tmp_tokens)):\n",
    "        # 1-of-N encoding\n",
    "        tmp_seq = tmp_tokens[:i+1]\n",
    "        tmp_seq_encoded = []\n",
    "        for token in tmp_seq:\n",
    "            tmp_seq_encoded.append(word2index[token])\n",
    "        english_literature_sentences_seq.append(tmp_seq_encoded)\n",
    "        english_literature_sentences_length.append(len(tmp_seq_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences 210783\n",
      "mean sentence length 14.081591020148684\n",
      "max sentence length 40\n"
     ]
    }
   ],
   "source": [
    "print('number of sequences', len(english_literature_sentences_seq))\n",
    "print('mean sentence length', sum(english_literature_sentences_length) / len(english_literature_sentences_length))\n",
    "max_sentence_length = max(english_literature_sentences_length)\n",
    "print('max sentence length', max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input and target data for training the model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "english_literature_sentences_seq = pad_sequences(english_literature_sentences_seq, maxlen=max_sentence_length, padding='pre')\n",
    "english_literature_sentences_seq = np.array(english_literature_sentences_seq)\n",
    "x_BTT = english_literature_sentences_seq[:, :-1]\n",
    "\n",
    "y_BTT = to_categorical(english_literature_sentences_seq[:, -1], vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 39, 500)           7154500   \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 14309)             7168809   \n",
      "=================================================================\n",
      "Total params: 14,823,809\n",
      "Trainable params: 14,823,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the network with BTT\n",
    "model_BTT = Sequential()\n",
    "model_BTT.add(Embedding(vocabulary_size, 500, input_length=max_sentence_length-1))\n",
    "model_BTT.add(SimpleRNN(units=500, activation='sigmoid'))\n",
    "model_BTT.add(Dense(vocabulary_size, activation='softmax'))\n",
    "\n",
    "model_BTT.summary()\n",
    "model_BTT.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9439319799527297354\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6700198133\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10997302691272896303\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Epoch 1/60\n",
      "210783/210783 [==============================] - 79s 375us/sample - loss: 5.8948 - acc: 0.1356 - lo\n",
      "Epoch 2/60\n",
      "210783/210783 [==============================] - 78s 371us/sample - loss: 5.1529 - acc: 0.1712\n",
      "Epoch 3/60\n",
      "210783/210783 [==============================] - 78s 370us/sample - loss: 4.7894 - acc: 0.1909\n",
      "Epoch 4/60\n",
      "210783/210783 [==============================] - 77s 364us/sample - loss: 4.4633 - acc: 0.2063\n",
      "Epoch 5/60\n",
      "210783/210783 [==============================] - 76s 360us/sample - loss: 4.1465 - acc: 0.2204\n",
      "Epoch 6/60\n",
      "210783/210783 [==============================] - 76s 360us/sample - loss: 3.8386 - acc: 0.2394\n",
      "Epoch 7/60\n",
      "210783/210783 [==============================] - 74s 350us/sample - loss: 3.5553 - acc: 0.2707\n",
      "Epoch 8/60\n",
      "210783/210783 [==============================] - 74s 351us/sample - loss: 3.3101 - acc: 0.3058\n",
      "Epoch 9/60\n",
      "210783/210783 [==============================] - 74s 351us/sample - loss: 3.1023 - acc: 0.3391\n",
      "Epoch 10/60\n",
      "210783/210783 [==============================] - 76s 359us/sample - loss: 2.9201 - acc: 0.3698\n",
      "Epoch 11/60\n",
      "210783/210783 [==============================] - 75s 358us/sample - loss: 2.7664 - acc: 0.3960\n",
      "Epoch 12/60\n",
      "210783/210783 [==============================] - 76s 359us/sample - loss: 2.6275 - acc: 0.4214\n",
      "Epoch 13/60\n",
      "210783/210783 [==============================] - 78s 369us/sample - loss: 2.5408 - acc: 0.4365\n",
      "Epoch 14/60\n",
      "210783/210783 [==============================] - 72s 344us/sample - loss: 2.4382 - acc: 0.4559\n",
      "Epoch 15/60\n",
      "210783/210783 [==============================] - 75s 355us/sample - loss: 2.3030 - acc: 0.4826\n",
      "Epoch 16/60\n",
      "210783/210783 [==============================] - 76s 359us/sample - loss: 2.1813 - acc: 0.5073\n",
      "Epoch 17/60\n",
      "210783/210783 [==============================] - 76s 359us/sample - loss: 2.0648 - acc: 0.5309\n",
      "Epoch 18/60\n",
      "210783/210783 [==============================] - 76s 359us/sample - loss: 1.9492 - acc: 0.5556\n",
      "Epoch 19/60\n",
      "210783/210783 [==============================] - 76s 360us/sample - loss: 1.8569 - acc: 0.5747\n",
      "Epoch 20/60\n",
      "210783/210783 [==============================] - 76s 359us/sample - loss: 1.7684 - acc: 0.5947\n",
      "Epoch 21/60\n",
      "210783/210783 [==============================] - 77s 364us/sample - loss: 1.6724 - acc: 0.6165\n",
      "Epoch 22/60\n",
      "210783/210783 [==============================] - 78s 370us/sample - loss: 1.5815 - acc: 0.6375\n",
      "Epoch 23/60\n",
      "210783/210783 [==============================] - 76s 360us/sample - loss: 1.5038 - acc: 0.6556\n",
      "Epoch 24/60\n",
      "210783/210783 [==============================] - 73s 348us/sample - loss: 1.4275 - acc: 0.6727 - loss: 1.4277 - acc: 0.\n",
      "Epoch 25/60\n",
      "210783/210783 [==============================] - 73s 348us/sample - loss: 1.3644 - acc: 0.6869\n",
      "Epoch 26/60\n",
      "210783/210783 [==============================] - 74s 349us/sample - loss: 1.3104 - acc: 0.6991\n",
      "Epoch 27/60\n",
      "210783/210783 [==============================] - 74s 350us/sample - loss: 1.2412 - acc: 0.7159\n",
      "Epoch 28/60\n",
      "210783/210783 [==============================] - 74s 351us/sample - loss: 1.1815 - acc: 0.7301\n",
      "Epoch 29/60\n",
      "210783/210783 [==============================] - 74s 352us/sample - loss: 1.1370 - acc: 0.7394\n",
      "Epoch 30/60\n",
      "210783/210783 [==============================] - 73s 347us/sample - loss: 1.0926 - acc: 0.7492\n",
      "Epoch 31/60\n",
      "210783/210783 [==============================] - 75s 354us/sample - loss: 1.0463 - acc: 0.7611\n",
      "Epoch 32/60\n",
      "210783/210783 [==============================] - 73s 348us/sample - loss: 1.0004 - acc: 0.7709\n",
      "Epoch 33/60\n",
      "210783/210783 [==============================] - 72s 344us/sample - loss: 0.9739 - acc: 0.7755\n",
      "Epoch 34/60\n",
      "210783/210783 [==============================] - 73s 344us/sample - loss: 0.9464 - acc: 0.7821\n",
      "Epoch 35/60\n",
      "210783/210783 [==============================] - 73s 346us/sample - loss: 0.8999 - acc: 0.7935\n",
      "Epoch 36/60\n",
      "210783/210783 [==============================] - 73s 345us/sample - loss: 0.8741 - acc: 0.7984\n",
      "Epoch 37/60\n",
      "210783/210783 [==============================] - 73s 347us/sample - loss: 0.8307 - acc: 0.8095\n",
      "Epoch 38/60\n",
      "210783/210783 [==============================] - 75s 355us/sample - loss: 0.8107 - acc: 0.8125\n",
      "Epoch 39/60\n",
      "210783/210783 [==============================] - 74s 350us/sample - loss: 0.7761 - acc: 0.8206\n",
      "Epoch 40/60\n",
      "210783/210783 [==============================] - 73s 348us/sample - loss: 0.7479 - acc: 0.8281\n",
      "Epoch 41/60\n",
      "210783/210783 [==============================] - 74s 349us/sample - loss: 0.7594 - acc: 0.8227\n",
      "Epoch 42/60\n",
      "210783/210783 [==============================] - 74s 352us/sample - loss: 0.7324 - acc: 0.8294\n",
      "Epoch 43/60\n",
      "210783/210783 [==============================] - 74s 350us/sample - loss: 0.6958 - acc: 0.8387\n",
      "Epoch 44/60\n",
      "210783/210783 [==============================] - 74s 350us/sample - loss: 0.6970 - acc: 0.8365 - loss: 0\n",
      "Epoch 45/60\n",
      "210783/210783 [==============================] - 74s 352us/sample - loss: 0.6771 - acc: 0.8407\n",
      "Epoch 46/60\n",
      "210783/210783 [==============================] - 73s 349us/sample - loss: 0.6544 - acc: 0.8469\n",
      "Epoch 47/60\n",
      "210783/210783 [==============================] - 73s 348us/sample - loss: 0.6411 - acc: 0.8492\n",
      "Epoch 48/60\n",
      "210783/210783 [==============================] - 74s 351us/sample - loss: 0.6671 - acc: 0.8400\n",
      "Epoch 49/60\n",
      "210783/210783 [==============================] - 73s 347us/sample - loss: 0.7093 - acc: 0.8261\n",
      "Epoch 50/60\n",
      "210783/210783 [==============================] - 73s 345us/sample - loss: 0.6432 - acc: 0.8447\n",
      "Epoch 51/60\n",
      "210783/210783 [==============================] - 73s 344us/sample - loss: 0.6042 - acc: 0.8560\n",
      "Epoch 52/60\n",
      "210783/210783 [==============================] - 73s 346us/sample - loss: 0.5959 - acc: 0.8574\n",
      "Epoch 53/60\n",
      "210783/210783 [==============================] - 72s 344us/sample - loss: 0.6204 - acc: 0.8490\n",
      "Epoch 54/60\n",
      "210783/210783 [==============================] - 73s 345us/sample - loss: 0.5988 - acc: 0.8542\n",
      "Epoch 55/60\n",
      "210783/210783 [==============================] - 73s 345us/sample - loss: 0.6111 - acc: 0.8498\n",
      "Epoch 56/60\n",
      "210783/210783 [==============================] - 73s 346us/sample - loss: 0.5939 - acc: 0.8535\n",
      "Epoch 57/60\n",
      "210783/210783 [==============================] - 73s 346us/sample - loss: 0.5967 - acc: 0.8522\n",
      "Epoch 58/60\n",
      "210783/210783 [==============================] - 74s 352us/sample - loss: 0.5551 - acc: 0.8646\n",
      "Epoch 59/60\n",
      "210783/210783 [==============================] - 73s 347us/sample - loss: 0.5457 - acc: 0.8673\n",
      "Epoch 60/60\n",
      "210783/210783 [==============================] - 74s 353us/sample - loss: 0.5358 - acc: 0.8698\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())\n",
    "filepath = \"./model_BTT_max40.pth\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, save_weights_only=False)\n",
    "BTT_training_history = model_BTT.fit(\n",
    "    x_BTT,\n",
    "    y_BTT,\n",
    "    batch_size=128,\n",
    "    epochs=60,\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\songyih\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 39, 500)           7154500   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 500)               1501500   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 14309)             7168809   \n",
      "=================================================================\n",
      "Total params: 15,824,809\n",
      "Trainable params: 15,824,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "''' Elman + BTT model with the SimpleRnn unit replaced with a Gated Recurrent Unit (GRU)\n",
    "'''\n",
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model_BTT_GRU = Sequential()\n",
    "model_BTT_GRU.add(Embedding(vocabulary_size, 500, input_length=max_sentence_length-1))\n",
    "model_BTT_GRU.add(GRU(units=500, activation='sigmoid'))\n",
    "model_BTT_GRU.add(Dense(vocabulary_size, activation='softmax'))\n",
    "\n",
    "model_BTT_GRU.summary()\n",
    "model_BTT_GRU.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6155523581894805900\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6700198133\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11846570494518179389\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "WARNING:tensorflow:From C:\\Users\\songyih\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/60\n",
      "210783/210783 [==============================] - 58s 277us/sample - loss: 6.4454 - acc: 0.0906\n",
      "Epoch 2/60\n",
      "210783/210783 [==============================] - 55s 262us/sample - loss: 5.7829 - acc: 0.1321\n",
      "Epoch 3/60\n",
      "210783/210783 [==============================] - 56s 264us/sample - loss: 5.4074 - acc: 0.1531\n",
      "Epoch 4/60\n",
      "210783/210783 [==============================] - 56s 266us/sample - loss: 5.1580 - acc: 0.1688\n",
      "Epoch 5/60\n",
      "210783/210783 [==============================] - 56s 266us/sample - loss: 4.9595 - acc: 0.1833\n",
      "Epoch 6/60\n",
      "210783/210783 [==============================] - 57s 272us/sample - loss: 4.7916 - acc: 0.1950\n",
      "Epoch 7/60\n",
      "210783/210783 [==============================] - 56s 268us/sample - loss: 4.6360 - acc: 0.2038\n",
      "Epoch 8/60\n",
      "210783/210783 [==============================] - 56s 265us/sample - loss: 4.4891 - acc: 0.2129\n",
      "Epoch 9/60\n",
      "210783/210783 [==============================] - 56s 266us/sample - loss: 4.3489 - acc: 0.2216\n",
      "Epoch 10/60\n",
      "210783/210783 [==============================] - 57s 271us/sample - loss: 4.2140 - acc: 0.2294\n",
      "Epoch 11/60\n",
      "210783/210783 [==============================] - 57s 268us/sample - loss: 4.0776 - acc: 0.2373\n",
      "Epoch 12/60\n",
      "210783/210783 [==============================] - 56s 266us/sample - loss: 3.9418 - acc: 0.2478\n",
      "Epoch 13/60\n",
      "210783/210783 [==============================] - 56s 267us/sample - loss: 3.8105 - acc: 0.2618\n",
      "Epoch 14/60\n",
      "210783/210783 [==============================] - 60s 285us/sample - loss: 3.6806 - acc: 0.2783\n",
      "Epoch 15/60\n",
      "210783/210783 [==============================] - 60s 284us/sample - loss: 3.5556 - acc: 0.2956\n",
      "Epoch 16/60\n",
      "210783/210783 [==============================] - 60s 287us/sample - loss: 3.4363 - acc: 0.3128\n",
      "Epoch 17/60\n",
      "210783/210783 [==============================] - 60s 285us/sample - loss: 3.3246 - acc: 0.3299\n",
      "Epoch 18/60\n",
      "210783/210783 [==============================] - 61s 288us/sample - loss: 3.2171 - acc: 0.3463\n",
      "Epoch 19/60\n",
      "210783/210783 [==============================] - 61s 288us/sample - loss: 3.1116 - acc: 0.3629\n",
      "Epoch 20/60\n",
      "210783/210783 [==============================] - 58s 274us/sample - loss: 3.0169 - acc: 0.3772\n",
      "Epoch 21/60\n",
      "210783/210783 [==============================] - 57s 273us/sample - loss: 2.9213 - acc: 0.3927\n",
      "Epoch 22/60\n",
      "210783/210783 [==============================] - 58s 273us/sample - loss: 2.8296 - acc: 0.4080\n",
      "Epoch 23/60\n",
      "210783/210783 [==============================] - 58s 273us/sample - loss: 2.7413 - acc: 0.4225\n",
      "Epoch 24/60\n",
      "210783/210783 [==============================] - 58s 274us/sample - loss: 2.6582 - acc: 0.4373\n",
      "Epoch 25/60\n",
      "210783/210783 [==============================] - 57s 272us/sample - loss: 2.5792 - acc: 0.4507\n",
      "Epoch 26/60\n",
      "210783/210783 [==============================] - 57s 273us/sample - loss: 2.5036 - acc: 0.4639\n",
      "Epoch 27/60\n",
      "210783/210783 [==============================] - 58s 273us/sample - loss: 2.4295 - acc: 0.4773\n",
      "Epoch 28/60\n",
      "210783/210783 [==============================] - 58s 273us/sample - loss: 2.3678 - acc: 0.4900\n",
      "Epoch 29/60\n",
      "210783/210783 [==============================] - 58s 273us/sample - loss: 2.2896 - acc: 0.5047\n",
      "Epoch 30/60\n",
      "210783/210783 [==============================] - 57s 272us/sample - loss: 2.2062 - acc: 0.5218\n",
      "Epoch 31/60\n",
      "210783/210783 [==============================] - 57s 271us/sample - loss: 2.1297 - acc: 0.5378\n",
      "Epoch 32/60\n",
      "210783/210783 [==============================] - 57s 271us/sample - loss: 2.0629 - acc: 0.5515\n",
      "Epoch 33/60\n",
      "210783/210783 [==============================] - 57s 273us/sample - loss: 1.9952 - acc: 0.5657\n",
      "Epoch 34/60\n",
      "210783/210783 [==============================] - 58s 273us/sample - loss: 1.9355 - acc: 0.5788\n",
      "Epoch 35/60\n",
      "210783/210783 [==============================] - 58s 273us/sample - loss: 1.8726 - acc: 0.5923\n",
      "Epoch 36/60\n",
      "210783/210783 [==============================] - 58s 273us/sample - loss: 1.8328 - acc: 0.6002\n",
      "Epoch 37/60\n",
      "210783/210783 [==============================] - 58s 273us/sample - loss: 1.7687 - acc: 0.6145\n",
      "Epoch 38/60\n",
      "210783/210783 [==============================] - 58s 274us/sample - loss: 1.7169 - acc: 0.6261\n",
      "Epoch 39/60\n",
      "210783/210783 [==============================] - 58s 274us/sample - loss: 1.6670 - acc: 0.6367\n",
      "Epoch 40/60\n",
      "210783/210783 [==============================] - 58s 273us/sample - loss: 1.6096 - acc: 0.6501\n",
      "Epoch 41/60\n",
      "210783/210783 [==============================] - 57s 272us/sample - loss: 1.5526 - acc: 0.6626\n",
      "Epoch 42/60\n",
      "210783/210783 [==============================] - 58s 276us/sample - loss: 1.5092 - acc: 0.6726\n",
      "Epoch 43/60\n",
      "210783/210783 [==============================] - 58s 277us/sample - loss: 1.4536 - acc: 0.6853\n",
      "Epoch 44/60\n",
      "210783/210783 [==============================] - 58s 276us/sample - loss: 1.4098 - acc: 0.6953\n",
      "Epoch 45/60\n",
      "210783/210783 [==============================] - 58s 276us/sample - loss: 1.3654 - acc: 0.7057\n",
      "Epoch 46/60\n",
      "210783/210783 [==============================] - 58s 274us/sample - loss: 1.3224 - acc: 0.7153\n",
      "Epoch 47/60\n",
      "210783/210783 [==============================] - 58s 275us/sample - loss: 1.2760 - acc: 0.7263\n",
      "Epoch 48/60\n",
      "210783/210783 [==============================] - 61s 291us/sample - loss: 1.2516 - acc: 0.7306\n",
      "Epoch 49/60\n",
      "210783/210783 [==============================] - 63s 298us/sample - loss: 1.1987 - acc: 0.7428\n",
      "Epoch 50/60\n",
      "210783/210783 [==============================] - 63s 298us/sample - loss: 1.1507 - acc: 0.7543\n",
      "Epoch 51/60\n",
      "210783/210783 [==============================] - 63s 299us/sample - loss: 1.1123 - acc: 0.7638\n",
      "Epoch 52/60\n",
      "210783/210783 [==============================] - 63s 299us/sample - loss: 1.0755 - acc: 0.7715\n",
      "Epoch 53/60\n",
      "210783/210783 [==============================] - 63s 299us/sample - loss: 1.0434 - acc: 0.7785\n",
      "Epoch 54/60\n",
      "210783/210783 [==============================] - 63s 299us/sample - loss: 1.0070 - acc: 0.7877\n",
      "Epoch 55/60\n",
      "210783/210783 [==============================] - 63s 299us/sample - loss: 0.9726 - acc: 0.7961\n",
      "Epoch 56/60\n",
      "210783/210783 [==============================] - 63s 299us/sample - loss: 0.9462 - acc: 0.8013\n",
      "Epoch 57/60\n",
      "210783/210783 [==============================] - 63s 301us/sample - loss: 0.9097 - acc: 0.8104\n",
      "Epoch 58/60\n",
      "210783/210783 [==============================] - 63s 301us/sample - loss: 0.8759 - acc: 0.8183\n",
      "Epoch 59/60\n",
      "210783/210783 [==============================] - 63s 301us/sample - loss: 0.8484 - acc: 0.8246\n",
      "Epoch 60/60\n",
      "210783/210783 [==============================] - 64s 301us/sample - loss: 0.8129 - acc: 0.8331\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())\n",
    "filepath = \"./model_BTT_GRU.pth\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, save_weights_only=False)\n",
    "BTT_training_history = model_BTT_GRU.fit(\n",
    "    x_BTT,\n",
    "    y_BTT,\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your results in terms of cross-entropy loss with two other approach (part 1 and 2)\n",
    "\n",
    "- Elman + BTT with the SimpleRNN unit replaced with GRU (part 3): cross-entropy loss TODO, acc TODO\n",
    "- Elman + BTT (part 2): cross-entropy loss TODO, acc TODO\n",
    "- Elman + TBTT (part 3): cross-entropy loss TODO, acc TODO\n",
    "\n",
    "The cross-entropy loss of Elman + BTT network with the SimpleRNN unit replaced with GRU is the lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calf', '!', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I']\n",
      "['rehearsed', ',', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I']\n",
      "['brow-bound', 'with', 'the', 'world', ',', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am']\n",
      "['cloak', '!', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I']\n",
      "[\"weigh'd\", 'Your', 'father', \"'s\", 'the', 'world', ',', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am']\n",
      "['nuptials', 'of', 'the', 'world', ',', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am']\n",
      "[\"'Twere\", 'good', 'father', \"'s\", 'the', 'world', ',', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am']\n",
      "['ship', ',', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I']\n",
      "['Nothing', 'but', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I']\n",
      "['conjured', 'it', 'is', 'the', 'world', ',', 'I', 'am', 'I', 'am', 'I', 'am', 'I', 'am', 'I']\n"
     ]
    }
   ],
   "source": [
    "'''Use each model to generate 10 synthetic sentences of 15 words each\n",
    "'''\n",
    "word_num = 15\n",
    "sentence_num = 10\n",
    "\n",
    "# basic elman with TBTT model (part 1)\n",
    "elman_TBTT = load_model('./model_elman.pth')\n",
    "for i in range(sentence_num):\n",
    "    # randomly choose init word\n",
    "    init_encoded = np.random.randint(vocabulary_size)\n",
    "    encoded_sequence = [init_encoded]\n",
    "    # generate the predicted sequence\n",
    "    for _ in range(word_num - 1):\n",
    "        latest_word_encoded = [encoded_sequence[-1]]\n",
    "        latest_word_encoded = np.array(latest_word_encoded)\n",
    "        predicted_encoded = elman_TBTT.predict_classes(latest_word_encoded, verbose=0)\n",
    "        encoded_sequence.append(predicted_encoded[0])\n",
    "    # decode the sequence\n",
    "    decoded_sequence = []\n",
    "    for encoded_word in encoded_sequence:\n",
    "        decoded_sequence.append(index2word[encoded_word])\n",
    "    print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('deeplearning': conda)",
   "language": "python",
   "name": "python37664bitdeeplearningcondae305f790eddb4010bbd727137fcc23c4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
