{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Course (980)\n",
    "## Assignment Three \n",
    "\n",
    "__Assignment Goals:__\n",
    "\n",
    "- Implementing RNN based language models.\n",
    "- Implementing and applying a Recurrent Neural Network on text classification problem using TensorFlow.\n",
    "- Implementing __many to one__ and __many to many__ RNN sequence processing.\n",
    "\n",
    "In this assignment, you will implement RNN-based language models and compare extracted word representation from different models. You will also compare two different training methods for sequential data: Truncated Backpropagation Through Time __(TBTT)__ and Backpropagation Through Time __(BTT)__. \n",
    "Also, you will be asked to apply Vanilla RNN to capture word representations and solve a text classification problem. \n",
    "\n",
    "\n",
    "__DataSets__: You will use two datasets, an English Literature for language model task (part 1 to 4) and 20Newsgroups for text classification (part 5). \n",
    "\n",
    "\n",
    "1. (30 points) Implement the RNN based language model described by Mikolov et al.[1], also called __Elman network__ and train a language model on the English Literature dataset. This network contains input, hidden and output layer and is trained by standard backpropagation (TBTT with τ = 1) using the cross-entropy loss. \n",
    "   - The input represents the current word while using 1-of-N coding (thus its size is equal to the size of the vocabulary) and vector s(t − 1) that represents output values in the hidden layer from the previous time step. \n",
    "   - The hidden layer is a fully connected sigmoid layer with size 500. \n",
    "   - Softmax Output Layer to capture a valid probability distribution.\n",
    "   - The model is trained with truncated backpropagation through time (TBTT) with τ = 1: the weights of the network are updated based on the error vector computed only for the current time step.\n",
    "   \n",
    "   Download the English Literature dataset and train the language model as described, report the model cross-entropy loss on the train set. Use nltk.word_tokenize to tokenize the documents. \n",
    "For initialization, s(0) can be set to a vector of small values. Note that we are not interested in the *dynamic model* mentioned in the original paper. \n",
    "To make the implementation simpler you can use Keras to define neural net layers, including Keras.Embedding. (Keras.Embedding will create an additional mapping layer compared to the Elman architecture.) \n",
    "\n",
    "2. (20 points) TBTT has less computational cost and memory needs in comparison with *backpropagation through time algorithm (BTT)*. These benefits come at the cost of losing long term dependencies [2]. Now let's try to investigate computational costs and performance of learning our language model with BTT. For training the Elman-type RNN with BTT, one option is to perform mini-batch gradient descent with exactly one sentence per mini-batch. (The input  size will be [1, Sentence Length]). \n",
    "\n",
    "    1. Split the document into sentences (you can use nltk.tokenize.sent_tokenize).\n",
    "    2. For each sentence, perform one pass that computes the mean/sum loss for this sentence; then perform a gradient update for the whole sentence. (So the mini-batch size varies for the sentences with different lengths). You can truncate long sentences to fit the data in memory. \n",
    "    3. Report the model cross-entropy loss.\n",
    "\n",
    "3. (15 points) It does not seem that simple recurrent neural networks can capture truly exploit context information with long dependencies, because of the problem that gradients vanish and exploding. To solve this problem, gating mechanisms for recurrent neural networks were introduced. Try to learn your last model (Elman + BTT) with the SimpleRnn unit replaced with a Gated Recurrent Unit (GRU). Report the model cross-entropy loss. Compare your results in terms of cross-entropy loss with two other approach(part 1 and 2). Use each model to generate 10 synthetic sentences of 15 words each. Discuss the quality of the sentences generated - do they look like proper English? Do they match the training set?\n",
    "    Text generation from a given language model can be done using the following iterative process:\n",
    "   1. Set sequence = \\[first_word\\], chosen randomly.\n",
    "   2. Select a new word based on the sequence so far, add this word to the sequence, and repeat. At each iteration, select the word with maximum probability given the sequence so far. The trained language model outputs this probability. \n",
    "\n",
    "4. (15 points) The text describes how to extract a word representation from a trained RNN (Chapter 4). How we can evaluate the extracted word representation for your trained RNN? Compare the words representation extracted from each of the approaches using one of the existing methods.\n",
    "\n",
    "5. (20 points) We are aiming to learn an RNN model that predicts document categories given its content (text classification). For this task, we will use the 20Newsgroupst dataset. The 20Newsgroupst contains messages from twenty newsgroups.  We selected four major categories (comp, politics, rec, and religion) comprising around 13k documents altogether. Your model should learn word representations to support the classification task. For solving this problem modify the __Elman network__ architecture such that the last layer is a softmax layer with just 4 output neurons (one for each category). \n",
    "\n",
    "    1. Download the 20Newsgroups dataset, and use the implemented code from the notebook to read in the dataset.\n",
    "    2. Split the data into a training set (90 percent) and validation set (10 percent). Train the model on  20Newsgroups.\n",
    "    3. Report your accuracy results on the validation set.\n",
    "\n",
    "__NOTE__: Please use Jupyter Notebook. The notebook should include the final code, results and your answers. You should submit your Notebook in (.pdf or .html) and .ipynb format. (penalty 10 points) \n",
    "\n",
    "To reduce the parameters, you can merge all words that occur less often than a threshold into a special rare token (\\__unk__).\n",
    "\n",
    "__Instructions__:\n",
    "\n",
    "The university policy on academic dishonesty and plagiarism (cheating) will be taken very seriously in this course. Everything submitted should be your own writing or coding. You must not let other students copy your work. Spelling and grammar count.\n",
    "\n",
    "Your assignments will be marked based on correctness, originality (the implementations and ideas are from yourself), clarification and test performance.\n",
    "\n",
    "\n",
    "[1] Tom´ as Mikolov, Martin Kara ˇ fiat, Luk´ ´ as Burget, Jan ˇ Cernock´ ˇ y,Sanjeev Khudanpur: Recurrent neural network based language model, In: Proc. INTERSPEECH 2010\n",
    "\n",
    "[2] Tallec, Corentin, and Yann Ollivier. \"Unbiasing truncated backpropagation through time.\" arXiv preprint arXiv:1705.08209 (2017).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:comp.graphics\n",
      "Category:comp.os.ms-windows.misc\n",
      "Category:comp.sys.ibm.pc.hardware\n",
      "Category:comp.sys.mac.hardware\n",
      "Category:comp.windows.x\n",
      "Category:rec.autos\n",
      "Category:rec.motorcycles\n",
      "Category:rec.sport.baseball\n",
      "Category:rec.sport.hockey\n",
      "Category:soc.religion.christian\n",
      "Category:talk.politics.guns\n",
      "Category:talk.politics.mideast\n",
      "Category:talk.politics.misc\n",
      "Category:talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"This code is used to read all news and their labels\"\"\"\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def to_categories(name, cat=[\"politics\",\"rec\",\"comp\",\"religion\"]):\n",
    "    for i in range(len(cat)):\n",
    "        if str.find(name,cat[i])>-1:\n",
    "            return(i)\n",
    "    print(\"Unexpected folder: \" + name) # print the folder name which does not include expected categories\n",
    "    return(\"wth\")\n",
    "\n",
    "def data_loader(images_dir):\n",
    "    categories = os.listdir(data_path)\n",
    "    news = [] # news content\n",
    "    groups = [] # category which it belong to\n",
    "    \n",
    "    for cat in categories:\n",
    "        print(\"Category:\"+cat)\n",
    "        for the_new_path in glob.glob(data_path + '/' + cat + '/*'):\n",
    "            news.append(open(the_new_path,encoding = \"ISO-8859-1\", mode ='r').read().lower())\n",
    "            groups.append(cat)\n",
    "\n",
    "    return news, list(map(to_categories, groups))\n",
    "\n",
    "\n",
    "\n",
    "data_path = \"datasets/20news_subsampled\"\n",
    "news, groups = data_loader(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\songyih\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\songyih\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\songyih\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\songyih\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\songyih\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\songyih\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\songyih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254533\n"
     ]
    }
   ],
   "source": [
    "'''Implementing RNN based language model Elman network. (part 1)\n",
    "'''\n",
    "from nltk import word_tokenize, download\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# load the English Literature dataset\n",
    "english_literature_path = './datasets/English Literature.txt'\n",
    "with open(english_literature_path) as f:\n",
    "    english_literature_text = f.read()\n",
    "print(len(english_literature_text))\n",
    "\n",
    "# tokenize the English Literature dataset\n",
    "download('punkt')\n",
    "english_literature_tokens = word_tokenize(english_literature_text)\n",
    "print(len(english_literature_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14309\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary\n",
    "from collections import Counter\n",
    "\n",
    "word2index = {}\n",
    "index2word = []\n",
    "english_literature_counter = Counter(english_literature_tokens)\n",
    "for word, count in english_literature_counter.items():\n",
    "    index2word.append(word)\n",
    "    word2index[word] = len(word2index)\n",
    "\n",
    "vocabulary_size = len(word2index)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0%\r",
      "Progress: 0.04%\r",
      "Progress: 0.08%\r",
      "Progress: 0.12%\r",
      "Progress: 0.16%\r",
      "Progress: 0.2%\r",
      "Progress: 0.24%\r",
      "Progress: 0.28%\r",
      "Progress: 0.31%\r",
      "Progress: 0.35%\r",
      "Progress: 0.39%\r",
      "Progress: 0.43%\r",
      "Progress: 0.47%\r",
      "Progress: 0.51%\r",
      "Progress: 0.55%\r",
      "Progress: 0.59%\r",
      "Progress: 0.63%\r",
      "Progress: 0.67%\r",
      "Progress: 0.71%\r",
      "Progress: 0.75%\r",
      "Progress: 0.79%\r",
      "Progress: 0.83%\r",
      "Progress: 0.86%\r",
      "Progress: 0.9%\r",
      "Progress: 0.94%\r",
      "Progress: 0.98%\r",
      "Progress: 1.02%\r",
      "Progress: 1.06%\r",
      "Progress: 1.1%\r",
      "Progress: 1.14%\r",
      "Progress: 1.18%\r",
      "Progress: 1.22%\r",
      "Progress: 1.26%\r",
      "Progress: 1.3%\r",
      "Progress: 1.34%\r",
      "Progress: 1.38%\r",
      "Progress: 1.41%\r",
      "Progress: 1.45%\r",
      "Progress: 1.49%\r",
      "Progress: 1.53%\r",
      "Progress: 1.57%\r",
      "Progress: 1.61%\r",
      "Progress: 1.65%\r",
      "Progress: 1.69%\r",
      "Progress: 1.73%\r",
      "Progress: 1.77%\r",
      "Progress: 1.81%\r",
      "Progress: 1.85%\r",
      "Progress: 1.89%\r",
      "Progress: 1.93%\r",
      "Progress: 1.96%\r",
      "Progress: 2.0%\r",
      "Progress: 2.04%\r",
      "Progress: 2.08%\r",
      "Progress: 2.12%\r",
      "Progress: 2.16%\r",
      "Progress: 2.2%\r",
      "Progress: 2.24%\r",
      "Progress: 2.28%\r",
      "Progress: 2.32%\r",
      "Progress: 2.36%\r",
      "Progress: 2.4%\r",
      "Progress: 2.44%\r",
      "Progress: 2.48%\r",
      "Progress: 2.51%\r",
      "Progress: 2.55%\r",
      "Progress: 2.59%\r",
      "Progress: 2.63%\r",
      "Progress: 2.67%\r",
      "Progress: 2.71%\r",
      "Progress: 2.75%\r",
      "Progress: 2.79%\r",
      "Progress: 2.83%\r",
      "Progress: 2.87%\r",
      "Progress: 2.91%\r",
      "Progress: 2.95%\r",
      "Progress: 2.99%\r",
      "Progress: 3.03%\r",
      "Progress: 3.06%\r",
      "Progress: 3.1%\r",
      "Progress: 3.14%\r",
      "Progress: 3.18%\r",
      "Progress: 3.22%\r",
      "Progress: 3.26%\r",
      "Progress: 3.3%\r",
      "Progress: 3.34%\r",
      "Progress: 3.38%\r",
      "Progress: 3.42%\r",
      "Progress: 3.46%\r",
      "Progress: 3.5%\r",
      "Progress: 3.54%\r",
      "Progress: 3.58%\r",
      "Progress: 3.61%\r",
      "Progress: 3.65%\r",
      "Progress: 3.69%\r",
      "Progress: 3.73%\r",
      "Progress: 3.77%\r",
      "Progress: 3.81%\r",
      "Progress: 3.85%\r",
      "Progress: 3.89%\r",
      "Progress: 3.93%\r",
      "Progress: 3.97%\r",
      "Progress: 4.01%\r",
      "Progress: 4.05%\r",
      "Progress: 4.09%\r",
      "Progress: 4.13%\r",
      "Progress: 4.16%\r",
      "Progress: 4.2%\r",
      "Progress: 4.24%\r",
      "Progress: 4.28%\r",
      "Progress: 4.32%\r",
      "Progress: 4.36%\r",
      "Progress: 4.4%\r",
      "Progress: 4.44%\r",
      "Progress: 4.48%\r",
      "Progress: 4.52%\r",
      "Progress: 4.56%\r",
      "Progress: 4.6%\r",
      "Progress: 4.64%\r",
      "Progress: 4.68%\r",
      "Progress: 4.71%\r",
      "Progress: 4.75%\r",
      "Progress: 4.79%\r",
      "Progress: 4.83%\r",
      "Progress: 4.87%\r",
      "Progress: 4.91%\r",
      "Progress: 4.95%\r",
      "Progress: 4.99%\r",
      "Progress: 5.03%\r",
      "Progress: 5.07%\r",
      "Progress: 5.11%\r",
      "Progress: 5.15%\r",
      "Progress: 5.19%\r",
      "Progress: 5.23%\r",
      "Progress: 5.26%\r",
      "Progress: 5.3%\r",
      "Progress: 5.34%\r",
      "Progress: 5.38%\r",
      "Progress: 5.42%\r",
      "Progress: 5.46%\r",
      "Progress: 5.5%\r",
      "Progress: 5.54%\r",
      "Progress: 5.58%\r",
      "Progress: 5.62%\r",
      "Progress: 5.66%\r",
      "Progress: 5.7%\r",
      "Progress: 5.74%\r",
      "Progress: 5.78%\r",
      "Progress: 5.81%\r",
      "Progress: 5.85%\r",
      "Progress: 5.89%\r",
      "Progress: 5.93%\r",
      "Progress: 5.97%\r",
      "Progress: 6.01%\r",
      "Progress: 6.05%\r",
      "Progress: 6.09%\r",
      "Progress: 6.13%\r",
      "Progress: 6.17%\r",
      "Progress: 6.21%\r",
      "Progress: 6.25%\r",
      "Progress: 6.29%\r",
      "Progress: 6.33%\r",
      "Progress: 6.36%\r",
      "Progress: 6.4%\r",
      "Progress: 6.44%\r",
      "Progress: 6.48%\r",
      "Progress: 6.52%\r",
      "Progress: 6.56%\r",
      "Progress: 6.6%\r",
      "Progress: 6.64%\r",
      "Progress: 6.68%\r",
      "Progress: 6.72%\r",
      "Progress: 6.76%\r",
      "Progress: 6.8%\r",
      "Progress: 6.84%\r",
      "Progress: 6.88%\r",
      "Progress: 6.91%\r",
      "Progress: 6.95%\r",
      "Progress: 6.99%\r",
      "Progress: 7.03%\r",
      "Progress: 7.07%\r",
      "Progress: 7.11%\r",
      "Progress: 7.15%\r",
      "Progress: 7.19%\r",
      "Progress: 7.23%\r",
      "Progress: 7.27%\r",
      "Progress: 7.31%\r",
      "Progress: 7.35%\r",
      "Progress: 7.39%\r",
      "Progress: 7.43%\r",
      "Progress: 7.46%\r",
      "Progress: 7.5%\r",
      "Progress: 7.54%\r",
      "Progress: 7.58%\r",
      "Progress: 7.62%\r",
      "Progress: 7.66%\r",
      "Progress: 7.7%\r",
      "Progress: 7.74%\r",
      "Progress: 7.78%\r",
      "Progress: 7.82%\r",
      "Progress: 7.86%\r",
      "Progress: 7.9%\r",
      "Progress: 7.94%\r",
      "Progress: 7.98%\r",
      "Progress: 8.01%\r",
      "Progress: 8.05%\r",
      "Progress: 8.09%\r",
      "Progress: 8.13%\r",
      "Progress: 8.17%\r",
      "Progress: 8.21%\r",
      "Progress: 8.25%\r",
      "Progress: 8.29%\r",
      "Progress: 8.33%\r",
      "Progress: 8.37%\r",
      "Progress: 8.41%\r",
      "Progress: 8.45%\r",
      "Progress: 8.49%\r",
      "Progress: 8.53%\r",
      "Progress: 8.56%\r",
      "Progress: 8.6%\r",
      "Progress: 8.64%\r",
      "Progress: 8.68%\r",
      "Progress: 8.72%\r",
      "Progress: 8.76%\r",
      "Progress: 8.8%\r",
      "Progress: 8.84%\r",
      "Progress: 8.88%\r",
      "Progress: 8.92%\r",
      "Progress: 8.96%\r",
      "Progress: 9.0%\r",
      "Progress: 9.04%\r",
      "Progress: 9.08%\r",
      "Progress: 9.11%\r",
      "Progress: 9.15%\r",
      "Progress: 9.19%\r",
      "Progress: 9.23%\r",
      "Progress: 9.27%\r",
      "Progress: 9.31%\r",
      "Progress: 9.35%\r",
      "Progress: 9.39%\r",
      "Progress: 9.43%\r",
      "Progress: 9.47%\r",
      "Progress: 9.51%\r",
      "Progress: 9.55%\r",
      "Progress: 9.59%\r",
      "Progress: 9.63%\r",
      "Progress: 9.66%\r",
      "Progress: 9.7%\r",
      "Progress: 9.74%\r",
      "Progress: 9.78%\r",
      "Progress: 9.82%\r",
      "Progress: 9.86%\r",
      "Progress: 9.9%\r",
      "Progress: 9.94%\r",
      "Progress: 9.98%\r",
      "Progress: 10.02%\r",
      "Progress: 10.06%\r",
      "Progress: 10.1%\r",
      "Progress: 10.14%\r",
      "Progress: 10.18%\r",
      "Progress: 10.21%\r",
      "Progress: 10.25%\r",
      "Progress: 10.29%\r",
      "Progress: 10.33%\r",
      "Progress: 10.37%\r",
      "Progress: 10.41%\r",
      "Progress: 10.45%\r",
      "Progress: 10.49%\r",
      "Progress: 10.53%\r",
      "Progress: 10.57%\r",
      "Progress: 10.61%\r",
      "Progress: 10.65%\r",
      "Progress: 10.69%\r",
      "Progress: 10.73%\r",
      "Progress: 10.76%\r",
      "Progress: 10.8%\r",
      "Progress: 10.84%\r",
      "Progress: 10.88%\r",
      "Progress: 10.92%\r",
      "Progress: 10.96%\r",
      "Progress: 11.0%\r",
      "Progress: 11.04%\r",
      "Progress: 11.08%\r",
      "Progress: 11.12%\r",
      "Progress: 11.16%\r",
      "Progress: 11.2%\r",
      "Progress: 11.24%\r",
      "Progress: 11.28%\r",
      "Progress: 11.31%\r",
      "Progress: 11.35%\r",
      "Progress: 11.39%\r",
      "Progress: 11.43%\r",
      "Progress: 11.47%\r",
      "Progress: 11.51%\r",
      "Progress: 11.55%\r",
      "Progress: 11.59%\r",
      "Progress: 11.63%\r",
      "Progress: 11.67%\r",
      "Progress: 11.71%\r",
      "Progress: 11.75%\r",
      "Progress: 11.79%\r",
      "Progress: 11.83%\r",
      "Progress: 11.86%\r",
      "Progress: 11.9%\r",
      "Progress: 11.94%\r",
      "Progress: 11.98%\r",
      "Progress: 12.02%\r",
      "Progress: 12.06%\r",
      "Progress: 12.1%\r",
      "Progress: 12.14%\r",
      "Progress: 12.18%\r",
      "Progress: 12.22%\r",
      "Progress: 12.26%\r",
      "Progress: 12.3%\r",
      "Progress: 12.34%\r",
      "Progress: 12.38%\r",
      "Progress: 12.41%\r",
      "Progress: 12.45%\r",
      "Progress: 12.49%\r",
      "Progress: 12.53%\r",
      "Progress: 12.57%\r",
      "Progress: 12.61%\r",
      "Progress: 12.65%\r",
      "Progress: 12.69%\r",
      "Progress: 12.73%\r",
      "Progress: 12.77%\r",
      "Progress: 12.81%\r",
      "Progress: 12.85%\r",
      "Progress: 12.89%\r",
      "Progress: 12.93%\r",
      "Progress: 12.96%\r",
      "Progress: 13.0%\r",
      "Progress: 13.04%\r",
      "Progress: 13.08%\r",
      "Progress: 13.12%\r",
      "Progress: 13.16%\r",
      "Progress: 13.2%\r",
      "Progress: 13.24%\r",
      "Progress: 13.28%\r",
      "Progress: 13.32%\r",
      "Progress: 13.36%\r",
      "Progress: 13.4%\r",
      "Progress: 13.44%\r",
      "Progress: 13.48%\r",
      "Progress: 13.51%\r",
      "Progress: 13.55%\r",
      "Progress: 13.59%\r",
      "Progress: 13.63%\r",
      "Progress: 13.67%\r",
      "Progress: 13.71%\r",
      "Progress: 13.75%\r",
      "Progress: 13.79%\r",
      "Progress: 13.83%\r",
      "Progress: 13.87%\r",
      "Progress: 13.91%\r",
      "Progress: 13.95%\r",
      "Progress: 13.99%\r",
      "Progress: 14.03%\r",
      "Progress: 14.06%\r",
      "Progress: 14.1%\r",
      "Progress: 14.14%\r",
      "Progress: 14.18%\r",
      "Progress: 14.22%\r",
      "Progress: 14.26%\r",
      "Progress: 14.3%\r",
      "Progress: 14.34%\r",
      "Progress: 14.38%\r",
      "Progress: 14.42%\r",
      "Progress: 14.46%\r",
      "Progress: 14.5%\r",
      "Progress: 14.54%\r",
      "Progress: 14.58%\r",
      "Progress: 14.62%\r",
      "Progress: 14.65%\r",
      "Progress: 14.69%\r",
      "Progress: 14.73%\r",
      "Progress: 14.77%\r",
      "Progress: 14.81%\r",
      "Progress: 14.85%\r",
      "Progress: 14.89%\r",
      "Progress: 14.93%\r",
      "Progress: 14.97%\r",
      "Progress: 15.01%\r",
      "Progress: 15.05%\r",
      "Progress: 15.09%\r",
      "Progress: 15.13%\r",
      "Progress: 15.17%\r",
      "Progress: 15.2%\r",
      "Progress: 15.24%\r",
      "Progress: 15.28%\r",
      "Progress: 15.32%\r",
      "Progress: 15.36%\r",
      "Progress: 15.4%\r",
      "Progress: 15.44%\r",
      "Progress: 15.48%\r",
      "Progress: 15.52%\r",
      "Progress: 15.56%\r",
      "Progress: 15.6%\r",
      "Progress: 15.64%\r",
      "Progress: 15.68%\r",
      "Progress: 15.72%\r",
      "Progress: 15.75%\r",
      "Progress: 15.79%\r",
      "Progress: 15.83%\r",
      "Progress: 15.87%\r",
      "Progress: 15.91%\r",
      "Progress: 15.95%\r",
      "Progress: 15.99%\r",
      "Progress: 16.03%\r",
      "Progress: 16.07%\r",
      "Progress: 16.11%\r",
      "Progress: 16.15%\r",
      "Progress: 16.19%\r",
      "Progress: 16.23%\r",
      "Progress: 16.27%\r",
      "Progress: 16.3%\r",
      "Progress: 16.34%\r",
      "Progress: 16.38%\r",
      "Progress: 16.42%\r",
      "Progress: 16.46%\r",
      "Progress: 16.5%\r",
      "Progress: 16.54%\r",
      "Progress: 16.58%\r",
      "Progress: 16.62%\r",
      "Progress: 16.66%\r",
      "Progress: 16.7%\r",
      "Progress: 16.74%\r",
      "Progress: 16.78%\r",
      "Progress: 16.82%\r",
      "Progress: 16.85%\r",
      "Progress: 16.89%\r",
      "Progress: 16.93%\r",
      "Progress: 16.97%\r",
      "Progress: 17.01%\r",
      "Progress: 17.05%\r",
      "Progress: 17.09%\r",
      "Progress: 17.13%\r",
      "Progress: 17.17%\r",
      "Progress: 17.21%\r",
      "Progress: 17.25%\r",
      "Progress: 17.29%\r",
      "Progress: 17.33%\r",
      "Progress: 17.37%\r",
      "Progress: 17.4%\r",
      "Progress: 17.44%\r",
      "Progress: 17.48%\r",
      "Progress: 17.52%\r",
      "Progress: 17.56%\r",
      "Progress: 17.6%\r",
      "Progress: 17.64%\r",
      "Progress: 17.68%\r",
      "Progress: 17.72%\r",
      "Progress: 17.76%\r",
      "Progress: 17.8%\r",
      "Progress: 17.84%\r",
      "Progress: 17.88%\r",
      "Progress: 17.92%\r",
      "Progress: 17.95%\r",
      "Progress: 17.99%\r",
      "Progress: 18.03%\r",
      "Progress: 18.07%\r",
      "Progress: 18.11%\r",
      "Progress: 18.15%\r",
      "Progress: 18.19%\r",
      "Progress: 18.23%\r",
      "Progress: 18.27%\r",
      "Progress: 18.31%\r",
      "Progress: 18.35%\r",
      "Progress: 18.39%\r",
      "Progress: 18.43%\r",
      "Progress: 18.47%\r",
      "Progress: 18.5%\r",
      "Progress: 18.54%\r",
      "Progress: 18.58%\r",
      "Progress: 18.62%\r",
      "Progress: 18.66%\r",
      "Progress: 18.7%\r",
      "Progress: 18.74%\r",
      "Progress: 18.78%\r",
      "Progress: 18.82%\r",
      "Progress: 18.86%\r",
      "Progress: 18.9%\r",
      "Progress: 18.94%\r",
      "Progress: 18.98%\r",
      "Progress: 19.02%\r",
      "Progress: 19.05%\r",
      "Progress: 19.09%\r",
      "Progress: 19.13%\r",
      "Progress: 19.17%\r",
      "Progress: 19.21%\r",
      "Progress: 19.25%\r",
      "Progress: 19.29%\r",
      "Progress: 19.33%\r",
      "Progress: 19.37%\r",
      "Progress: 19.41%\r",
      "Progress: 19.45%\r",
      "Progress: 19.49%\r",
      "Progress: 19.53%\r",
      "Progress: 19.57%\r",
      "Progress: 19.6%\r",
      "Progress: 19.64%\r",
      "Progress: 19.68%\r",
      "Progress: 19.72%\r",
      "Progress: 19.76%\r",
      "Progress: 19.8%\r",
      "Progress: 19.84%\r",
      "Progress: 19.88%\r",
      "Progress: 19.92%\r",
      "Progress: 19.96%\r",
      "Progress: 20.0%\r",
      "Progress: 20.04%\r",
      "Progress: 20.08%\r",
      "Progress: 20.12%\r",
      "Progress: 20.15%\r",
      "Progress: 20.19%\r",
      "Progress: 20.23%\r",
      "Progress: 20.27%\r",
      "Progress: 20.31%\r",
      "Progress: 20.35%\r",
      "Progress: 20.39%\r",
      "Progress: 20.43%\r",
      "Progress: 20.47%\r",
      "Progress: 20.51%\r",
      "Progress: 20.55%\r",
      "Progress: 20.59%\r",
      "Progress: 20.63%\r",
      "Progress: 20.67%\r",
      "Progress: 20.7%\r",
      "Progress: 20.74%\r",
      "Progress: 20.78%\r",
      "Progress: 20.82%\r",
      "Progress: 20.86%\r",
      "Progress: 20.9%\r",
      "Progress: 20.94%\r",
      "Progress: 20.98%\r",
      "Progress: 21.02%\r",
      "Progress: 21.06%\r",
      "Progress: 21.1%\r",
      "Progress: 21.14%\r",
      "Progress: 21.18%\r",
      "Progress: 21.22%\r",
      "Progress: 21.25%\r",
      "Progress: 21.29%\r",
      "Progress: 21.33%\r",
      "Progress: 21.37%\r",
      "Progress: 21.41%\r",
      "Progress: 21.45%\r",
      "Progress: 21.49%\r",
      "Progress: 21.53%\r",
      "Progress: 21.57%\r",
      "Progress: 21.61%\r",
      "Progress: 21.65%\r",
      "Progress: 21.69%\r",
      "Progress: 21.73%\r",
      "Progress: 21.77%\r",
      "Progress: 21.8%\r",
      "Progress: 21.84%\r",
      "Progress: 21.88%\r",
      "Progress: 21.92%\r",
      "Progress: 21.96%\r",
      "Progress: 22.0%\r",
      "Progress: 22.04%\r",
      "Progress: 22.08%\r",
      "Progress: 22.12%\r",
      "Progress: 22.16%\r",
      "Progress: 22.2%\r",
      "Progress: 22.24%\r",
      "Progress: 22.28%\r",
      "Progress: 22.32%\r",
      "Progress: 22.35%\r",
      "Progress: 22.39%\r",
      "Progress: 22.43%\r",
      "Progress: 22.47%\r",
      "Progress: 22.51%\r",
      "Progress: 22.55%\r",
      "Progress: 22.59%\r",
      "Progress: 22.63%\r",
      "Progress: 22.67%\r",
      "Progress: 22.71%\r",
      "Progress: 22.75%\r",
      "Progress: 22.79%\r",
      "Progress: 22.83%\r",
      "Progress: 22.87%\r",
      "Progress: 22.9%\r",
      "Progress: 22.94%\r",
      "Progress: 22.98%\r",
      "Progress: 23.02%\r",
      "Progress: 23.06%\r",
      "Progress: 23.1%\r",
      "Progress: 23.14%\r",
      "Progress: 23.18%\r",
      "Progress: 23.22%\r",
      "Progress: 23.26%\r",
      "Progress: 23.3%\r",
      "Progress: 23.34%\r",
      "Progress: 23.38%\r",
      "Progress: 23.42%\r",
      "Progress: 23.45%\r",
      "Progress: 23.49%\r",
      "Progress: 23.53%\r",
      "Progress: 23.57%\r",
      "Progress: 23.61%\r",
      "Progress: 23.65%\r",
      "Progress: 23.69%\r",
      "Progress: 23.73%\r",
      "Progress: 23.77%\r",
      "Progress: 23.81%\r",
      "Progress: 23.85%\r",
      "Progress: 23.89%\r",
      "Progress: 23.93%\r",
      "Progress: 23.97%\r",
      "Progress: 24.0%\r",
      "Progress: 24.04%\r",
      "Progress: 24.08%\r",
      "Progress: 24.12%\r",
      "Progress: 24.16%\r",
      "Progress: 24.2%\r",
      "Progress: 24.24%\r",
      "Progress: 24.28%\r",
      "Progress: 24.32%\r",
      "Progress: 24.36%\r",
      "Progress: 24.4%\r",
      "Progress: 24.44%\r",
      "Progress: 24.48%\r",
      "Progress: 24.52%\r",
      "Progress: 24.55%\r",
      "Progress: 24.59%\r",
      "Progress: 24.63%\r",
      "Progress: 24.67%\r",
      "Progress: 24.71%\r",
      "Progress: 24.75%\r",
      "Progress: 24.79%\r",
      "Progress: 24.83%\r",
      "Progress: 24.87%\r",
      "Progress: 24.91%\r",
      "Progress: 24.95%\r",
      "Progress: 24.99%\r",
      "Progress: 25.03%\r",
      "Progress: 25.07%\r",
      "Progress: 25.1%\r",
      "Progress: 25.14%\r",
      "Progress: 25.18%\r",
      "Progress: 25.22%\r",
      "Progress: 25.26%\r",
      "Progress: 25.3%\r",
      "Progress: 25.34%\r",
      "Progress: 25.38%\r",
      "Progress: 25.42%\r",
      "Progress: 25.46%\r",
      "Progress: 25.5%\r",
      "Progress: 25.54%\r",
      "Progress: 25.58%\r",
      "Progress: 25.62%\r",
      "Progress: 25.65%\r",
      "Progress: 25.69%\r",
      "Progress: 25.73%\r",
      "Progress: 25.77%\r",
      "Progress: 25.81%\r",
      "Progress: 25.85%\r",
      "Progress: 25.89%\r",
      "Progress: 25.93%\r",
      "Progress: 25.97%\r",
      "Progress: 26.01%\r",
      "Progress: 26.05%\r",
      "Progress: 26.09%\r",
      "Progress: 26.13%\r",
      "Progress: 26.17%\r",
      "Progress: 26.2%\r",
      "Progress: 26.24%\r",
      "Progress: 26.28%\r",
      "Progress: 26.32%\r",
      "Progress: 26.36%\r",
      "Progress: 26.4%\r",
      "Progress: 26.44%\r",
      "Progress: 26.48%\r",
      "Progress: 26.52%\r",
      "Progress: 26.56%\r",
      "Progress: 26.6%\r",
      "Progress: 26.64%\r",
      "Progress: 26.68%\r",
      "Progress: 26.72%\r",
      "Progress: 26.75%\r",
      "Progress: 26.79%\r",
      "Progress: 26.83%\r",
      "Progress: 26.87%\r",
      "Progress: 26.91%\r",
      "Progress: 26.95%\r",
      "Progress: 26.99%\r",
      "Progress: 27.03%\r",
      "Progress: 27.07%\r",
      "Progress: 27.11%\r",
      "Progress: 27.15%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress: 27.19%\r",
      "Progress: 27.23%\r",
      "Progress: 27.27%\r",
      "Progress: 27.3%\r",
      "Progress: 27.34%\r",
      "Progress: 27.38%\r",
      "Progress: 27.42%\r",
      "Progress: 27.46%\r",
      "Progress: 27.5%\r",
      "Progress: 27.54%\r",
      "Progress: 27.58%\r",
      "Progress: 27.62%\r",
      "Progress: 27.66%\r",
      "Progress: 27.7%\r",
      "Progress: 27.74%\r",
      "Progress: 27.78%\r",
      "Progress: 27.82%\r",
      "Progress: 27.85%\r",
      "Progress: 27.89%\r",
      "Progress: 27.93%\r",
      "Progress: 27.97%\r",
      "Progress: 28.01%\r",
      "Progress: 28.05%\r",
      "Progress: 28.09%\r",
      "Progress: 28.13%\r",
      "Progress: 28.17%\r",
      "Progress: 28.21%\r",
      "Progress: 28.25%\r",
      "Progress: 28.29%\r",
      "Progress: 28.33%\r",
      "Progress: 28.37%\r",
      "Progress: 28.4%\r",
      "Progress: 28.44%\r",
      "Progress: 28.48%\r",
      "Progress: 28.52%\r",
      "Progress: 28.56%\r",
      "Progress: 28.6%\r",
      "Progress: 28.64%\r",
      "Progress: 28.68%\r",
      "Progress: 28.72%\r",
      "Progress: 28.76%\r",
      "Progress: 28.8%\r",
      "Progress: 28.84%\r",
      "Progress: 28.88%\r",
      "Progress: 28.92%\r",
      "Progress: 28.95%\r",
      "Progress: 28.99%\r",
      "Progress: 29.03%\r",
      "Progress: 29.07%\r",
      "Progress: 29.11%\r",
      "Progress: 29.15%\r",
      "Progress: 29.19%\r",
      "Progress: 29.23%\r",
      "Progress: 29.27%\r",
      "Progress: 29.31%\r",
      "Progress: 29.35%\r",
      "Progress: 29.39%\r",
      "Progress: 29.43%\r",
      "Progress: 29.47%\r",
      "Progress: 29.51%\r",
      "Progress: 29.54%\r",
      "Progress: 29.58%\r",
      "Progress: 29.62%\r",
      "Progress: 29.66%\r",
      "Progress: 29.7%\r",
      "Progress: 29.74%\r",
      "Progress: 29.78%\r",
      "Progress: 29.82%\r",
      "Progress: 29.86%\r",
      "Progress: 29.9%\r",
      "Progress: 29.94%\r",
      "Progress: 29.98%\r",
      "Progress: 30.02%\r",
      "Progress: 30.06%\r",
      "Progress: 30.09%\r",
      "Progress: 30.13%\r",
      "Progress: 30.17%\r",
      "Progress: 30.21%\r",
      "Progress: 30.25%\r",
      "Progress: 30.29%\r",
      "Progress: 30.33%\r",
      "Progress: 30.37%\r",
      "Progress: 30.41%\r",
      "Progress: 30.45%\r",
      "Progress: 30.49%\r",
      "Progress: 30.53%\r",
      "Progress: 30.57%\r",
      "Progress: 30.61%\r",
      "Progress: 30.64%\r",
      "Progress: 30.68%\r",
      "Progress: 30.72%\r",
      "Progress: 30.76%\r",
      "Progress: 30.8%\r",
      "Progress: 30.84%\r",
      "Progress: 30.88%\r",
      "Progress: 30.92%\r",
      "Progress: 30.96%\r",
      "Progress: 31.0%\r",
      "Progress: 31.04%\r",
      "Progress: 31.08%\r",
      "Progress: 31.12%\r",
      "Progress: 31.16%\r",
      "Progress: 31.19%\r",
      "Progress: 31.23%\r",
      "Progress: 31.27%\r",
      "Progress: 31.31%\r",
      "Progress: 31.35%\r",
      "Progress: 31.39%\r",
      "Progress: 31.43%\r",
      "Progress: 31.47%\r",
      "Progress: 31.51%\r",
      "Progress: 31.55%\r",
      "Progress: 31.59%\r",
      "Progress: 31.63%\r",
      "Progress: 31.67%\r",
      "Progress: 31.71%\r",
      "Progress: 31.74%\r",
      "Progress: 31.78%\r",
      "Progress: 31.82%\r",
      "Progress: 31.86%\r",
      "Progress: 31.9%\r",
      "Progress: 31.94%\r",
      "Progress: 31.98%\r",
      "Progress: 32.02%\r",
      "Progress: 32.06%\r",
      "Progress: 32.1%\r",
      "Progress: 32.14%\r",
      "Progress: 32.18%\r",
      "Progress: 32.22%\r",
      "Progress: 32.26%\r",
      "Progress: 32.29%\r",
      "Progress: 32.33%\r",
      "Progress: 32.37%\r",
      "Progress: 32.41%\r",
      "Progress: 32.45%\r",
      "Progress: 32.49%\r",
      "Progress: 32.53%\r",
      "Progress: 32.57%\r",
      "Progress: 32.61%\r",
      "Progress: 32.65%\r",
      "Progress: 32.69%\r",
      "Progress: 32.73%\r",
      "Progress: 32.77%\r",
      "Progress: 32.81%\r",
      "Progress: 32.84%\r",
      "Progress: 32.88%\r",
      "Progress: 32.92%\r",
      "Progress: 32.96%\r",
      "Progress: 33.0%\r",
      "Progress: 33.04%\r",
      "Progress: 33.08%\r",
      "Progress: 33.12%\r",
      "Progress: 33.16%\r",
      "Progress: 33.2%\r",
      "Progress: 33.24%\r",
      "Progress: 33.28%\r",
      "Progress: 33.32%\r",
      "Progress: 33.36%\r",
      "Progress: 33.39%\r",
      "Progress: 33.43%\r",
      "Progress: 33.47%\r",
      "Progress: 33.51%\r",
      "Progress: 33.55%\r",
      "Progress: 33.59%\r",
      "Progress: 33.63%\r",
      "Progress: 33.67%\r",
      "Progress: 33.71%\r",
      "Progress: 33.75%\r",
      "Progress: 33.79%\r",
      "Progress: 33.83%\r",
      "Progress: 33.87%\r",
      "Progress: 33.91%\r",
      "Progress: 33.94%\r",
      "Progress: 33.98%\r",
      "Progress: 34.02%\r",
      "Progress: 34.06%\r",
      "Progress: 34.1%\r",
      "Progress: 34.14%\r",
      "Progress: 34.18%\r",
      "Progress: 34.22%\r",
      "Progress: 34.26%\r",
      "Progress: 34.3%\r",
      "Progress: 34.34%\r",
      "Progress: 34.38%\r",
      "Progress: 34.42%\r",
      "Progress: 34.46%\r",
      "Progress: 34.49%\r",
      "Progress: 34.53%\r",
      "Progress: 34.57%\r",
      "Progress: 34.61%\r",
      "Progress: 34.65%\r",
      "Progress: 34.69%\r",
      "Progress: 34.73%\r",
      "Progress: 34.77%\r",
      "Progress: 34.81%\r",
      "Progress: 34.85%\r",
      "Progress: 34.89%\r",
      "Progress: 34.93%\r",
      "Progress: 34.97%\r",
      "Progress: 35.01%\r",
      "Progress: 35.04%\r",
      "Progress: 35.08%\r",
      "Progress: 35.12%\r",
      "Progress: 35.16%\r",
      "Progress: 35.2%\r",
      "Progress: 35.24%\r",
      "Progress: 35.28%\r",
      "Progress: 35.32%\r",
      "Progress: 35.36%\r",
      "Progress: 35.4%\r",
      "Progress: 35.44%\r",
      "Progress: 35.48%\r",
      "Progress: 35.52%\r",
      "Progress: 35.56%\r",
      "Progress: 35.59%\r",
      "Progress: 35.63%\r",
      "Progress: 35.67%\r",
      "Progress: 35.71%\r",
      "Progress: 35.75%\r",
      "Progress: 35.79%\r",
      "Progress: 35.83%\r",
      "Progress: 35.87%\r",
      "Progress: 35.91%\r",
      "Progress: 35.95%\r",
      "Progress: 35.99%\r",
      "Progress: 36.03%\r",
      "Progress: 36.07%\r",
      "Progress: 36.11%\r",
      "Progress: 36.14%\r",
      "Progress: 36.18%\r",
      "Progress: 36.22%\r",
      "Progress: 36.26%\r",
      "Progress: 36.3%\r",
      "Progress: 36.34%\r",
      "Progress: 36.38%\r",
      "Progress: 36.42%\r",
      "Progress: 36.46%\r",
      "Progress: 36.5%\r",
      "Progress: 36.54%\r",
      "Progress: 36.58%\r",
      "Progress: 36.62%\r",
      "Progress: 36.66%\r",
      "Progress: 36.69%\r",
      "Progress: 36.73%\r",
      "Progress: 36.77%\r",
      "Progress: 36.81%\r",
      "Progress: 36.85%\r",
      "Progress: 36.89%\r",
      "Progress: 36.93%\r",
      "Progress: 36.97%\r",
      "Progress: 37.01%\r",
      "Progress: 37.05%\r",
      "Progress: 37.09%\r",
      "Progress: 37.13%\r",
      "Progress: 37.17%\r",
      "Progress: 37.21%\r",
      "Progress: 37.24%\r",
      "Progress: 37.28%\r",
      "Progress: 37.32%\r",
      "Progress: 37.36%\r",
      "Progress: 37.4%\r",
      "Progress: 37.44%\r",
      "Progress: 37.48%\r",
      "Progress: 37.52%\r",
      "Progress: 37.56%\r",
      "Progress: 37.6%\r",
      "Progress: 37.64%\r",
      "Progress: 37.68%\r",
      "Progress: 37.72%\r",
      "Progress: 37.76%\r",
      "Progress: 37.79%\r",
      "Progress: 37.83%\r",
      "Progress: 37.87%\r",
      "Progress: 37.91%\r",
      "Progress: 37.95%\r",
      "Progress: 37.99%\r",
      "Progress: 38.03%\r",
      "Progress: 38.07%\r",
      "Progress: 38.11%\r",
      "Progress: 38.15%\r",
      "Progress: 38.19%\r",
      "Progress: 38.23%\r",
      "Progress: 38.27%\r",
      "Progress: 38.31%\r",
      "Progress: 38.34%\r",
      "Progress: 38.38%\r",
      "Progress: 38.42%\r",
      "Progress: 38.46%\r",
      "Progress: 38.5%\r",
      "Progress: 38.54%\r",
      "Progress: 38.58%\r",
      "Progress: 38.62%\r",
      "Progress: 38.66%\r",
      "Progress: 38.7%\r",
      "Progress: 38.74%\r",
      "Progress: 38.78%\r",
      "Progress: 38.82%\r",
      "Progress: 38.86%\r",
      "Progress: 38.89%\r",
      "Progress: 38.93%\r",
      "Progress: 38.97%\r",
      "Progress: 39.01%\r",
      "Progress: 39.05%\r",
      "Progress: 39.09%\r",
      "Progress: 39.13%\r",
      "Progress: 39.17%\r",
      "Progress: 39.21%\r",
      "Progress: 39.25%\r",
      "Progress: 39.29%\r",
      "Progress: 39.33%\r",
      "Progress: 39.37%\r",
      "Progress: 39.41%\r",
      "Progress: 39.44%\r",
      "Progress: 39.48%\r",
      "Progress: 39.52%\r",
      "Progress: 39.56%\r",
      "Progress: 39.6%\r",
      "Progress: 39.64%\r",
      "Progress: 39.68%\r",
      "Progress: 39.72%\r",
      "Progress: 39.76%\r",
      "Progress: 39.8%\r",
      "Progress: 39.84%\r",
      "Progress: 39.88%\r",
      "Progress: 39.92%\r",
      "Progress: 39.96%\r",
      "Progress: 39.99%\r",
      "Progress: 40.03%\r",
      "Progress: 40.07%\r",
      "Progress: 40.11%\r",
      "Progress: 40.15%\r",
      "Progress: 40.19%\r",
      "Progress: 40.23%\r",
      "Progress: 40.27%\r",
      "Progress: 40.31%\r",
      "Progress: 40.35%\r",
      "Progress: 40.39%\r",
      "Progress: 40.43%\r",
      "Progress: 40.47%\r",
      "Progress: 40.51%\r",
      "Progress: 40.54%\r",
      "Progress: 40.58%\r",
      "Progress: 40.62%\r",
      "Progress: 40.66%\r",
      "Progress: 40.7%\r",
      "Progress: 40.74%\r",
      "Progress: 40.78%\r",
      "Progress: 40.82%\r",
      "Progress: 40.86%\r",
      "Progress: 40.9%\r",
      "Progress: 40.94%\r",
      "Progress: 40.98%\r",
      "Progress: 41.02%\r",
      "Progress: 41.06%\r",
      "Progress: 41.09%\r",
      "Progress: 41.13%\r",
      "Progress: 41.17%\r",
      "Progress: 41.21%\r",
      "Progress: 41.25%\r",
      "Progress: 41.29%\r",
      "Progress: 41.33%\r",
      "Progress: 41.37%\r",
      "Progress: 41.41%\r",
      "Progress: 41.45%\r",
      "Progress: 41.49%\r",
      "Progress: 41.53%\r",
      "Progress: 41.57%\r",
      "Progress: 41.61%\r",
      "Progress: 41.64%\r",
      "Progress: 41.68%\r",
      "Progress: 41.72%\r",
      "Progress: 41.76%\r",
      "Progress: 41.8%\r",
      "Progress: 41.84%\r",
      "Progress: 41.88%\r",
      "Progress: 41.92%\r",
      "Progress: 41.96%\r",
      "Progress: 42.0%\r",
      "Progress: 42.04%\r",
      "Progress: 42.08%\r",
      "Progress: 42.12%\r",
      "Progress: 42.16%\r",
      "Progress: 42.19%\r",
      "Progress: 42.23%\r",
      "Progress: 42.27%\r",
      "Progress: 42.31%\r",
      "Progress: 42.35%\r",
      "Progress: 42.39%\r",
      "Progress: 42.43%\r",
      "Progress: 42.47%\r",
      "Progress: 42.51%\r",
      "Progress: 42.55%\r",
      "Progress: 42.59%\r",
      "Progress: 42.63%\r",
      "Progress: 42.67%\r",
      "Progress: 42.71%\r",
      "Progress: 42.74%\r",
      "Progress: 42.78%\r",
      "Progress: 42.82%\r",
      "Progress: 42.86%\r",
      "Progress: 42.9%\r",
      "Progress: 42.94%\r",
      "Progress: 42.98%\r",
      "Progress: 43.02%\r",
      "Progress: 43.06%\r",
      "Progress: 43.1%\r",
      "Progress: 43.14%\r",
      "Progress: 43.18%\r",
      "Progress: 43.22%\r",
      "Progress: 43.26%\r",
      "Progress: 43.29%\r",
      "Progress: 43.33%\r",
      "Progress: 43.37%\r",
      "Progress: 43.41%\r",
      "Progress: 43.45%\r",
      "Progress: 43.49%\r",
      "Progress: 43.53%\r",
      "Progress: 43.57%\r",
      "Progress: 43.61%\r",
      "Progress: 43.65%\r",
      "Progress: 43.69%\r",
      "Progress: 43.73%\r",
      "Progress: 43.77%\r",
      "Progress: 43.81%\r",
      "Progress: 43.85%\r",
      "Progress: 43.88%\r",
      "Progress: 43.92%\r",
      "Progress: 43.96%\r",
      "Progress: 44.0%\r",
      "Progress: 44.04%\r",
      "Progress: 44.08%\r",
      "Progress: 44.12%\r",
      "Progress: 44.16%\r",
      "Progress: 44.2%\r",
      "Progress: 44.24%\r",
      "Progress: 44.28%\r",
      "Progress: 44.32%\r",
      "Progress: 44.36%\r",
      "Progress: 44.4%\r",
      "Progress: 44.43%\r",
      "Progress: 44.47%\r",
      "Progress: 44.51%\r",
      "Progress: 44.55%\r",
      "Progress: 44.59%\r",
      "Progress: 44.63%\r",
      "Progress: 44.67%\r",
      "Progress: 44.71%\r",
      "Progress: 44.75%\r",
      "Progress: 44.79%\r",
      "Progress: 44.83%\r",
      "Progress: 44.87%\r",
      "Progress: 44.91%\r",
      "Progress: 44.95%\r",
      "Progress: 44.98%\r",
      "Progress: 45.02%\r",
      "Progress: 45.06%\r",
      "Progress: 45.1%\r",
      "Progress: 45.14%\r",
      "Progress: 45.18%\r",
      "Progress: 45.22%\r",
      "Progress: 45.26%\r",
      "Progress: 45.3%\r",
      "Progress: 45.34%\r",
      "Progress: 45.38%\r",
      "Progress: 45.42%\r",
      "Progress: 45.46%\r",
      "Progress: 45.5%\r",
      "Progress: 45.53%\r",
      "Progress: 45.57%\r",
      "Progress: 45.61%\r",
      "Progress: 45.65%\r",
      "Progress: 45.69%\r",
      "Progress: 45.73%\r",
      "Progress: 45.77%\r",
      "Progress: 45.81%\r",
      "Progress: 45.85%\r",
      "Progress: 45.89%\r",
      "Progress: 45.93%\r",
      "Progress: 45.97%\r",
      "Progress: 46.01%\r",
      "Progress: 46.05%\r",
      "Progress: 46.08%\r",
      "Progress: 46.12%\r",
      "Progress: 46.16%\r",
      "Progress: 46.2%\r",
      "Progress: 46.24%\r",
      "Progress: 46.28%\r",
      "Progress: 46.32%\r",
      "Progress: 46.36%\r",
      "Progress: 46.4%\r",
      "Progress: 46.44%\r",
      "Progress: 46.48%\r",
      "Progress: 46.52%\r",
      "Progress: 46.56%\r",
      "Progress: 46.6%\r",
      "Progress: 46.63%\r",
      "Progress: 46.67%\r",
      "Progress: 46.71%\r",
      "Progress: 46.75%\r",
      "Progress: 46.79%\r",
      "Progress: 46.83%\r",
      "Progress: 46.87%\r",
      "Progress: 46.91%\r",
      "Progress: 46.95%\r",
      "Progress: 46.99%\r",
      "Progress: 47.03%\r",
      "Progress: 47.07%\r",
      "Progress: 47.11%\r",
      "Progress: 47.15%\r",
      "Progress: 47.18%\r",
      "Progress: 47.22%\r",
      "Progress: 47.26%\r",
      "Progress: 47.3%\r",
      "Progress: 47.34%\r",
      "Progress: 47.38%\r",
      "Progress: 47.42%\r",
      "Progress: 47.46%\r",
      "Progress: 47.5%\r",
      "Progress: 47.54%\r",
      "Progress: 47.58%\r",
      "Progress: 47.62%\r",
      "Progress: 47.66%\r",
      "Progress: 47.7%\r",
      "Progress: 47.73%\r",
      "Progress: 47.77%\r",
      "Progress: 47.81%\r",
      "Progress: 47.85%\r",
      "Progress: 47.89%\r",
      "Progress: 47.93%\r",
      "Progress: 47.97%\r",
      "Progress: 48.01%\r",
      "Progress: 48.05%\r",
      "Progress: 48.09%\r",
      "Progress: 48.13%\r",
      "Progress: 48.17%\r",
      "Progress: 48.21%\r",
      "Progress: 48.25%\r",
      "Progress: 48.28%\r",
      "Progress: 48.32%\r",
      "Progress: 48.36%\r",
      "Progress: 48.4%\r",
      "Progress: 48.44%\r",
      "Progress: 48.48%\r",
      "Progress: 48.52%\r",
      "Progress: 48.56%\r",
      "Progress: 48.6%\r",
      "Progress: 48.64%\r",
      "Progress: 48.68%\r",
      "Progress: 48.72%\r",
      "Progress: 48.76%\r",
      "Progress: 48.8%\r",
      "Progress: 48.83%\r",
      "Progress: 48.87%\r",
      "Progress: 48.91%\r",
      "Progress: 48.95%\r",
      "Progress: 48.99%\r",
      "Progress: 49.03%\r",
      "Progress: 49.07%\r",
      "Progress: 49.11%\r",
      "Progress: 49.15%\r",
      "Progress: 49.19%\r",
      "Progress: 49.23%\r",
      "Progress: 49.27%\r",
      "Progress: 49.31%\r",
      "Progress: 49.35%\r",
      "Progress: 49.38%\r",
      "Progress: 49.42%\r",
      "Progress: 49.46%\r",
      "Progress: 49.5%\r",
      "Progress: 49.54%\r",
      "Progress: 49.58%\r",
      "Progress: 49.62%\r",
      "Progress: 49.66%\r",
      "Progress: 49.7%\r",
      "Progress: 49.74%\r",
      "Progress: 49.78%\r",
      "Progress: 49.82%\r",
      "Progress: 49.86%\r",
      "Progress: 49.9%\r",
      "Progress: 49.93%\r",
      "Progress: 49.97%\r",
      "Progress: 50.01%\r",
      "Progress: 50.05%\r",
      "Progress: 50.09%\r",
      "Progress: 50.13%\r",
      "Progress: 50.17%\r",
      "Progress: 50.21%\r",
      "Progress: 50.25%\r",
      "Progress: 50.29%\r",
      "Progress: 50.33%\r",
      "Progress: 50.37%\r",
      "Progress: 50.41%\r",
      "Progress: 50.45%\r",
      "Progress: 50.48%\r",
      "Progress: 50.52%\r",
      "Progress: 50.56%\r",
      "Progress: 50.6%\r",
      "Progress: 50.64%\r",
      "Progress: 50.68%\r",
      "Progress: 50.72%\r",
      "Progress: 50.76%\r",
      "Progress: 50.8%\r",
      "Progress: 50.84%\r",
      "Progress: 50.88%\r",
      "Progress: 50.92%\r",
      "Progress: 50.96%\r",
      "Progress: 51.0%\r",
      "Progress: 51.03%\r",
      "Progress: 51.07%\r",
      "Progress: 51.11%\r",
      "Progress: 51.15%\r",
      "Progress: 51.19%\r",
      "Progress: 51.23%\r",
      "Progress: 51.27%\r",
      "Progress: 51.31%\r",
      "Progress: 51.35%\r",
      "Progress: 51.39%\r",
      "Progress: 51.43%\r",
      "Progress: 51.47%\r",
      "Progress: 51.51%\r",
      "Progress: 51.55%\r",
      "Progress: 51.58%\r",
      "Progress: 51.62%\r",
      "Progress: 51.66%\r",
      "Progress: 51.7%\r",
      "Progress: 51.74%\r",
      "Progress: 51.78%\r",
      "Progress: 51.82%\r",
      "Progress: 51.86%\r",
      "Progress: 51.9%\r",
      "Progress: 51.94%\r",
      "Progress: 51.98%\r",
      "Progress: 52.02%\r",
      "Progress: 52.06%\r",
      "Progress: 52.1%\r",
      "Progress: 52.13%\r",
      "Progress: 52.17%\r",
      "Progress: 52.21%\r",
      "Progress: 52.25%\r",
      "Progress: 52.29%\r",
      "Progress: 52.33%\r",
      "Progress: 52.37%\r",
      "Progress: 52.41%\r",
      "Progress: 52.45%\r",
      "Progress: 52.49%\r",
      "Progress: 52.53%\r",
      "Progress: 52.57%\r",
      "Progress: 52.61%\r",
      "Progress: 52.65%\r",
      "Progress: 52.68%\r",
      "Progress: 52.72%\r",
      "Progress: 52.76%\r",
      "Progress: 52.8%\r",
      "Progress: 52.84%\r",
      "Progress: 52.88%\r",
      "Progress: 52.92%\r",
      "Progress: 52.96%\r",
      "Progress: 53.0%\r",
      "Progress: 53.04%\r",
      "Progress: 53.08%\r",
      "Progress: 53.12%\r",
      "Progress: 53.16%\r",
      "Progress: 53.2%\r",
      "Progress: 53.23%\r",
      "Progress: 53.27%\r",
      "Progress: 53.31%\r",
      "Progress: 53.35%\r",
      "Progress: 53.39%\r",
      "Progress: 53.43%\r",
      "Progress: 53.47%\r",
      "Progress: 53.51%\r",
      "Progress: 53.55%\r",
      "Progress: 53.59%\r",
      "Progress: 53.63%\r",
      "Progress: 53.67%\r",
      "Progress: 53.71%\r",
      "Progress: 53.75%\r",
      "Progress: 53.78%\r",
      "Progress: 53.82%\r",
      "Progress: 53.86%\r",
      "Progress: 53.9%\r",
      "Progress: 53.94%\r",
      "Progress: 53.98%\r",
      "Progress: 54.02%\r",
      "Progress: 54.06%\r",
      "Progress: 54.1%\r",
      "Progress: 54.14%\r",
      "Progress: 54.18%\r",
      "Progress: 54.22%\r",
      "Progress: 54.26%\r",
      "Progress: 54.3%\r",
      "Progress: 54.33%\r",
      "Progress: 54.37%\r",
      "Progress: 54.41%\r",
      "Progress: 54.45%\r",
      "Progress: 54.49%\r",
      "Progress: 54.53%\r",
      "Progress: 54.57%\r",
      "Progress: 54.61%\r",
      "Progress: 54.65%\r",
      "Progress: 54.69%\r",
      "Progress: 54.73%\r",
      "Progress: 54.77%\r",
      "Progress: 54.81%\r",
      "Progress: 54.85%\r",
      "Progress: 54.88%\r",
      "Progress: 54.92%\r",
      "Progress: 54.96%\r",
      "Progress: 55.0%\r",
      "Progress: 55.04%\r",
      "Progress: 55.08%\r",
      "Progress: 55.12%\r",
      "Progress: 55.16%\r",
      "Progress: 55.2%\r",
      "Progress: 55.24%\r",
      "Progress: 55.28%\r",
      "Progress: 55.32%\r",
      "Progress: 55.36%\r",
      "Progress: 55.4%\r",
      "Progress: 55.43%\r",
      "Progress: 55.47%\r",
      "Progress: 55.51%\r",
      "Progress: 55.55%\r",
      "Progress: 55.59%\r",
      "Progress: 55.63%\r",
      "Progress: 55.67%\r",
      "Progress: 55.71%\r",
      "Progress: 55.75%\r",
      "Progress: 55.79%\r",
      "Progress: 55.83%\r",
      "Progress: 55.87%\r",
      "Progress: 55.91%\r",
      "Progress: 55.95%\r",
      "Progress: 55.98%\r",
      "Progress: 56.02%\r",
      "Progress: 56.06%\r",
      "Progress: 56.1%\r",
      "Progress: 56.14%\r",
      "Progress: 56.18%\r",
      "Progress: 56.22%\r",
      "Progress: 56.26%\r",
      "Progress: 56.3%\r",
      "Progress: 56.34%\r",
      "Progress: 56.38%\r",
      "Progress: 56.42%\r",
      "Progress: 56.46%\r",
      "Progress: 56.5%\r",
      "Progress: 56.53%\r",
      "Progress: 56.57%\r",
      "Progress: 56.61%\r",
      "Progress: 56.65%\r",
      "Progress: 56.69%\r",
      "Progress: 56.73%\r",
      "Progress: 56.77%\r",
      "Progress: 56.81%\r",
      "Progress: 56.85%\r",
      "Progress: 56.89%\r",
      "Progress: 56.93%\r",
      "Progress: 56.97%\r",
      "Progress: 57.01%\r",
      "Progress: 57.05%\r",
      "Progress: 57.08%\r",
      "Progress: 57.12%\r",
      "Progress: 57.16%\r",
      "Progress: 57.2%\r",
      "Progress: 57.24%\r",
      "Progress: 57.28%\r",
      "Progress: 57.32%\r",
      "Progress: 57.36%\r",
      "Progress: 57.4%\r",
      "Progress: 57.44%\r",
      "Progress: 57.48%\r",
      "Progress: 57.52%\r",
      "Progress: 57.56%\r",
      "Progress: 57.6%\r",
      "Progress: 57.63%\r",
      "Progress: 57.67%\r",
      "Progress: 57.71%\r",
      "Progress: 57.75%\r",
      "Progress: 57.79%\r",
      "Progress: 57.83%\r",
      "Progress: 57.87%\r",
      "Progress: 57.91%\r",
      "Progress: 57.95%\r",
      "Progress: 57.99%\r",
      "Progress: 58.03%\r",
      "Progress: 58.07%\r",
      "Progress: 58.11%\r",
      "Progress: 58.15%\r",
      "Progress: 58.18%\r",
      "Progress: 58.22%\r",
      "Progress: 58.26%\r",
      "Progress: 58.3%\r",
      "Progress: 58.34%\r",
      "Progress: 58.38%\r",
      "Progress: 58.42%\r",
      "Progress: 58.46%\r",
      "Progress: 58.5%\r",
      "Progress: 58.54%\r",
      "Progress: 58.58%\r",
      "Progress: 58.62%\r",
      "Progress: 58.66%\r",
      "Progress: 58.7%\r",
      "Progress: 58.74%\r",
      "Progress: 58.77%\r",
      "Progress: 58.81%\r",
      "Progress: 58.85%\r",
      "Progress: 58.89%\r",
      "Progress: 58.93%\r",
      "Progress: 58.97%\r",
      "Progress: 59.01%\r",
      "Progress: 59.05%\r",
      "Progress: 59.09%\r",
      "Progress: 59.13%\r",
      "Progress: 59.17%\r",
      "Progress: 59.21%\r",
      "Progress: 59.25%\r",
      "Progress: 59.29%\r",
      "Progress: 59.32%\r",
      "Progress: 59.36%\r",
      "Progress: 59.4%\r",
      "Progress: 59.44%\r",
      "Progress: 59.48%\r",
      "Progress: 59.52%\r",
      "Progress: 59.56%\r",
      "Progress: 59.6%\r",
      "Progress: 59.64%\r",
      "Progress: 59.68%\r",
      "Progress: 59.72%\r",
      "Progress: 59.76%\r",
      "Progress: 59.8%\r",
      "Progress: 59.84%\r",
      "Progress: 59.87%\r",
      "Progress: 59.91%\r",
      "Progress: 59.95%\r",
      "Progress: 59.99%\r",
      "Progress: 60.03%\r",
      "Progress: 60.07%\r",
      "Progress: 60.11%\r",
      "Progress: 60.15%\r",
      "Progress: 60.19%\r",
      "Progress: 60.23%\r",
      "Progress: 60.27%\r",
      "Progress: 60.31%\r",
      "Progress: 60.35%\r",
      "Progress: 60.39%\r",
      "Progress: 60.42%\r",
      "Progress: 60.46%\r",
      "Progress: 60.5%\r",
      "Progress: 60.54%\r",
      "Progress: 60.58%\r",
      "Progress: 60.62%\r",
      "Progress: 60.66%\r",
      "Progress: 60.7%\r",
      "Progress: 60.74%\r",
      "Progress: 60.78%\r",
      "Progress: 60.82%\r",
      "Progress: 60.86%\r",
      "Progress: 60.9%\r",
      "Progress: 60.94%\r",
      "Progress: 60.97%\r",
      "Progress: 61.01%\r",
      "Progress: 61.05%\r",
      "Progress: 61.09%\r",
      "Progress: 61.13%\r",
      "Progress: 61.17%\r",
      "Progress: 61.21%\r",
      "Progress: 61.25%\r",
      "Progress: 61.29%\r",
      "Progress: 61.33%\r",
      "Progress: 61.37%\r",
      "Progress: 61.41%\r",
      "Progress: 61.45%\r",
      "Progress: 61.49%\r",
      "Progress: 61.52%\r",
      "Progress: 61.56%\r",
      "Progress: 61.6%\r",
      "Progress: 61.64%\r",
      "Progress: 61.68%\r",
      "Progress: 61.72%\r",
      "Progress: 61.76%\r",
      "Progress: 61.8%\r",
      "Progress: 61.84%\r",
      "Progress: 61.88%\r",
      "Progress: 61.92%\r",
      "Progress: 61.96%\r",
      "Progress: 62.0%\r",
      "Progress: 62.04%\r",
      "Progress: 62.07%\r",
      "Progress: 62.11%\r",
      "Progress: 62.15%\r",
      "Progress: 62.19%\r",
      "Progress: 62.23%\r",
      "Progress: 62.27%\r",
      "Progress: 62.31%\r",
      "Progress: 62.35%\r",
      "Progress: 62.39%\r",
      "Progress: 62.43%\r",
      "Progress: 62.47%\r",
      "Progress: 62.51%\r",
      "Progress: 62.55%\r",
      "Progress: 62.59%\r",
      "Progress: 62.62%\r",
      "Progress: 62.66%\r",
      "Progress: 62.7%\r",
      "Progress: 62.74%\r",
      "Progress: 62.78%\r",
      "Progress: 62.82%\r",
      "Progress: 62.86%\r",
      "Progress: 62.9%\r",
      "Progress: 62.94%\r",
      "Progress: 62.98%\r",
      "Progress: 63.02%\r",
      "Progress: 63.06%\r",
      "Progress: 63.1%\r",
      "Progress: 63.14%\r",
      "Progress: 63.17%\r",
      "Progress: 63.21%\r",
      "Progress: 63.25%\r",
      "Progress: 63.29%\r",
      "Progress: 63.33%\r",
      "Progress: 63.37%\r",
      "Progress: 63.41%\r",
      "Progress: 63.45%\r",
      "Progress: 63.49%\r",
      "Progress: 63.53%\r",
      "Progress: 63.57%\r",
      "Progress: 63.61%\r",
      "Progress: 63.65%\r",
      "Progress: 63.69%\r",
      "Progress: 63.72%\r",
      "Progress: 63.76%\r",
      "Progress: 63.8%\r",
      "Progress: 63.84%\r",
      "Progress: 63.88%\r",
      "Progress: 63.92%\r",
      "Progress: 63.96%\r",
      "Progress: 64.0%\r",
      "Progress: 64.04%\r",
      "Progress: 64.08%\r",
      "Progress: 64.12%\r",
      "Progress: 64.16%\r",
      "Progress: 64.2%\r",
      "Progress: 64.24%\r",
      "Progress: 64.27%\r",
      "Progress: 64.31%\r",
      "Progress: 64.35%\r",
      "Progress: 64.39%\r",
      "Progress: 64.43%\r",
      "Progress: 64.47%\r",
      "Progress: 64.51%\r",
      "Progress: 64.55%\r",
      "Progress: 64.59%\r",
      "Progress: 64.63%\r",
      "Progress: 64.67%\r",
      "Progress: 64.71%\r",
      "Progress: 64.75%\r",
      "Progress: 64.79%\r",
      "Progress: 64.82%\r",
      "Progress: 64.86%\r",
      "Progress: 64.9%\r",
      "Progress: 64.94%\r",
      "Progress: 64.98%\r",
      "Progress: 65.02%\r",
      "Progress: 65.06%\r",
      "Progress: 65.1%\r",
      "Progress: 65.14%\r",
      "Progress: 65.18%\r",
      "Progress: 65.22%\r",
      "Progress: 65.26%\r",
      "Progress: 65.3%\r",
      "Progress: 65.34%\r",
      "Progress: 65.37%\r",
      "Progress: 65.41%\r",
      "Progress: 65.45%\r",
      "Progress: 65.49%\r",
      "Progress: 65.53%\r",
      "Progress: 65.57%\r",
      "Progress: 65.61%\r",
      "Progress: 65.65%\r",
      "Progress: 65.69%\r",
      "Progress: 65.73%\r",
      "Progress: 65.77%\r",
      "Progress: 65.81%\r",
      "Progress: 65.85%\r",
      "Progress: 65.89%\r",
      "Progress: 65.92%\r",
      "Progress: 65.96%\r",
      "Progress: 66.0%\r",
      "Progress: 66.04%\r",
      "Progress: 66.08%\r",
      "Progress: 66.12%\r",
      "Progress: 66.16%\r",
      "Progress: 66.2%\r",
      "Progress: 66.24%\r",
      "Progress: 66.28%\r",
      "Progress: 66.32%\r",
      "Progress: 66.36%\r",
      "Progress: 66.4%\r",
      "Progress: 66.44%\r",
      "Progress: 66.47%\r",
      "Progress: 66.51%\r",
      "Progress: 66.55%\r",
      "Progress: 66.59%\r",
      "Progress: 66.63%\r",
      "Progress: 66.67%\r",
      "Progress: 66.71%\r",
      "Progress: 66.75%\r",
      "Progress: 66.79%\r",
      "Progress: 66.83%\r",
      "Progress: 66.87%\r",
      "Progress: 66.91%\r",
      "Progress: 66.95%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 66.99%\r",
      "Progress: 67.02%\r",
      "Progress: 67.06%\r",
      "Progress: 67.1%\r",
      "Progress: 67.14%\r",
      "Progress: 67.18%\r",
      "Progress: 67.22%\r",
      "Progress: 67.26%\r",
      "Progress: 67.3%\r",
      "Progress: 67.34%\r",
      "Progress: 67.38%\r",
      "Progress: 67.42%\r",
      "Progress: 67.46%\r",
      "Progress: 67.5%\r",
      "Progress: 67.54%\r",
      "Progress: 67.57%\r",
      "Progress: 67.61%\r",
      "Progress: 67.65%\r",
      "Progress: 67.69%\r",
      "Progress: 67.73%\r",
      "Progress: 67.77%\r",
      "Progress: 67.81%\r",
      "Progress: 67.85%\r",
      "Progress: 67.89%\r",
      "Progress: 67.93%\r",
      "Progress: 67.97%\r",
      "Progress: 68.01%\r",
      "Progress: 68.05%\r",
      "Progress: 68.09%\r",
      "Progress: 68.12%\r",
      "Progress: 68.16%\r",
      "Progress: 68.2%\r",
      "Progress: 68.24%\r",
      "Progress: 68.28%\r",
      "Progress: 68.32%\r",
      "Progress: 68.36%\r",
      "Progress: 68.4%\r",
      "Progress: 68.44%\r",
      "Progress: 68.48%\r",
      "Progress: 68.52%\r",
      "Progress: 68.56%\r",
      "Progress: 68.6%\r",
      "Progress: 68.64%\r",
      "Progress: 68.67%\r",
      "Progress: 68.71%\r",
      "Progress: 68.75%\r",
      "Progress: 68.79%\r",
      "Progress: 68.83%\r",
      "Progress: 68.87%\r",
      "Progress: 68.91%\r",
      "Progress: 68.95%\r",
      "Progress: 68.99%\r",
      "Progress: 69.03%\r",
      "Progress: 69.07%\r",
      "Progress: 69.11%\r",
      "Progress: 69.15%\r",
      "Progress: 69.19%\r",
      "Progress: 69.22%\r",
      "Progress: 69.26%\r",
      "Progress: 69.3%\r",
      "Progress: 69.34%\r",
      "Progress: 69.38%\r",
      "Progress: 69.42%\r",
      "Progress: 69.46%\r",
      "Progress: 69.5%\r",
      "Progress: 69.54%\r",
      "Progress: 69.58%\r",
      "Progress: 69.62%\r",
      "Progress: 69.66%\r",
      "Progress: 69.7%\r",
      "Progress: 69.74%\r",
      "Progress: 69.77%\r",
      "Progress: 69.81%\r",
      "Progress: 69.85%\r",
      "Progress: 69.89%\r",
      "Progress: 69.93%\r",
      "Progress: 69.97%\r",
      "Progress: 70.01%\r",
      "Progress: 70.05%\r",
      "Progress: 70.09%\r",
      "Progress: 70.13%\r",
      "Progress: 70.17%\r",
      "Progress: 70.21%\r",
      "Progress: 70.25%\r",
      "Progress: 70.29%\r",
      "Progress: 70.32%\r",
      "Progress: 70.36%\r",
      "Progress: 70.4%\r",
      "Progress: 70.44%\r",
      "Progress: 70.48%\r",
      "Progress: 70.52%\r",
      "Progress: 70.56%\r",
      "Progress: 70.6%\r",
      "Progress: 70.64%\r",
      "Progress: 70.68%\r",
      "Progress: 70.72%\r",
      "Progress: 70.76%\r",
      "Progress: 70.8%\r",
      "Progress: 70.84%\r",
      "Progress: 70.87%\r",
      "Progress: 70.91%\r",
      "Progress: 70.95%\r",
      "Progress: 70.99%\r",
      "Progress: 71.03%\r",
      "Progress: 71.07%\r",
      "Progress: 71.11%\r",
      "Progress: 71.15%\r",
      "Progress: 71.19%\r",
      "Progress: 71.23%\r",
      "Progress: 71.27%\r",
      "Progress: 71.31%\r",
      "Progress: 71.35%\r",
      "Progress: 71.39%\r",
      "Progress: 71.42%\r",
      "Progress: 71.46%\r",
      "Progress: 71.5%\r",
      "Progress: 71.54%\r",
      "Progress: 71.58%\r",
      "Progress: 71.62%\r",
      "Progress: 71.66%\r",
      "Progress: 71.7%\r",
      "Progress: 71.74%\r",
      "Progress: 71.78%\r",
      "Progress: 71.82%\r",
      "Progress: 71.86%\r",
      "Progress: 71.9%\r",
      "Progress: 71.94%\r",
      "Progress: 71.97%\r",
      "Progress: 72.01%\r",
      "Progress: 72.05%\r",
      "Progress: 72.09%\r",
      "Progress: 72.13%\r",
      "Progress: 72.17%\r",
      "Progress: 72.21%\r",
      "Progress: 72.25%\r",
      "Progress: 72.29%\r",
      "Progress: 72.33%\r",
      "Progress: 72.37%\r",
      "Progress: 72.41%\r",
      "Progress: 72.45%\r",
      "Progress: 72.49%\r",
      "Progress: 72.52%\r",
      "Progress: 72.56%\r",
      "Progress: 72.6%\r",
      "Progress: 72.64%\r",
      "Progress: 72.68%\r",
      "Progress: 72.72%\r",
      "Progress: 72.76%\r",
      "Progress: 72.8%\r",
      "Progress: 72.84%\r",
      "Progress: 72.88%\r",
      "Progress: 72.92%\r",
      "Progress: 72.96%\r",
      "Progress: 73.0%\r",
      "Progress: 73.04%\r",
      "Progress: 73.08%\r",
      "Progress: 73.11%\r",
      "Progress: 73.15%\r",
      "Progress: 73.19%\r",
      "Progress: 73.23%\r",
      "Progress: 73.27%\r",
      "Progress: 73.31%\r",
      "Progress: 73.35%\r",
      "Progress: 73.39%\r",
      "Progress: 73.43%\r",
      "Progress: 73.47%\r",
      "Progress: 73.51%\r",
      "Progress: 73.55%\r",
      "Progress: 73.59%\r",
      "Progress: 73.63%\r",
      "Progress: 73.66%\r",
      "Progress: 73.7%\r",
      "Progress: 73.74%\r",
      "Progress: 73.78%\r",
      "Progress: 73.82%\r",
      "Progress: 73.86%\r",
      "Progress: 73.9%\r",
      "Progress: 73.94%\r",
      "Progress: 73.98%\r",
      "Progress: 74.02%\r",
      "Progress: 74.06%\r",
      "Progress: 74.1%\r",
      "Progress: 74.14%\r",
      "Progress: 74.18%\r",
      "Progress: 74.21%\r",
      "Progress: 74.25%\r",
      "Progress: 74.29%\r",
      "Progress: 74.33%\r",
      "Progress: 74.37%\r",
      "Progress: 74.41%\r",
      "Progress: 74.45%\r",
      "Progress: 74.49%\r",
      "Progress: 74.53%\r",
      "Progress: 74.57%\r",
      "Progress: 74.61%\r",
      "Progress: 74.65%\r",
      "Progress: 74.69%\r",
      "Progress: 74.73%\r",
      "Progress: 74.76%\r",
      "Progress: 74.8%\r",
      "Progress: 74.84%\r",
      "Progress: 74.88%\r",
      "Progress: 74.92%\r",
      "Progress: 74.96%\r",
      "Progress: 75.0%\r",
      "Progress: 75.04%\r",
      "Progress: 75.08%\r",
      "Progress: 75.12%\r",
      "Progress: 75.16%\r",
      "Progress: 75.2%\r",
      "Progress: 75.24%\r",
      "Progress: 75.28%\r",
      "Progress: 75.31%\r",
      "Progress: 75.35%\r",
      "Progress: 75.39%\r",
      "Progress: 75.43%\r",
      "Progress: 75.47%\r",
      "Progress: 75.51%\r",
      "Progress: 75.55%\r",
      "Progress: 75.59%\r",
      "Progress: 75.63%\r",
      "Progress: 75.67%\r",
      "Progress: 75.71%\r",
      "Progress: 75.75%\r",
      "Progress: 75.79%\r",
      "Progress: 75.83%\r",
      "Progress: 75.86%\r",
      "Progress: 75.9%\r",
      "Progress: 75.94%\r",
      "Progress: 75.98%\r",
      "Progress: 76.02%\r",
      "Progress: 76.06%\r",
      "Progress: 76.1%\r",
      "Progress: 76.14%\r",
      "Progress: 76.18%\r",
      "Progress: 76.22%\r",
      "Progress: 76.26%\r",
      "Progress: 76.3%\r",
      "Progress: 76.34%\r",
      "Progress: 76.38%\r",
      "Progress: 76.41%\r",
      "Progress: 76.45%\r",
      "Progress: 76.49%\r",
      "Progress: 76.53%\r",
      "Progress: 76.57%\r",
      "Progress: 76.61%\r",
      "Progress: 76.65%\r",
      "Progress: 76.69%\r",
      "Progress: 76.73%\r",
      "Progress: 76.77%\r",
      "Progress: 76.81%\r",
      "Progress: 76.85%\r",
      "Progress: 76.89%\r",
      "Progress: 76.93%\r",
      "Progress: 76.96%\r",
      "Progress: 77.0%\r",
      "Progress: 77.04%\r",
      "Progress: 77.08%\r",
      "Progress: 77.12%\r",
      "Progress: 77.16%\r",
      "Progress: 77.2%\r",
      "Progress: 77.24%\r",
      "Progress: 77.28%\r",
      "Progress: 77.32%\r",
      "Progress: 77.36%\r",
      "Progress: 77.4%\r",
      "Progress: 77.44%\r",
      "Progress: 77.48%\r",
      "Progress: 77.51%\r",
      "Progress: 77.55%\r",
      "Progress: 77.59%\r",
      "Progress: 77.63%\r",
      "Progress: 77.67%\r",
      "Progress: 77.71%\r",
      "Progress: 77.75%\r",
      "Progress: 77.79%\r",
      "Progress: 77.83%\r",
      "Progress: 77.87%\r",
      "Progress: 77.91%\r",
      "Progress: 77.95%\r",
      "Progress: 77.99%\r",
      "Progress: 78.03%\r",
      "Progress: 78.06%\r",
      "Progress: 78.1%\r",
      "Progress: 78.14%\r",
      "Progress: 78.18%\r",
      "Progress: 78.22%\r",
      "Progress: 78.26%\r",
      "Progress: 78.3%\r",
      "Progress: 78.34%\r",
      "Progress: 78.38%\r",
      "Progress: 78.42%\r",
      "Progress: 78.46%\r",
      "Progress: 78.5%\r",
      "Progress: 78.54%\r",
      "Progress: 78.58%\r",
      "Progress: 78.61%\r",
      "Progress: 78.65%\r",
      "Progress: 78.69%\r",
      "Progress: 78.73%\r",
      "Progress: 78.77%\r",
      "Progress: 78.81%\r",
      "Progress: 78.85%\r",
      "Progress: 78.89%\r",
      "Progress: 78.93%\r",
      "Progress: 78.97%\r",
      "Progress: 79.01%\r",
      "Progress: 79.05%\r",
      "Progress: 79.09%\r",
      "Progress: 79.13%\r",
      "Progress: 79.16%\r",
      "Progress: 79.2%\r",
      "Progress: 79.24%\r",
      "Progress: 79.28%\r",
      "Progress: 79.32%\r",
      "Progress: 79.36%\r",
      "Progress: 79.4%\r",
      "Progress: 79.44%\r",
      "Progress: 79.48%\r",
      "Progress: 79.52%\r",
      "Progress: 79.56%\r",
      "Progress: 79.6%\r",
      "Progress: 79.64%\r",
      "Progress: 79.68%\r",
      "Progress: 79.71%\r",
      "Progress: 79.75%\r",
      "Progress: 79.79%\r",
      "Progress: 79.83%\r",
      "Progress: 79.87%\r",
      "Progress: 79.91%\r",
      "Progress: 79.95%\r",
      "Progress: 79.99%\r",
      "Progress: 80.03%\r",
      "Progress: 80.07%\r",
      "Progress: 80.11%\r",
      "Progress: 80.15%\r",
      "Progress: 80.19%\r",
      "Progress: 80.23%\r",
      "Progress: 80.26%\r",
      "Progress: 80.3%\r",
      "Progress: 80.34%\r",
      "Progress: 80.38%\r",
      "Progress: 80.42%\r",
      "Progress: 80.46%\r",
      "Progress: 80.5%\r",
      "Progress: 80.54%\r",
      "Progress: 80.58%\r",
      "Progress: 80.62%\r",
      "Progress: 80.66%\r",
      "Progress: 80.7%\r",
      "Progress: 80.74%\r",
      "Progress: 80.78%\r",
      "Progress: 80.81%\r",
      "Progress: 80.85%\r",
      "Progress: 80.89%\r",
      "Progress: 80.93%\r",
      "Progress: 80.97%\r",
      "Progress: 81.01%\r",
      "Progress: 81.05%\r",
      "Progress: 81.09%\r",
      "Progress: 81.13%\r",
      "Progress: 81.17%\r",
      "Progress: 81.21%\r",
      "Progress: 81.25%\r",
      "Progress: 81.29%\r",
      "Progress: 81.33%\r",
      "Progress: 81.36%\r",
      "Progress: 81.4%\r",
      "Progress: 81.44%\r",
      "Progress: 81.48%\r",
      "Progress: 81.52%\r",
      "Progress: 81.56%\r",
      "Progress: 81.6%\r",
      "Progress: 81.64%\r",
      "Progress: 81.68%\r",
      "Progress: 81.72%\r",
      "Progress: 81.76%\r",
      "Progress: 81.8%\r",
      "Progress: 81.84%\r",
      "Progress: 81.88%\r",
      "Progress: 81.91%\r",
      "Progress: 81.95%\r",
      "Progress: 81.99%\r",
      "Progress: 82.03%\r",
      "Progress: 82.07%\r",
      "Progress: 82.11%\r",
      "Progress: 82.15%\r",
      "Progress: 82.19%\r",
      "Progress: 82.23%\r",
      "Progress: 82.27%\r",
      "Progress: 82.31%\r",
      "Progress: 82.35%\r",
      "Progress: 82.39%\r",
      "Progress: 82.43%\r",
      "Progress: 82.46%\r",
      "Progress: 82.5%\r",
      "Progress: 82.54%\r",
      "Progress: 82.58%\r",
      "Progress: 82.62%\r",
      "Progress: 82.66%\r",
      "Progress: 82.7%\r",
      "Progress: 82.74%\r",
      "Progress: 82.78%\r",
      "Progress: 82.82%\r",
      "Progress: 82.86%\r",
      "Progress: 82.9%\r",
      "Progress: 82.94%\r",
      "Progress: 82.98%\r",
      "Progress: 83.01%\r",
      "Progress: 83.05%\r",
      "Progress: 83.09%\r",
      "Progress: 83.13%\r",
      "Progress: 83.17%\r",
      "Progress: 83.21%\r",
      "Progress: 83.25%\r",
      "Progress: 83.29%\r",
      "Progress: 83.33%\r",
      "Progress: 83.37%\r",
      "Progress: 83.41%\r",
      "Progress: 83.45%\r",
      "Progress: 83.49%\r",
      "Progress: 83.53%\r",
      "Progress: 83.56%\r",
      "Progress: 83.6%\r",
      "Progress: 83.64%\r",
      "Progress: 83.68%\r",
      "Progress: 83.72%\r",
      "Progress: 83.76%\r",
      "Progress: 83.8%\r",
      "Progress: 83.84%\r",
      "Progress: 83.88%\r",
      "Progress: 83.92%\r",
      "Progress: 83.96%\r",
      "Progress: 84.0%\r",
      "Progress: 84.04%\r",
      "Progress: 84.08%\r",
      "Progress: 84.11%\r",
      "Progress: 84.15%\r",
      "Progress: 84.19%\r",
      "Progress: 84.23%\r",
      "Progress: 84.27%\r",
      "Progress: 84.31%\r",
      "Progress: 84.35%\r",
      "Progress: 84.39%\r",
      "Progress: 84.43%\r",
      "Progress: 84.47%\r",
      "Progress: 84.51%\r",
      "Progress: 84.55%\r",
      "Progress: 84.59%\r",
      "Progress: 84.63%\r",
      "Progress: 84.66%\r",
      "Progress: 84.7%\r",
      "Progress: 84.74%\r",
      "Progress: 84.78%\r",
      "Progress: 84.82%\r",
      "Progress: 84.86%\r",
      "Progress: 84.9%\r",
      "Progress: 84.94%\r",
      "Progress: 84.98%\r",
      "Progress: 85.02%\r",
      "Progress: 85.06%\r",
      "Progress: 85.1%\r",
      "Progress: 85.14%\r",
      "Progress: 85.18%\r",
      "Progress: 85.21%\r",
      "Progress: 85.25%\r",
      "Progress: 85.29%\r",
      "Progress: 85.33%\r",
      "Progress: 85.37%\r",
      "Progress: 85.41%\r",
      "Progress: 85.45%\r",
      "Progress: 85.49%\r",
      "Progress: 85.53%\r",
      "Progress: 85.57%\r",
      "Progress: 85.61%\r",
      "Progress: 85.65%\r",
      "Progress: 85.69%\r",
      "Progress: 85.73%\r",
      "Progress: 85.76%\r",
      "Progress: 85.8%\r",
      "Progress: 85.84%\r",
      "Progress: 85.88%\r",
      "Progress: 85.92%\r",
      "Progress: 85.96%\r",
      "Progress: 86.0%\r",
      "Progress: 86.04%\r",
      "Progress: 86.08%\r",
      "Progress: 86.12%\r",
      "Progress: 86.16%\r",
      "Progress: 86.2%\r",
      "Progress: 86.24%\r",
      "Progress: 86.28%\r",
      "Progress: 86.31%\r",
      "Progress: 86.35%\r",
      "Progress: 86.39%\r",
      "Progress: 86.43%\r",
      "Progress: 86.47%\r",
      "Progress: 86.51%\r",
      "Progress: 86.55%\r",
      "Progress: 86.59%\r",
      "Progress: 86.63%\r",
      "Progress: 86.67%\r",
      "Progress: 86.71%\r",
      "Progress: 86.75%\r",
      "Progress: 86.79%\r",
      "Progress: 86.83%\r",
      "Progress: 86.86%\r",
      "Progress: 86.9%\r",
      "Progress: 86.94%\r",
      "Progress: 86.98%\r",
      "Progress: 87.02%\r",
      "Progress: 87.06%\r",
      "Progress: 87.1%\r",
      "Progress: 87.14%\r",
      "Progress: 87.18%\r",
      "Progress: 87.22%\r",
      "Progress: 87.26%\r",
      "Progress: 87.3%\r",
      "Progress: 87.34%\r",
      "Progress: 87.38%\r",
      "Progress: 87.41%\r",
      "Progress: 87.45%\r",
      "Progress: 87.49%\r",
      "Progress: 87.53%\r",
      "Progress: 87.57%\r",
      "Progress: 87.61%\r",
      "Progress: 87.65%\r",
      "Progress: 87.69%\r",
      "Progress: 87.73%\r",
      "Progress: 87.77%\r",
      "Progress: 87.81%\r",
      "Progress: 87.85%\r",
      "Progress: 87.89%\r",
      "Progress: 87.93%\r",
      "Progress: 87.97%\r",
      "Progress: 88.0%\r",
      "Progress: 88.04%\r",
      "Progress: 88.08%\r",
      "Progress: 88.12%\r",
      "Progress: 88.16%\r",
      "Progress: 88.2%\r",
      "Progress: 88.24%\r",
      "Progress: 88.28%\r",
      "Progress: 88.32%\r",
      "Progress: 88.36%\r",
      "Progress: 88.4%\r",
      "Progress: 88.44%\r",
      "Progress: 88.48%\r",
      "Progress: 88.52%\r",
      "Progress: 88.55%\r",
      "Progress: 88.59%\r",
      "Progress: 88.63%\r",
      "Progress: 88.67%\r",
      "Progress: 88.71%\r",
      "Progress: 88.75%\r",
      "Progress: 88.79%\r",
      "Progress: 88.83%\r",
      "Progress: 88.87%\r",
      "Progress: 88.91%\r",
      "Progress: 88.95%\r",
      "Progress: 88.99%\r",
      "Progress: 89.03%\r",
      "Progress: 89.07%\r",
      "Progress: 89.1%\r",
      "Progress: 89.14%\r",
      "Progress: 89.18%\r",
      "Progress: 89.22%\r",
      "Progress: 89.26%\r",
      "Progress: 89.3%\r",
      "Progress: 89.34%\r",
      "Progress: 89.38%\r",
      "Progress: 89.42%\r",
      "Progress: 89.46%\r",
      "Progress: 89.5%\r",
      "Progress: 89.54%\r",
      "Progress: 89.58%\r",
      "Progress: 89.62%\r",
      "Progress: 89.65%\r",
      "Progress: 89.69%\r",
      "Progress: 89.73%\r",
      "Progress: 89.77%\r",
      "Progress: 89.81%\r",
      "Progress: 89.85%\r",
      "Progress: 89.89%\r",
      "Progress: 89.93%\r",
      "Progress: 89.97%\r",
      "Progress: 90.01%\r",
      "Progress: 90.05%\r",
      "Progress: 90.09%\r",
      "Progress: 90.13%\r",
      "Progress: 90.17%\r",
      "Progress: 90.2%\r",
      "Progress: 90.24%\r",
      "Progress: 90.28%\r",
      "Progress: 90.32%\r",
      "Progress: 90.36%\r",
      "Progress: 90.4%\r",
      "Progress: 90.44%\r",
      "Progress: 90.48%\r",
      "Progress: 90.52%\r",
      "Progress: 90.56%\r",
      "Progress: 90.6%\r",
      "Progress: 90.64%\r",
      "Progress: 90.68%\r",
      "Progress: 90.72%\r",
      "Progress: 90.75%\r",
      "Progress: 90.79%\r",
      "Progress: 90.83%\r",
      "Progress: 90.87%\r",
      "Progress: 90.91%\r",
      "Progress: 90.95%\r",
      "Progress: 90.99%\r",
      "Progress: 91.03%\r",
      "Progress: 91.07%\r",
      "Progress: 91.11%\r",
      "Progress: 91.15%\r",
      "Progress: 91.19%\r",
      "Progress: 91.23%\r",
      "Progress: 91.27%\r",
      "Progress: 91.3%\r",
      "Progress: 91.34%\r",
      "Progress: 91.38%\r",
      "Progress: 91.42%\r",
      "Progress: 91.46%\r",
      "Progress: 91.5%\r",
      "Progress: 91.54%\r",
      "Progress: 91.58%\r",
      "Progress: 91.62%\r",
      "Progress: 91.66%\r",
      "Progress: 91.7%\r",
      "Progress: 91.74%\r",
      "Progress: 91.78%\r",
      "Progress: 91.82%\r",
      "Progress: 91.85%\r",
      "Progress: 91.89%\r",
      "Progress: 91.93%\r",
      "Progress: 91.97%\r",
      "Progress: 92.01%\r",
      "Progress: 92.05%\r",
      "Progress: 92.09%\r",
      "Progress: 92.13%\r",
      "Progress: 92.17%\r",
      "Progress: 92.21%\r",
      "Progress: 92.25%\r",
      "Progress: 92.29%\r",
      "Progress: 92.33%\r",
      "Progress: 92.37%\r",
      "Progress: 92.4%\r",
      "Progress: 92.44%\r",
      "Progress: 92.48%\r",
      "Progress: 92.52%\r",
      "Progress: 92.56%\r",
      "Progress: 92.6%\r",
      "Progress: 92.64%\r",
      "Progress: 92.68%\r",
      "Progress: 92.72%\r",
      "Progress: 92.76%\r",
      "Progress: 92.8%\r",
      "Progress: 92.84%\r",
      "Progress: 92.88%\r",
      "Progress: 92.92%\r",
      "Progress: 92.95%\r",
      "Progress: 92.99%\r",
      "Progress: 93.03%\r",
      "Progress: 93.07%\r",
      "Progress: 93.11%\r",
      "Progress: 93.15%\r",
      "Progress: 93.19%\r",
      "Progress: 93.23%\r",
      "Progress: 93.27%\r",
      "Progress: 93.31%\r",
      "Progress: 93.35%\r",
      "Progress: 93.39%\r",
      "Progress: 93.43%\r",
      "Progress: 93.47%\r",
      "Progress: 93.5%\r",
      "Progress: 93.54%\r",
      "Progress: 93.58%\r",
      "Progress: 93.62%\r",
      "Progress: 93.66%\r",
      "Progress: 93.7%\r",
      "Progress: 93.74%\r",
      "Progress: 93.78%\r",
      "Progress: 93.82%\r",
      "Progress: 93.86%\r",
      "Progress: 93.9%\r",
      "Progress: 93.94%\r",
      "Progress: 93.98%\r",
      "Progress: 94.02%\r",
      "Progress: 94.05%\r",
      "Progress: 94.09%\r",
      "Progress: 94.13%\r",
      "Progress: 94.17%\r",
      "Progress: 94.21%\r",
      "Progress: 94.25%\r",
      "Progress: 94.29%\r",
      "Progress: 94.33%\r",
      "Progress: 94.37%\r",
      "Progress: 94.41%\r",
      "Progress: 94.45%\r",
      "Progress: 94.49%\r",
      "Progress: 94.53%\r",
      "Progress: 94.57%\r",
      "Progress: 94.6%\r",
      "Progress: 94.64%\r",
      "Progress: 94.68%\r",
      "Progress: 94.72%\r",
      "Progress: 94.76%\r",
      "Progress: 94.8%\r",
      "Progress: 94.84%\r",
      "Progress: 94.88%\r",
      "Progress: 94.92%\r",
      "Progress: 94.96%\r",
      "Progress: 95.0%\r",
      "Progress: 95.04%\r",
      "Progress: 95.08%\r",
      "Progress: 95.12%\r",
      "Progress: 95.15%\r",
      "Progress: 95.19%\r",
      "Progress: 95.23%\r",
      "Progress: 95.27%\r",
      "Progress: 95.31%\r",
      "Progress: 95.35%\r",
      "Progress: 95.39%\r",
      "Progress: 95.43%\r",
      "Progress: 95.47%\r",
      "Progress: 95.51%\r",
      "Progress: 95.55%\r",
      "Progress: 95.59%\r",
      "Progress: 95.63%\r",
      "Progress: 95.67%\r",
      "Progress: 95.7%\r",
      "Progress: 95.74%\r",
      "Progress: 95.78%\r",
      "Progress: 95.82%\r",
      "Progress: 95.86%\r",
      "Progress: 95.9%\r",
      "Progress: 95.94%\r",
      "Progress: 95.98%\r",
      "Progress: 96.02%\r",
      "Progress: 96.06%\r",
      "Progress: 96.1%\r",
      "Progress: 96.14%\r",
      "Progress: 96.18%\r",
      "Progress: 96.22%\r",
      "Progress: 96.25%\r",
      "Progress: 96.29%\r",
      "Progress: 96.33%\r",
      "Progress: 96.37%\r",
      "Progress: 96.41%\r",
      "Progress: 96.45%\r",
      "Progress: 96.49%\r",
      "Progress: 96.53%\r",
      "Progress: 96.57%\r",
      "Progress: 96.61%\r",
      "Progress: 96.65%\r",
      "Progress: 96.69%\r",
      "Progress: 96.73%\r",
      "Progress: 96.77%\r",
      "Progress: 96.8%\r",
      "Progress: 96.84%\r",
      "Progress: 96.88%\r",
      "Progress: 96.92%\r",
      "Progress: 96.96%\r",
      "Progress: 97.0%\r",
      "Progress: 97.04%\r",
      "Progress: 97.08%\r",
      "Progress: 97.12%\r",
      "Progress: 97.16%\r",
      "Progress: 97.2%\r",
      "Progress: 97.24%\r",
      "Progress: 97.28%\r",
      "Progress: 97.32%\r",
      "Progress: 97.35%\r",
      "Progress: 97.39%\r",
      "Progress: 97.43%\r",
      "Progress: 97.47%\r",
      "Progress: 97.51%\r",
      "Progress: 97.55%\r",
      "Progress: 97.59%\r",
      "Progress: 97.63%\r",
      "Progress: 97.67%\r",
      "Progress: 97.71%\r",
      "Progress: 97.75%\r",
      "Progress: 97.79%\r",
      "Progress: 97.83%\r",
      "Progress: 97.87%\r",
      "Progress: 97.9%\r",
      "Progress: 97.94%\r",
      "Progress: 97.98%\r",
      "Progress: 98.02%\r",
      "Progress: 98.06%\r",
      "Progress: 98.1%\r",
      "Progress: 98.14%\r",
      "Progress: 98.18%\r",
      "Progress: 98.22%\r",
      "Progress: 98.26%\r",
      "Progress: 98.3%\r",
      "Progress: 98.34%\r",
      "Progress: 98.38%\r",
      "Progress: 98.42%\r",
      "Progress: 98.45%\r",
      "Progress: 98.49%\r",
      "Progress: 98.53%\r",
      "Progress: 98.57%\r",
      "Progress: 98.61%\r",
      "Progress: 98.65%\r",
      "Progress: 98.69%\r",
      "Progress: 98.73%\r",
      "Progress: 98.77%\r",
      "Progress: 98.81%\r",
      "Progress: 98.85%\r",
      "Progress: 98.89%\r",
      "Progress: 98.93%\r",
      "Progress: 98.97%\r",
      "Progress: 99.0%\r",
      "Progress: 99.04%\r",
      "Progress: 99.08%\r",
      "Progress: 99.12%\r",
      "Progress: 99.16%\r",
      "Progress: 99.2%\r",
      "Progress: 99.24%\r",
      "Progress: 99.28%\r",
      "Progress: 99.32%\r",
      "Progress: 99.36%\r",
      "Progress: 99.4%\r",
      "Progress: 99.44%\r",
      "Progress: 99.48%\r",
      "Progress: 99.52%\r",
      "Progress: 99.55%\r",
      "Progress: 99.59%\r",
      "Progress: 99.63%\r",
      "Progress: 99.67%\r",
      "Progress: 99.71%\r",
      "Progress: 99.75%\r",
      "Progress: 99.79%\r",
      "Progress: 99.83%\r",
      "Progress: 99.87%\r",
      "Progress: 99.91%\r",
      "Progress: 99.95%\r",
      "Progress: 99.99%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(254532, 1) (254532, 14309)\n"
     ]
    }
   ],
   "source": [
    "# preprocess the dataset to get training data\n",
    "\n",
    "max_input_len = 1\n",
    "step = 1\n",
    "x = []\n",
    "y = []\n",
    "for i in range(0, len(english_literature_tokens) - max_input_len, step):\n",
    "    if i % 100 == 0:\n",
    "        print(\"Progress: {0}%\".format(round(i / len(english_literature_tokens) * 100, 2)), end=\"\\r\")\n",
    "    curr_words = english_literature_tokens[i:i + max_input_len]\n",
    "    x.append([word2index.get(curr_word, 0) for curr_word in curr_words])\n",
    "    next_word = english_literature_tokens[i + max_input_len]\n",
    "    y.append(word2index.get(next_word, 0))\n",
    "X = np.array(x)\n",
    "Y = to_categorical(y, vocabulary_size)\n",
    "print(\"\")\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 1, 500)            7154500   \n",
      "_________________________________________________________________\n",
      "simple_rnn_6 (SimpleRNN)     (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 14309)             7168809   \n",
      "=================================================================\n",
      "Total params: 14,823,809\n",
      "Trainable params: 14,823,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_elman = Sequential()\n",
    "model_elman.add(Embedding(vocabulary_size, 500, input_length=max_input_len))\n",
    "model_elman.add(SimpleRNN(units=500, activation='sigmoid'))\n",
    "model_elman.add(Dense(vocabulary_size, activation='softmax'))\n",
    "\n",
    "model_elman.summary()\n",
    "model_elman.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10287761693967540431\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6700198133\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12434136452038456957\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Epoch 1/20\n",
      "254532/254532 [==============================] - 53s 208us/sample - loss: 6.1979 - acc: 0.1224\n",
      "Epoch 2/20\n",
      "254532/254532 [==============================] - 48s 189us/sample - loss: 5.5174 - acc: 0.1526\n",
      "Epoch 3/20\n",
      "254532/254532 [==============================] - 48s 189us/sample - loss: 5.2357 - acc: 0.1665\n",
      "Epoch 4/20\n",
      "254532/254532 [==============================] - 48s 189us/sample - loss: 5.0299 - acc: 0.1753\n",
      "Epoch 5/20\n",
      "254532/254532 [==============================] - 48s 187us/sample - loss: 4.8672 - acc: 0.1808\n",
      "Epoch 6/20\n",
      "254532/254532 [==============================] - 48s 188us/sample - loss: 4.7328 - acc: 0.1837\n",
      "Epoch 7/20\n",
      "254532/254532 [==============================] - 48s 188us/sample - loss: 4.6128 - acc: 0.1862\n",
      "Epoch 8/20\n",
      "254532/254532 [==============================] - 48s 189us/sample - loss: 4.5197 - acc: 0.1876\n",
      "Epoch 9/20\n",
      "254532/254532 [==============================] - 48s 188us/sample - loss: 4.4488 - acc: 0.1884\n",
      "Epoch 10/20\n",
      "254532/254532 [==============================] - 47s 186us/sample - loss: 4.3873 - acc: 0.1892\n",
      "Epoch 11/20\n",
      "254532/254532 [==============================] - 48s 188us/sample - loss: 4.3437 - acc: 0.1900\n",
      "Epoch 12/20\n",
      "254532/254532 [==============================] - 48s 187us/sample - loss: 4.3097 - acc: 0.1906\n",
      "Epoch 13/20\n",
      "254532/254532 [==============================] - 47s 186us/sample - loss: 4.2829 - acc: 0.1913\n",
      "Epoch 14/20\n",
      "254532/254532 [==============================] - 48s 187us/sample - loss: 4.2612 - acc: 0.1923\n",
      "Epoch 15/20\n",
      "254532/254532 [==============================] - 48s 187us/sample - loss: 4.2413 - acc: 0.1928\n",
      "Epoch 16/20\n",
      "254532/254532 [==============================] - 47s 186us/sample - loss: 4.2252 - acc: 0.1934\n",
      "Epoch 17/20\n",
      "254532/254532 [==============================] - 48s 188us/sample - loss: 4.2130 - acc: 0.1936\n",
      "Epoch 18/20\n",
      "254532/254532 [==============================] - 48s 187us/sample - loss: 4.2054 - acc: 0.1942\n",
      "Epoch 19/20\n",
      "254532/254532 [==============================] - 48s 188us/sample - loss: 4.2032 - acc: 0.1951\n",
      "Epoch 20/20\n",
      "254532/254532 [==============================] - 48s 187us/sample - loss: 4.1998 - acc: 0.1959\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())\n",
    "filepath = \"./model_elman.pth\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, save_weights_only=False)\n",
    "elman_training_history = model_elman.fit(\n",
    "    X,\n",
    "    Y,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=[checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the model cross-entropy loss.\n",
    "The model cross-entropy loss for the elman + TBTT model (part 1) on the train set is 4.1998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Elman network + backpropagation through time algorithm (part 2)\n",
    "'''\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# prepare sentence sequences of the dataset\n",
    "english_literature_sentences = sent_tokenize(english_literature_text)\n",
    "english_literature_sentences_seq = []\n",
    "english_literature_sentences_length = []\n",
    "max_length = 40\n",
    "for sentence in english_literature_sentences:\n",
    "    tmp_tokens = word_tokenize(sentence)\n",
    "    if len(tmp_tokens) > max_length:\n",
    "        tmp_tokens = tmp_tokens[:max_length]\n",
    "    for i in range(1, len(tmp_tokens)):\n",
    "        # 1-of-N encoding\n",
    "        tmp_seq = tmp_tokens[:i+1]\n",
    "        tmp_seq_encoded = []\n",
    "        for token in tmp_seq:\n",
    "            tmp_seq_encoded.append(word2index[token])\n",
    "        english_literature_sentences_seq.append(tmp_seq_encoded)\n",
    "        english_literature_sentences_length.append(len(tmp_seq_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences 210783\n",
      "mean sentence length 14.081591020148684\n",
      "max sentence length 40\n"
     ]
    }
   ],
   "source": [
    "print('number of sequences', len(english_literature_sentences_seq))\n",
    "print('mean sentence length', sum(english_literature_sentences_length) / len(english_literature_sentences_length))\n",
    "max_sentence_length = max(english_literature_sentences_length)\n",
    "print('max sentence length', max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input and target data for training the model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "english_literature_sentences_seq = pad_sequences(english_literature_sentences_seq, maxlen=max_sentence_length, padding='pre')\n",
    "english_literature_sentences_seq = np.array(english_literature_sentences_seq)\n",
    "x_BTT = english_literature_sentences_seq[:, :-1]\n",
    "\n",
    "y_BTT = to_categorical(english_literature_sentences_seq[:, -1], vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 39, 500)           7154500   \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 14309)             7168809   \n",
      "=================================================================\n",
      "Total params: 14,823,809\n",
      "Trainable params: 14,823,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the network with BTT\n",
    "model_BTT = Sequential()\n",
    "model_BTT.add(Embedding(vocabulary_size, 500, input_length=max_sentence_length-1))\n",
    "model_BTT.add(SimpleRNN(units=500, activation='sigmoid'))\n",
    "model_BTT.add(Dense(vocabulary_size, activation='softmax'))\n",
    "\n",
    "model_BTT.summary()\n",
    "model_BTT.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4406640222819138814\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6700198133\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13219286407285742176\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Epoch 1/60\n",
      "210783/210783 [==============================] - 75s 357us/sample - loss: 5.8978 - acc: 0.1354 - loss: 5.9117 - acc: 0.134 -\n",
      "Epoch 2/60\n",
      "210783/210783 [==============================] - 71s 339us/sample - loss: 5.1589 - acc: 0.1709\n",
      "Epoch 3/60\n",
      "210783/210783 [==============================] - 71s 337us/sample - loss: 4.7923 - acc: 0.1912\n",
      "Epoch 4/60\n",
      "210783/210783 [==============================] - 71s 335us/sample - loss: 4.4710 - acc: 0.2065\n",
      "Epoch 5/60\n",
      "210783/210783 [==============================] - 71s 337us/sample - loss: 4.1532 - acc: 0.2208\n",
      "Epoch 6/60\n",
      "210783/210783 [==============================] - 72s 341us/sample - loss: 3.8396 - acc: 0.2395\n",
      "Epoch 7/60\n",
      "210783/210783 [==============================] - 72s 341us/sample - loss: 3.5509 - acc: 0.2708\n",
      "Epoch 8/60\n",
      "210783/210783 [==============================] - 72s 342us/sample - loss: 3.3028 - acc: 0.3063\n",
      "Epoch 9/60\n",
      "210783/210783 [==============================] - 72s 341us/sample - loss: 3.0983 - acc: 0.3385\n",
      "Epoch 10/60\n",
      "210783/210783 [==============================] - 71s 338us/sample - loss: 2.9141 - acc: 0.3694\n",
      "Epoch 11/60\n",
      "210783/210783 [==============================] - 70s 330us/sample - loss: 2.7600 - acc: 0.3963\n",
      "Epoch 12/60\n",
      "210783/210783 [==============================] - 69s 328us/sample - loss: 2.6309 - acc: 0.4196\n",
      "Epoch 13/60\n",
      "210783/210783 [==============================] - 69s 328us/sample - loss: 2.4981 - acc: 0.4445\n",
      "Epoch 14/60\n",
      "210783/210783 [==============================] - 69s 328us/sample - loss: 2.3790 - acc: 0.4676\n",
      "Epoch 15/60\n",
      "210783/210783 [==============================] - 69s 328us/sample - loss: 2.2696 - acc: 0.4892\n",
      "Epoch 16/60\n",
      "210783/210783 [==============================] - 69s 328us/sample - loss: 2.1710 - acc: 0.5084\n",
      "Epoch 17/60\n",
      "210783/210783 [==============================] - 69s 329us/sample - loss: 2.0684 - acc: 0.5301\n",
      "Epoch 18/60\n",
      "210783/210783 [==============================] - 69s 329us/sample - loss: 1.9736 - acc: 0.5515\n",
      "Epoch 19/60\n",
      "210783/210783 [==============================] - 69s 328us/sample - loss: 1.8821 - acc: 0.5707\n",
      "Epoch 20/60\n",
      "210783/210783 [==============================] - 69s 329us/sample - loss: 1.8165 - acc: 0.5844\n",
      "Epoch 21/60\n",
      "210783/210783 [==============================] - 69s 329us/sample - loss: 1.7425 - acc: 0.6010\n",
      "Epoch 22/60\n",
      "210783/210783 [==============================] - 69s 329us/sample - loss: 1.6499 - acc: 0.6223\n",
      "Epoch 23/60\n",
      "210783/210783 [==============================] - 69s 329us/sample - loss: 1.5417 - acc: 0.6470\n",
      "Epoch 24/60\n",
      "210783/210783 [==============================] - 69s 328us/sample - loss: 1.5061 - acc: 0.6547\n",
      "Epoch 25/60\n",
      "210783/210783 [==============================] - 69s 329us/sample - loss: 1.4078 - acc: 0.6788\n",
      "Epoch 26/60\n",
      "210783/210783 [==============================] - 69s 329us/sample - loss: 1.3743 - acc: 0.6856\n",
      "Epoch 27/60\n",
      "210783/210783 [==============================] - 69s 329us/sample - loss: 1.3115 - acc: 0.6992\n",
      "Epoch 28/60\n",
      "210783/210783 [==============================] - 69s 329us/sample - loss: 1.2528 - acc: 0.7124\n",
      "Epoch 29/60\n",
      "210783/210783 [==============================] - 69s 329us/sample - loss: 1.1599 - acc: 0.7359\n",
      "Epoch 30/60\n",
      "210783/210783 [==============================] - 69s 327us/sample - loss: 1.2259 - acc: 0.7199\n",
      "Epoch 31/60\n",
      "210783/210783 [==============================] - 69s 328us/sample - loss: 1.1592 - acc: 0.7339\n",
      "Epoch 32/60\n",
      "210783/210783 [==============================] - 69s 329us/sample - loss: 1.0972 - acc: 0.7490\n",
      "Epoch 33/60\n",
      "210783/210783 [==============================] - 69s 328us/sample - loss: 1.0754 - acc: 0.7542\n",
      "Epoch 34/60\n",
      "210783/210783 [==============================] - 69s 328us/sample - loss: 0.9798 - acc: 0.7768\n",
      "Epoch 35/60\n",
      "210783/210783 [==============================] - 69s 328us/sample - loss: 0.9058 - acc: 0.7950\n",
      "Epoch 36/60\n",
      "210783/210783 [==============================] - 69s 328us/sample - loss: 0.8545 - acc: 0.8086\n",
      "Epoch 37/60\n",
      "210783/210783 [==============================] - 69s 328us/sample - loss: 0.8126 - acc: 0.8184\n",
      "Epoch 38/60\n",
      "210783/210783 [==============================] - 69s 327us/sample - loss: 0.9555 - acc: 0.7817\n",
      "Epoch 39/60\n",
      "210783/210783 [==============================] - 69s 327us/sample - loss: 0.9690 - acc: 0.7769\n",
      "Epoch 40/60\n",
      "210783/210783 [==============================] - 69s 327us/sample - loss: 0.9602 - acc: 0.7786\n",
      "Epoch 41/60\n",
      "210783/210783 [==============================] - 70s 333us/sample - loss: 0.7996 - acc: 0.8163\n",
      "Epoch 42/60\n",
      "210783/210783 [==============================] - 71s 336us/sample - loss: 0.7670 - acc: 0.8261\n",
      "Epoch 43/60\n",
      "210783/210783 [==============================] - 70s 334us/sample - loss: 0.8035 - acc: 0.8145\n",
      "Epoch 44/60\n",
      "210783/210783 [==============================] - 71s 335us/sample - loss: 0.7626 - acc: 0.8244\n",
      "Epoch 45/60\n",
      "210783/210783 [==============================] - 71s 334us/sample - loss: 0.7135 - acc: 0.8378\n",
      "Epoch 46/60\n",
      "210783/210783 [==============================] - 71s 337us/sample - loss: 0.6794 - acc: 0.8457\n",
      "Epoch 47/60\n",
      "210783/210783 [==============================] - 71s 338us/sample - loss: 0.6857 - acc: 0.8434\n",
      "Epoch 48/60\n",
      "210783/210783 [==============================] - 71s 338us/sample - loss: 0.6507 - acc: 0.8521\n",
      "Epoch 49/60\n",
      "210783/210783 [==============================] - 71s 335us/sample - loss: 0.7741 - acc: 0.8192\n",
      "Epoch 50/60\n",
      "210783/210783 [==============================] - 71s 336us/sample - loss: 0.6776 - acc: 0.8413\n",
      "Epoch 51/60\n",
      "210783/210783 [==============================] - 71s 335us/sample - loss: 0.7101 - acc: 0.8332\n",
      "Epoch 52/60\n",
      "210783/210783 [==============================] - 70s 334us/sample - loss: 0.7991 - acc: 0.8168\n",
      "Epoch 53/60\n",
      "210783/210783 [==============================] - 71s 336us/sample - loss: 0.6600 - acc: 0.8491\n",
      "Epoch 54/60\n",
      "210783/210783 [==============================] - 71s 336us/sample - loss: 0.7992 - acc: 0.8136\n",
      "Epoch 55/60\n",
      "210783/210783 [==============================] - 70s 334us/sample - loss: 0.7041 - acc: 0.8347\n",
      "Epoch 56/60\n",
      "210783/210783 [==============================] - 71s 336us/sample - loss: 0.6926 - acc: 0.8415\n",
      "Epoch 57/60\n",
      "210783/210783 [==============================] - 71s 339us/sample - loss: 0.5841 - acc: 0.8640\n",
      "Epoch 58/60\n",
      "210783/210783 [==============================] - 71s 339us/sample - loss: 0.5690 - acc: 0.8689\n",
      "Epoch 59/60\n",
      "210783/210783 [==============================] - 71s 335us/sample - loss: 0.5758 - acc: 0.8671\n",
      "Epoch 60/60\n",
      "210783/210783 [==============================] - 71s 336us/sample - loss: 0.5191 - acc: 0.8812\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())\n",
    "filepath = \"./model_BTT.pth\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, save_weights_only=False)\n",
    "BTT_training_history = model_BTT.fit(\n",
    "    x_BTT,\n",
    "    y_BTT,\n",
    "    batch_size=128,\n",
    "    epochs=60,\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the model cross-entropy loss.\n",
    "The model cross-entropy loss for the elman + BTT model (part 2) on the train set is 0.5191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\songyih\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 39, 500)           7154500   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 500)               1501500   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 14309)             7168809   \n",
      "=================================================================\n",
      "Total params: 15,824,809\n",
      "Trainable params: 15,824,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "''' Elman + BTT model with the SimpleRnn unit replaced with a Gated Recurrent Unit (part 3)\n",
    "'''\n",
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model_BTT_GRU = Sequential()\n",
    "model_BTT_GRU.add(Embedding(vocabulary_size, 500, input_length=max_sentence_length-1))\n",
    "model_BTT_GRU.add(GRU(units=500, activation='sigmoid'))\n",
    "model_BTT_GRU.add(Dense(vocabulary_size, activation='softmax'))\n",
    "\n",
    "model_BTT_GRU.summary()\n",
    "model_BTT_GRU.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6192848861112352318\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6700198133\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16910545245568130575\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "WARNING:tensorflow:From C:\\Users\\songyih\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "210783/210783 [==============================] - 132s 629us/sample - loss: 5.9056 - acc: 0.1354\n",
      "Epoch 2/100\n",
      "210783/210783 [==============================] - 128s 605us/sample - loss: 5.1159 - acc: 0.1751\n",
      "Epoch 3/100\n",
      "210783/210783 [==============================] - 128s 606us/sample - loss: 4.6989 - acc: 0.1985\n",
      "Epoch 4/100\n",
      "210783/210783 [==============================] - 127s 604us/sample - loss: 4.3138 - acc: 0.2184\n",
      "Epoch 5/100\n",
      "210783/210783 [==============================] - 127s 604us/sample - loss: 3.9169 - acc: 0.2385\n",
      "Epoch 6/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 3.5159 - acc: 0.2794\n",
      "Epoch 7/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 3.1664 - acc: 0.3358\n",
      "Epoch 8/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 2.8839 - acc: 0.3831\n",
      "Epoch 9/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 2.6343 - acc: 0.4268\n",
      "Epoch 10/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 2.4109 - acc: 0.4687\n",
      "Epoch 11/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 2.2013 - acc: 0.5114\n",
      "Epoch 12/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 2.0082 - acc: 0.5529\n",
      "Epoch 13/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 1.8329 - acc: 0.5919\n",
      "Epoch 14/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 1.6699 - acc: 0.6291\n",
      "Epoch 15/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 1.5212 - acc: 0.6641\n",
      "Epoch 16/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 1.3847 - acc: 0.6952\n",
      "Epoch 17/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 1.2613 - acc: 0.7244\n",
      "Epoch 18/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 1.1493 - acc: 0.7506\n",
      "Epoch 19/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 1.0496 - acc: 0.7736\n",
      "Epoch 20/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.9609 - acc: 0.7945\n",
      "Epoch 21/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.8822 - acc: 0.8132\n",
      "Epoch 22/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.8150 - acc: 0.8281\n",
      "Epoch 23/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.7473 - acc: 0.8437\n",
      "Epoch 24/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.6904 - acc: 0.8568\n",
      "Epoch 25/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.6392 - acc: 0.8681\n",
      "Epoch 26/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.6048 - acc: 0.8755\n",
      "Epoch 27/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.5711 - acc: 0.8820\n",
      "Epoch 28/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.5375 - acc: 0.8895\n",
      "Epoch 29/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.5074 - acc: 0.8948\n",
      "Epoch 30/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.4871 - acc: 0.8989\n",
      "Epoch 31/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.4622 - acc: 0.9042\n",
      "Epoch 32/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.4536 - acc: 0.9043\n",
      "Epoch 33/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.4629 - acc: 0.8998\n",
      "Epoch 34/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.4242 - acc: 0.9095\n",
      "Epoch 35/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.4215 - acc: 0.9098\n",
      "Epoch 36/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.4084 - acc: 0.9120\n",
      "Epoch 37/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3892 - acc: 0.9158\n",
      "Epoch 38/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3882 - acc: 0.9153\n",
      "Epoch 39/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3806 - acc: 0.9161\n",
      "Epoch 40/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3771 - acc: 0.9162\n",
      "Epoch 41/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.3740 - acc: 0.9160\n",
      "Epoch 42/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.3653 - acc: 0.9177\n",
      "Epoch 43/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.3535 - acc: 0.9206\n",
      "Epoch 44/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.3568 - acc: 0.9188\n",
      "Epoch 45/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3674 - acc: 0.9154\n",
      "Epoch 46/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3564 - acc: 0.9176\n",
      "Epoch 47/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3552 - acc: 0.9182\n",
      "Epoch 48/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.3422 - acc: 0.9208\n",
      "Epoch 49/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.3378 - acc: 0.9216\n",
      "Epoch 50/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.3356 - acc: 0.9222\n",
      "Epoch 51/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.3225 - acc: 0.9243\n",
      "Epoch 52/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3415 - acc: 0.9194\n",
      "Epoch 53/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3299 - acc: 0.9217\n",
      "Epoch 54/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3559 - acc: 0.9151\n",
      "Epoch 55/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3623 - acc: 0.9126\n",
      "Epoch 56/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3311 - acc: 0.9214\n",
      "Epoch 57/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.3184 - acc: 0.9245\n",
      "Epoch 58/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3347 - acc: 0.9201\n",
      "Epoch 59/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.3311 - acc: 0.9207\n",
      "Epoch 60/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.3830 - acc: 0.9072\n",
      "Epoch 61/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.3331 - acc: 0.9201\n",
      "Epoch 62/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.3170 - acc: 0.9241\n",
      "Epoch 63/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3271 - acc: 0.9214\n",
      "Epoch 64/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.3275 - acc: 0.9203\n",
      "Epoch 65/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.3177 - acc: 0.9226\n",
      "Epoch 66/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.3126 - acc: 0.9242\n",
      "Epoch 67/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.3091 - acc: 0.9247\n",
      "Epoch 68/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.3038 - acc: 0.9259\n",
      "Epoch 69/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3096 - acc: 0.9236\n",
      "Epoch 70/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3086 - acc: 0.9238\n",
      "Epoch 71/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3105 - acc: 0.9237\n",
      "Epoch 72/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3101 - acc: 0.9231\n",
      "Epoch 73/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.2973 - acc: 0.9265\n",
      "Epoch 74/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.3098 - acc: 0.9230\n",
      "Epoch 75/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.3003 - acc: 0.9252\n",
      "Epoch 76/100\n",
      "210783/210783 [==============================] - 126s 599us/sample - loss: 0.2956 - acc: 0.9266\n",
      "Epoch 77/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.2968 - acc: 0.9254\n",
      "Epoch 78/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.2978 - acc: 0.9252\n",
      "Epoch 79/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.3045 - acc: 0.9242\n",
      "Epoch 80/100\n",
      "210783/210783 [==============================] - 126s 596us/sample - loss: 0.3028 - acc: 0.9237\n",
      "Epoch 81/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.2958 - acc: 0.9260\n",
      "Epoch 82/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.2972 - acc: 0.9256\n",
      "Epoch 83/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.2921 - acc: 0.9267\n",
      "Epoch 84/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.2906 - acc: 0.9269\n",
      "Epoch 85/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.2968 - acc: 0.9250\n",
      "Epoch 86/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.2919 - acc: 0.9258\n",
      "Epoch 87/100\n",
      "210783/210783 [==============================] - 126s 598us/sample - loss: 0.2892 - acc: 0.9268\n",
      "Epoch 88/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.2914 - acc: 0.9260\n",
      "Epoch 89/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.3133 - acc: 0.9209\n",
      "Epoch 90/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.3056 - acc: 0.9219\n",
      "Epoch 91/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.2925 - acc: 0.9258\n",
      "Epoch 92/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.2989 - acc: 0.9238\n",
      "Epoch 93/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.2972 - acc: 0.9247\n",
      "Epoch 94/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.2988 - acc: 0.9239\n",
      "Epoch 95/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.3080 - acc: 0.9215\n",
      "Epoch 96/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.3053 - acc: 0.9218\n",
      "Epoch 97/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.2982 - acc: 0.9237\n",
      "Epoch 98/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.3002 - acc: 0.9240\n",
      "Epoch 99/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.3087 - acc: 0.9216\n",
      "Epoch 100/100\n",
      "210783/210783 [==============================] - 126s 597us/sample - loss: 0.3027 - acc: 0.9229\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())\n",
    "filepath = \"./model_BTT_GRU.pth\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, save_weights_only=False)\n",
    "BTT_GRU_training_history = model_BTT_GRU.fit(\n",
    "    x_BTT,\n",
    "    y_BTT,\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the model cross-entropy loss.\n",
    "The model cross-entropy loss for the elman + BTT with GRU is 0.2892\n",
    "\n",
    "### Compare your results in terms of cross-entropy loss with two other approach (part 1 and 2)\n",
    "\n",
    "- Elman + TBTT (part 1): cross-entropy loss 4.1998, acc 0.1959 \n",
    "- Elman + BTT (part 2): best cross-entropy loss 0.5191, acc 0.8812\n",
    "- Elman + BTT with the SimpleRNN unit replaced with GRU (part 3): best cross-entropy loss 0.2892, acc 0.9268 \n",
    "\n",
    "The cross-entropy loss of Elman + BTT network with the SimpleRNN unit replaced with GRU is the best among these three model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studded all the world , I have no more than the world , I have\n",
      "gaze this island . PROSPERO : I have no more than the world , I\n",
      "chid'st me , I have no more than the world , I have no more\n",
      "measuring his name , I have no more than the world , I have no\n",
      "planched gate And , I have no more than the world , I have no\n",
      "Senators : I have no more than the world , I have no more than\n",
      "drift ; And , I have no more than the world , I have no\n",
      "grazing , I have no more than the world , I have no more than\n",
      "cap of the world , I have no more than the world , I have\n",
      "corrupted foul play the world , I have no more than the world , I\n"
     ]
    }
   ],
   "source": [
    "'''Use each model to generate 10 synthetic sentences of 15 words each\n",
    "'''\n",
    "word_num = 15\n",
    "sentence_num = 10\n",
    "\n",
    "# basic elman with TBTT model (part 1)\n",
    "elman_TBTT = load_model('./model_elman.pth')\n",
    "for i in range(sentence_num):\n",
    "    # randomly choose init word\n",
    "    init_encoded = np.random.randint(vocabulary_size)\n",
    "    encoded_sequence = [init_encoded]\n",
    "    # generate the predicted sequence\n",
    "    for _ in range(word_num - 1):\n",
    "        latest_word_encoded = [encoded_sequence[-1]]\n",
    "        latest_word_encoded = np.array(latest_word_encoded)\n",
    "        predicted_encoded = elman_TBTT.predict_classes(latest_word_encoded, verbose=0)\n",
    "        encoded_sequence.append(predicted_encoded[0])\n",
    "    # decode the sequence\n",
    "    decoded_sequence = []\n",
    "    for encoded_word in encoded_sequence:\n",
    "        decoded_sequence.append(index2word[encoded_word])\n",
    "    print(' '.join(decoded_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dip'dst in view ; but first was struck with me than never will be ruled\n",
      "bud of better ; he does offend my brother ? ' Lord , how have\n",
      "noisemaker ! ' I hate thee by your side ; and see this night or\n",
      "punto reverso ! ' I not ? -- No ; I will resist such entertainment\n",
      "strokedst me and madest much of him ! ' I ' the plain way is\n",
      "births : On whom God will never yet a word , we hear the minstrels\n",
      "It is a hint That wrings mine eyes to't . ' n ' the air\n",
      "awaked him , we 'll be put to woo . ' I ' the part\n",
      "footing of the city ? ' song , the great subject well , she 'll\n",
      "Fill me for that gird , good Tranio , for that thou likest it not\n"
     ]
    }
   ],
   "source": [
    "# elman with BTT model (part 2)\n",
    "elman_BTT = load_model('./model_BTT.pth')\n",
    "for i in range(sentence_num):\n",
    "    # randomly choose init word\n",
    "    init_encoded = np.random.randint(vocabulary_size)\n",
    "    encoded_sequence = [init_encoded]\n",
    "    # generate the predicted sequence\n",
    "    for _ in range(word_num - 1):\n",
    "        input_sequence = pad_sequences([encoded_sequence], maxlen=max_sentence_length-1, padding='pre')\n",
    "        input_sequence = np.array(input_sequence)\n",
    "        predicted_encoded = elman_BTT.predict_classes(input_sequence, verbose=0)\n",
    "        encoded_sequence.append(predicted_encoded[0])\n",
    "    # decode the sequence\n",
    "    decoded_sequence = []\n",
    "    for encoded_word in encoded_sequence:\n",
    "        decoded_sequence.append(index2word[encoded_word])\n",
    "    print(' '.join(decoded_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devised are thee ? ' to the Capitol ! -- I am : and here\n",
      "pluck in the topsail . ' the air doth burn . ' the air doth\n",
      "fan , this Claudio is mine only son . ' the other way In that\n",
      "shield me not first ? ' to the gaol . ' the house , how\n",
      "abusing Baptista is to a cause to sigh , Then he shall command his mind\n",
      "ye are not so mad -- That thou hast cause to pry into this morning\n",
      "commonly and the air And each more villain : if these thing it is ,\n",
      "pieces : He hath not possible nor prayers ; and he be too noble for\n",
      "couples : You seem to hear of this : you have such vantage in this\n",
      "fixes renown 'd two in this field We fall in broil . ' the last\n"
     ]
    }
   ],
   "source": [
    "# elman with BTT model with the SimpleRnn unit replaced with GRU (part 3)\n",
    "elman_BTT_GRU = load_model('./model_BTT_GRU.pth')\n",
    "for i in range(sentence_num):\n",
    "    # randomly choose init word\n",
    "    init_encoded = np.random.randint(vocabulary_size)\n",
    "    encoded_sequence = [init_encoded]\n",
    "    # generate the predicted sequence\n",
    "    for _ in range(word_num - 1):\n",
    "        input_sequence = pad_sequences([encoded_sequence], maxlen=max_sentence_length-1, padding='pre')\n",
    "        input_sequence = np.array(input_sequence)\n",
    "        predicted_encoded = elman_BTT_GRU.predict_classes(input_sequence, verbose=0)\n",
    "        encoded_sequence.append(predicted_encoded[0])\n",
    "    # decode the sequence\n",
    "    decoded_sequence = []\n",
    "    for encoded_word in encoded_sequence:\n",
    "        decoded_sequence.append(index2word[encoded_word])\n",
    "    print(' '.join(decoded_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss the quality of the sentences generated\n",
    "\n",
    "The sentences generated by basic elman with TBTT model (part 1) doesn't look like English at all and seems to be overfitting as it is simply repeating some phrases, while sentences generated by the later 2 models look much better. Although the elman with BTT model cannot form a nice sentence, it produces some natural phrases. And the elman + BTT with SimpleRNN replaced with GRU performs the best among these three models. Though it still doesn't work perfectly, it is able to somehow generate some sentences that looks like English, and matches the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Compare the words representation extracted from each of the approaches using one of the existing methods. (part 4)\n",
    "Here I choose to use intrinsic evaluation method. \n",
    "I evaluate the model by compare the similarity of my three models and gold standard similarity dataset\n",
    "'''\n",
    "\n",
    "# load wordsim_similarity_goldstandard as benchmark\n",
    "goldstandard_word0 = []\n",
    "goldstandard_word1 = []\n",
    "goldstandard_similarity = []\n",
    "vocabulary_keys = word2index.keys()\n",
    "found_pairs = 0\n",
    "total_pairs = 0\n",
    "with open('./datasets/wordsim_similarity_goldstandard.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        total_pairs += 1\n",
    "        temp = line.strip().split('\\t')\n",
    "        # only save the pair of words that can be found in our vocabulary\n",
    "        if temp[0] in vocabulary_keys and temp[1] in vocabulary_keys:\n",
    "            found_pairs += 1\n",
    "            goldstandard_word0.append(temp[0])\n",
    "            goldstandard_word1.append(temp[1])\n",
    "            goldstandard_similarity.append(float(temp[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 66 pair of words in local vocabulary out of 203 pair of words in wordsim_similarity_goldstandard\n"
     ]
    }
   ],
   "source": [
    "print('Found {0} pair of words in local vocabulary out of {1} pair of words in wordsim_similarity_goldstandard'.format(found_pairs, total_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\songyih\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\songyih\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# calculate similarity for elman + TBTT model (part 1) on the found pair of words\n",
    "\n",
    "# load model\n",
    "elman_TBTT = load_model('./model_elman.pth')\n",
    "elman_TBTT_embedding = elman_TBTT.layers[0].get_weights()[0]\n",
    "elman_TBTT_embedding = np.array(elman_TBTT_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# do the calculation\n",
    "elman_TBTT_similarity = []\n",
    "for i in range(len(goldstandard_similarity)):\n",
    "    index_word0 = word2index[goldstandard_word0[i]]\n",
    "    index_word1 = word2index[goldstandard_word1[i]]\n",
    "    elman_TBTT_similarity.append(cosine_similarity(elman_TBTT_embedding[index_word0].reshape(1, -1), elman_TBTT_embedding[index_word1].reshape(1, -1))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation between elman + TBTT (part 1) and gold starndard similarity is  0.1581\n"
     ]
    }
   ],
   "source": [
    "# calculate the Spearman rank correlation on our similarity and goldstandard similarity\n",
    "\n",
    "from scipy import stats\n",
    "elman_TBTT_correlation = stats.spearmanr(np.array(elman_TBTT_similarity), np.array(goldstandard_similarity))\n",
    "print('Spearman rank correlation between elman + TBTT (part 1) and gold starndard similarity is ', elman_TBTT_correlation.correlation.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate similarity for elman + BTT model (part 2) on the found pair of words\n",
    "\n",
    "elman_BTT = load_model('./model_BTT_max40.pth')\n",
    "elman_BTT_embedding = elman_BTT.layers[0].get_weights()[0]\n",
    "elman_BTT_embedding = np.array(elman_BTT_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# do the calculation\n",
    "elman_BTT_similarity = []\n",
    "for i in range(len(goldstandard_similarity)):\n",
    "    index_word0 = word2index[goldstandard_word0[i]]\n",
    "    index_word1 = word2index[goldstandard_word1[i]]\n",
    "    elman_BTT_similarity.append(cosine_similarity(elman_BTT_embedding[index_word0].reshape(1, -1), elman_BTT_embedding[index_word1].reshape(1, -1))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation between elman + BTT (part 2) and gold starndard similarity is  0.2478\n"
     ]
    }
   ],
   "source": [
    "# calculate the Spearman rank correlation on our similarity and goldstandard similarity\n",
    "\n",
    "from scipy import stats\n",
    "elman_BTT_correlation = stats.spearmanr(np.array(elman_BTT_similarity), np.array(goldstandard_similarity))\n",
    "print('Spearman rank correlation between elman + BTT (part 2) and gold starndard similarity is ', elman_BTT_correlation.correlation.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate similarity for elman + BTT model with GRU (part 3) on the found pair of words\n",
    "\n",
    "# load model\n",
    "elman_BTT_GRU = load_model('./model_BTT_GRU.pth')\n",
    "elman_BTT_GRU_embedding = elman_BTT_GRU.layers[0].get_weights()[0]\n",
    "elman_BTT_GRU_embedding = np.array(elman_BTT_GRU_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# do the calculation\n",
    "elman_BTT_GRU_similarity = []\n",
    "for i in range(len(goldstandard_similarity)):\n",
    "    index_word0 = word2index[goldstandard_word0[i]]\n",
    "    index_word1 = word2index[goldstandard_word1[i]]\n",
    "    elman_BTT_GRU_similarity.append(cosine_similarity(elman_BTT_GRU_embedding[index_word0].reshape(1, -1), elman_BTT_GRU_embedding[index_word1].reshape(1, -1))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation between elman + BTT with GRU (part 3) and gold starndard similarity is  0.2482\n"
     ]
    }
   ],
   "source": [
    "# calculate the Spearman rank correlation on our similarity and goldstandard similarity\n",
    "\n",
    "from scipy import stats\n",
    "elman_BTT_GRU_correlation = stats.spearmanr(np.array(elman_BTT_GRU_similarity), np.array(goldstandard_similarity))\n",
    "print('Spearman rank correlation between elman + BTT with GRU (part 3) and gold starndard similarity is ', elman_BTT_GRU_correlation.correlation.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the words representation extracted from each of the approaches using one of the existing methods.\n",
    "\n",
    "I used intrinsic evaluation method to evaluate the extracted words representations\n",
    "\n",
    "I chose the gold standard similarity dataset as benchmark, then found the word pairs that appears on both my local vocabulary and benchmark dataset.\n",
    "\n",
    "After that I calculated the similarity for the found word pairs on the three models.\n",
    "Finally I calculated the Spearman rank correlation between the similarity of my models and the benchmark.\n",
    "\n",
    "Here are the correlation results:\n",
    "\n",
    "- elman + TBTT (part 1): 0.1581\n",
    "- elman + BTT (part 2): 0.2478\n",
    "- elman + BTT with GRU (part 3): 0.2482\n",
    "\n",
    "The result is actually consistent with the text generation result above, that the elman + TBTT works the worst and the other two are much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Learn an RNN model that predicts document categories given its content (part 5)\n",
    "'''\n",
    "\n",
    "news_tokens = []\n",
    "for news_item in news:\n",
    "    news_tokens.append(word_tokenize(news_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207442\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# build vocabulary\n",
    "word2index_news = {}\n",
    "index2word_news = []\n",
    "flattend_news_tokens = []\n",
    "for sublist in news_tokens:\n",
    "    for item in sublist:\n",
    "        flattend_news_tokens.append(item)\n",
    "news_counter = Counter(flattend_news_tokens)\n",
    "\n",
    "for word, count in news_counter.items():\n",
    "    index2word_news.append(word)\n",
    "    word2index_news[word] = len(word2index_news)\n",
    "\n",
    "news_vocabulary_size = len(word2index_news)\n",
    "print(news_vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the dataset\n",
    "max_news_length = 400\n",
    "news_encoded = []\n",
    "news_encoded_length = []\n",
    "for news_tokens_item in news_tokens:\n",
    "    if len(news_tokens_item) > max_news_length:\n",
    "        news_tokens_item = news_tokens_item[:max_news_length]\n",
    "    tmp_encoded = []\n",
    "    for news_token in news_tokens_item:\n",
    "        tmp_encoded.append(word2index_news[news_token])\n",
    "    news_encoded.append(tmp_encoded)\n",
    "    news_encoded_length.append(len(tmp_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of news 13108\n",
      "mean news length 238.1869087580104\n",
      "max news length 400\n"
     ]
    }
   ],
   "source": [
    "print('number of news', len(news_encoded))\n",
    "print('mean news length', sum(news_encoded_length) / len(news_encoded_length))\n",
    "max_news_length = max(news_encoded_length)\n",
    "print('max news length', max_news_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# prepare the input and target for the network\n",
    "news_encoded_padded = pad_sequences(news_encoded, maxlen=max_news_length, padding='pre')\n",
    "x_news = np.array(news_encoded_padded)\n",
    "y_news = to_categorical(groups, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_news_train, x_news_test, y_news_train, y_news_test = train_test_split(x_news, y_news, test_size=0.1, stratify=y_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 400, 400)          82976800  \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 500)               450500    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               128256    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 83,556,584\n",
      "Trainable params: 83,556,584\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the new network structure\n",
    "model_news = Sequential()\n",
    "model_news.add(Embedding(news_vocabulary_size, 400, input_length=max_news_length))\n",
    "model_news.add(SimpleRNN(units=500, activation='sigmoid'))\n",
    "model_news.add(Dropout(0.5))\n",
    "model_news.add(Dense(256, activation='sigmoid'))\n",
    "model_news.add(Dropout(0.5))\n",
    "model_news.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model_news.summary()\n",
    "model_news.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7053203860371525633\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6700198133\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12412288893670240266\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Train on 11797 samples, validate on 1311 samples\n",
      "Epoch 1/20\n",
      "11797/11797 [==============================] - 29s 2ms/sample - loss: 1.3739 - acc: 0.3376 - val_loss: 1.3090 - val_acc: 0.3722\n",
      "Epoch 2/20\n",
      "11797/11797 [==============================] - 29s 2ms/sample - loss: 1.3144 - acc: 0.3673 - val_loss: 1.2973 - val_acc: 0.3722\n",
      "Epoch 3/20\n",
      "11797/11797 [==============================] - 28s 2ms/sample - loss: 1.1890 - acc: 0.4512 - val_loss: 1.0961 - val_acc: 0.5286\n",
      "Epoch 4/20\n",
      "11797/11797 [==============================] - 28s 2ms/sample - loss: 0.8521 - acc: 0.6516 - val_loss: 1.0089 - val_acc: 0.5881\n",
      "Epoch 5/20\n",
      "11797/11797 [==============================] - 29s 2ms/sample - loss: 0.5872 - acc: 0.7726 - val_loss: 0.9626 - val_acc: 0.6102\n",
      "Epoch 6/20\n",
      "11797/11797 [==============================] - 26s 2ms/sample - loss: 0.3978 - acc: 0.8554 - val_loss: 0.9736 - val_acc: 0.6331\n",
      "Epoch 7/20\n",
      "11797/11797 [==============================] - 27s 2ms/sample - loss: 0.2928 - acc: 0.8979 - val_loss: 1.0004 - val_acc: 0.6812\n",
      "Epoch 8/20\n",
      "11797/11797 [==============================] - 26s 2ms/sample - loss: 0.2244 - acc: 0.9242 - val_loss: 1.1662 - val_acc: 0.6377\n",
      "Epoch 9/20\n",
      "11797/11797 [==============================] - 26s 2ms/sample - loss: 0.1895 - acc: 0.9401 - val_loss: 1.1066 - val_acc: 0.6606\n",
      "Epoch 10/20\n",
      "11797/11797 [==============================] - 26s 2ms/sample - loss: 0.1383 - acc: 0.9551 - val_loss: 1.1644 - val_acc: 0.6873\n",
      "Epoch 11/20\n",
      "11797/11797 [==============================] - 26s 2ms/sample - loss: 0.1250 - acc: 0.9582 - val_loss: 1.2155 - val_acc: 0.6850\n",
      "Epoch 12/20\n",
      "11797/11797 [==============================] - 26s 2ms/sample - loss: 0.1293 - acc: 0.9569 - val_loss: 1.1849 - val_acc: 0.6926\n",
      "Epoch 13/20\n",
      "11797/11797 [==============================] - 26s 2ms/sample - loss: 0.1079 - acc: 0.9672 - val_loss: 1.2746 - val_acc: 0.6888\n",
      "Epoch 14/20\n",
      "11797/11797 [==============================] - 26s 2ms/sample - loss: 0.1028 - acc: 0.9663 - val_loss: 1.3048 - val_acc: 0.7079\n",
      "Epoch 15/20\n",
      "11797/11797 [==============================] - 26s 2ms/sample - loss: 0.1122 - acc: 0.9624 - val_loss: 1.3464 - val_acc: 0.6773\n",
      "Epoch 16/20\n",
      "11797/11797 [==============================] - 26s 2ms/sample - loss: 0.0910 - acc: 0.9702 - val_loss: 1.3871 - val_acc: 0.6850\n",
      "Epoch 17/20\n",
      "11797/11797 [==============================] - 26s 2ms/sample - loss: 0.2187 - acc: 0.9375 - val_loss: 1.9257 - val_acc: 0.3257\n",
      "Epoch 18/20\n",
      "11797/11797 [==============================] - 26s 2ms/sample - loss: 0.5127 - acc: 0.8009 - val_loss: 1.3939 - val_acc: 0.6323\n",
      "Epoch 19/20\n",
      "11797/11797 [==============================] - 26s 2ms/sample - loss: 0.1708 - acc: 0.9457 - val_loss: 1.3796 - val_acc: 0.6629\n",
      "Epoch 20/20\n",
      "11797/11797 [==============================] - 26s 2ms/sample - loss: 0.1208 - acc: 0.9674 - val_loss: 1.3930 - val_acc: 0.6621\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())\n",
    "filepath = \"./model_news.pth\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False)\n",
    "news_training_history = model_news.fit(\n",
    "    x_news_train,\n",
    "    y_news_train,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_news_test, y_news_test),\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report your accuracy results on the validation set.\n",
    "The best validation loss of the model is 0.9626 at epoch 5, and the best accuracy of the model is 0.7079 at epoch 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('deeplearning': conda)",
   "language": "python",
   "name": "python37664bitdeeplearningcondae305f790eddb4010bbd727137fcc23c4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
